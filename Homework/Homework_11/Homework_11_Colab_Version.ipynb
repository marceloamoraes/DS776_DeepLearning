{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#### RUN THIS IN EVERY NEW COLAB SESSION\n",
    "#### RUN IT if you change runtimes\n",
    "#### shouldn't need to run after a kernel restart in the same session\n",
    "\n",
    "from google.colab import drive\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "COLAB_NOTEBOOKS_DIR = Path(\"/content/drive/MyDrive/Colab Notebooks\")\n",
    "\n",
    "########## MODIFY THIS PATH TO AS NEEDED ##########\n",
    "WORKING_DIR = COLAB_NOTEBOOKS_DIR / \"Homework_11\"\n",
    "################################################### \n",
    "sys.path.append(str(WORKING_DIR))\n",
    "\n",
    "# ✅ Now you can import from helpers.py in the your homework folder\n",
    "\n",
    "# ✅ Install JupyterLab so the nbconvert lab template becomes available\n",
    "%pip install -q jupyterlab jupyterlab_widgets\n",
    "!jupyter nbconvert --to html --template lab --stdout --output dummy /dev/null || true\n",
    "\n",
    "# ✅ Install the introdl course package\n",
    "!wget -q https://github.com/DataScienceUWL/DS776/raw/main/Lessons/Course_Tools/introdl.zip\n",
    "!unzip -q introdl.zip -d introdl_pkg\n",
    "%pip install -q -e introdl_pkg --no-cache-dir\n",
    "\n",
    "src_path = Path(\"introdl_pkg/src\").resolve()\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Reload the introdl package (no kernel restart needed)\n",
    "import importlib\n",
    "try:\n",
    "    import introdl\n",
    "    importlib.reload(introdl)\n",
    "except ImportError:\n",
    "    import introdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.49.0'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Run this cell later when you want to export your notebook to HTML\n",
    "# see post @420 in Piazza for how to do this in CoCalc\n",
    "\n",
    "from introdl.utils import convert_nb_to_html\n",
    "my_html_file = (WORKING_DIR / \"Homework_11_MY_NAME.html\").resolve()  # change file name as needed\n",
    "my_notebooks_dir = (WORKING_DIR / \"Homework_11_Colab_Version.ipynb\").resolve() # must include name of this notebook\n",
    "convert_nb_to_html(output_filename = my_html_file, notebook_path = my_notebooks_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add other packages as needed here\n",
    "\n",
    "from introdl.utils import config_paths_keys\n",
    "\n",
    "paths = config_paths_keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 11 - Text Generation\n",
    "\n",
    "Complete the tasks below.  Generate an HTML version of your final notebook and upload it to Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Understanding Compute Costs. (10 points)\n",
    "\n",
    "1. Search for and report the vocabulary size, embedding dimension, and number of decoder layer for Llama-3.3 (a 70B parameter model) and Llama-3.1-405B.  Present your results in a table.\n",
    "\n",
    "2. Compare the FLOPs generating 300 tokens from a prompt of 500 tokens for Llama-3.3 and Llama-3.1-405B.\n",
    "\n",
    "3. Now suppose the conversation has grown so that the prompt (with context) is now 50,000 tokens.  How many times more compute are required for each of the models to generate 300 tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Task 2 - Build an LLM Scorer for Determining Text Similarity (10 points)\n",
    "\n",
    "Often we want to compare generated text to a reference text.  For example suppose we've prompted our text generation model and we expect it to generate the text `reference_text =  \"The quick brown fox jumps over the lazy dog.\"` and it generates `generated_text = \"The fast crimson fox bounded over the sluggish brown hound.\"`.  Then we'd want to produce a high similarity score because the two texts say essentially the same thing.  However if the generated text were \"A slow green lizard leaps over a sleepy cat.\" we'd expect to get low similarity score.  For this exercise you're going to use a Gemini API-based model or a similar high-quality API-based model to build an `llm_score` function and then apply it to score text generated by a small local model.\n",
    "\n",
    "#### Steps:\n",
    "- Write a function named `llm_score` that:\n",
    "   - Takes two arguments: `generated_text` (the text produced by the model) and `reference_text` (the target text for comparison).\n",
    "   - Returns a **single numeric score** between **0 and 100**, where **0** indicates no similarity and **100** indicates perfect similarity.\n",
    "   - The returned score must be a number, not a string.\n",
    "\n",
    "-  Use the Open API client with the Gemini 2.0 Flash Lite model as in the lesson.  You can use a different API if you like, but the model should be at least as good as Gemini 2.0 Flash Lite, GPT 4o Mini, of a Llama 3 70B Instruct model.\n",
    "-  Craft a prompt that clearly instructs the model to provide only a numeric score.\n",
    "- Set `temperature=0.0` to ensure a deterministic response.\n",
    "- Limit the response length with `max_tokens=10`.\n",
    "\n",
    "\n",
    "#### Hint:\n",
    "- Use the following format for the prompt (adjust as needed):\n",
    "  ```python\n",
    "  prompt = (\n",
    "      f\"Evaluate the similarity between the following generated text and reference text. \"\n",
    "      f\"Score their similarity on a scale from 0 to 100, where 0 means no similarity and 100 means perfect similarity.\\n\\n\"\n",
    "      f\"Reference Text: {reference_text}\\n\\n\"\n",
    "      f\"Generated Text: {generated_text}\\n\\n\"\n",
    "      f\"Respond with only the numeric score.\"\n",
    "  )\n",
    "  ```\n",
    "\n",
    "- Example function signature:\n",
    "  ```python\n",
    "  def llm_score(generated_text, reference_text):\n",
    "      # Your implementation here\n",
    "  ```\n",
    "\n",
    "Create a working `llm_score` function and show with at least four different examples that the scores produced are plausible.  Your solution should use and mimic code from the lesson and should not be AI-generated.  The point here is for you to understand the code in the leson well enough to do this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Task 3 - Evaluate Locally Generated Text with BERTScore (10 points)\n",
    "\n",
    "### **What is BERTScore?**\n",
    "**BERTScore** is a modern evaluation metric for comparing the similarity of two texts using contextual embeddings from a pretrained transformer model (e.g., BERT, RoBERTa, DeBERTa). \n",
    "\n",
    "\n",
    "### **How It Works**\n",
    "1. **Embedding Extraction**:\n",
    "   - Both the reference text and the generated text are **tokenized** and passed through a transformer model.\n",
    "   - The model outputs contextualized embeddings for each token (e.g., vectors representing words considering their surrounding context).\n",
    "\n",
    "2. **Matching Tokens**:\n",
    "   - For each token in the generated text, the closest (most similar) token in the reference text is identified based on **cosine similarity**.\n",
    "   - The similarity score between these tokens is recorded.\n",
    "\n",
    "3. **Calculating Scores**:\n",
    "   - **Precision (P):** Measures how much of the generated text matches the reference text.\n",
    "   - **Recall (R):** Measures how much of the reference text is covered by the generated text.\n",
    "   - **F1 Score:** The harmonic mean of Precision and Recall, providing a balanced measure.\n",
    "\n",
    "\n",
    "### **How to Interpret the Scores**\n",
    "- **Precision (P):** High precision means that the generated text is closely aligned with the reference text. A score of 1.0 would indicate that all tokens in the generated text are well-matched with relevant tokens in the reference text.\n",
    "- **Recall (R):** High recall means that most of the important tokens from the reference text are present in the generated text. A score of 1.0 would indicate that the generated text covers all the important tokens from the reference.\n",
    "- **F1 Score:** Combines Precision and Recall. A high F1 score indicates both good coverage (recall) and relevance (precision). It’s often the most important score to consider when evaluating generated text.\n",
    "\n",
    "**Typical Range:** Scores are usually between **0 and 1**, where:\n",
    "- **0:** No meaningful similarity.\n",
    "- **1:** Perfect similarity.\n",
    "\n",
    "\n",
    "### **Example Interpretation**\n",
    "| Metric      | Score   | Interpretation                          |\n",
    "|-------------|---------|-----------------------------------------|\n",
    "| Precision   | 0.85    | The generated text matches most of the reference's key concepts. |\n",
    "| Recall      | 0.78    | The generated text misses some important concepts present in the reference. |\n",
    "| F1 Score    | 0.81    | A good overall similarity, with room for improvement in coverage. |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTScore Explanation and Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What is BERTScore?**\n",
    "**BERTScore** is a modern evaluation metric for comparing the similarity of two texts using contextual embeddings from a pretrained transformer model (e.g., BERT, RoBERTa, DeBERTa). Unlike traditional metrics like BLEU and ROUGE that rely on exact word overlap, BERTScore measures similarity in **semantic meaning** by comparing embeddings.\n",
    "\n",
    "### **How It Works**\n",
    "1. **Embedding Extraction**:\n",
    "   - Both the reference text and the generated text are **tokenized** and passed through a transformer model.\n",
    "   - The model outputs contextualized embeddings for each token (e.g., vectors representing words considering their surrounding context).\n",
    "\n",
    "2. **Matching Tokens**:\n",
    "   - For each token in the generated text, the closest (most similar) token in the reference text is identified based on **cosine similarity**.\n",
    "   - The similarity score between these tokens is recorded.\n",
    "\n",
    "3. **Calculating Scores**:\n",
    "   - **Precision (P):** Measures how much of the generated text matches the reference text.\n",
    "   - **Recall (R):** Measures how much of the reference text is covered by the generated text.\n",
    "   - **F1 Score:** The harmonic mean of Precision and Recall, providing a balanced measure.\n",
    "\n",
    "### **How to Interpret the Scores**\n",
    "- **Precision (P):** High precision means that the generated text is closely aligned with the reference text. A score of 1.0 would indicate that all tokens in the generated text are well-matched with relevant tokens in the reference text.\n",
    "- **Recall (R):** High recall means that most of the important tokens from the reference text are present in the generated text. A score of 1.0 would indicate that the generated text covers all the important tokens from the reference.\n",
    "- **F1 Score:** Combines Precision and Recall. A high F1 score indicates both good coverage (recall) and relevance (precision). It’s often the most important score to consider when evaluating generated text.\n",
    "\n",
    "**Typical Range:** Scores are usually between **0 and 1**, where:\n",
    "- **0:** No meaningful similarity.\n",
    "- **1:** Perfect similarity.\n",
    "\n",
    "\n",
    "### **Example Interpretation**\n",
    "| Metric      | Score   | Interpretation                          |\n",
    "|-------------|---------|-----------------------------------------|\n",
    "| Precision   | 0.85    | The generated text matches most of the reference's key concepts. |\n",
    "| Recall      | 0.78    | The generated text misses some important concepts present in the reference. |\n",
    "| F1 Score    | 0.81    | A good overall similarity, with room for improvement in coverage. |\n",
    "\n",
    "### **Example Code:**\n",
    "The code in the following cell demonstrates how to use generate text with a local model, clean the output of the model, and compute the BERTScores.  You'll modify the code to complete the exercises that follow for this task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Generated Text</th>\n",
       "      <th>BERTScore P</th>\n",
       "      <th>BERTScore R</th>\n",
       "      <th>BERTScore F1</th>\n",
       "      <th>LLM Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Greedy</th>\n",
       "      <td>Supervised learning is used in image recognition, natural language processing, and other areas where a clear label is given to the model. Unsupervised learning is used in clustering, dimensionality reduction, and other areas where no clear label is given to the model. Step 1: Define Supervised Learning Supervised learning is a type of machine learning where the model is trained on labeled data. The model learns from examples that are already labeled, allowing it to learn the relationship between input and output. The goal of supervised learning is to make predictions on new, unseen data based on the patterns learned from the labeled data. Step 2: Define Unsupervised Learning Unsupervised learning, on the other hand, is a type of machine learning where the model is trained on unlabeled data. The model learns to identify patterns, relationships, and structure in the data without any prior knowledge of the expected output. The goal</td>\n",
       "      <td>0.5696</td>\n",
       "      <td>0.7600</td>\n",
       "      <td>0.6512</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top-K</th>\n",
       "      <td>In supervised learning, the model is trained on labeled data, where the target variable is already known. The model learns to predict the target variable based on the input features. In unsupervised learning, the model is trained on unlabeled data, where the target variable is not known. The model learns to identify patterns or relationships in the data. Supervised learning is often used for tasks such as classification, regression, and feature selection. Unsupervised learning is often used for tasks such as clustering, dimensionality reduction, and anomaly detection. Here is a simple example to illustrate the difference: Supervised Learning: You want to predict the price of a house based on its features (number of bedrooms, square footage, etc.). You have a dataset of labeled houses with their corresponding prices. You train a model on this data to learn the relationship between the features and the price. The model learns to predict the</td>\n",
       "      <td>0.5495</td>\n",
       "      <td>0.7514</td>\n",
       "      <td>0.6348</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from bert_score import score\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "def clean_output(text, prompt=\"\"):\n",
    "    \"\"\"\n",
    "    Cleans the model-generated text by removing the prompt, formatting, and common prefixes/suffixes.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The generated text from the model.\n",
    "        prompt (str): The prompt used for generating the text. If present at the beginning, it will be removed.\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text ready for evaluation.\n",
    "    \"\"\"\n",
    "    # Remove the prompt if it exists at the beginning of the text\n",
    "    if text.startswith(prompt):\n",
    "        text = text[len(prompt):].strip()\n",
    "    \n",
    "    # Remove markdown-like formatting (titles, headers, etc.)\n",
    "    text = re.sub(r\"\\*\\*.*?\\*\\*|=+\", \"\", text)\n",
    "    \n",
    "    # Remove bullet points, numbers, dashes, and unwanted newlines\n",
    "    text = re.sub(r\"(\\*|-|•|\\d+\\.)\\s\", \"\", text)\n",
    "    text = re.sub(r\"\\n+\", \" \", text)\n",
    "\n",
    "    # Remove anything that's not alphanumeric, standard punctuation, or whitespace\n",
    "    text = re.sub(r\"[^a-zA-Z0-9.,!?;:\\-()\\'\\\"\\s]\", \"\", text)\n",
    "\n",
    "    # Remove common prefixes like \"The answer is:\", \"Here is the explanation:\", etc.\n",
    "    unwanted_prefixes = [\n",
    "        \"The answer is:\", \"Here is the explanation:\", \n",
    "        \"In conclusion,\", \"To summarize,\", \"As follows:\"\n",
    "    ]\n",
    "    for prefix in unwanted_prefixes:\n",
    "        if text.startswith(prefix):\n",
    "            text = text[len(prefix):].strip()\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"Explain the difference between supervised and unsupervised learning.\"\n",
    "\n",
    "# Encode the prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate text using Greedy Decoding and Top-K Sampling\n",
    "max_length = 200  # Maximum length of the generated text\n",
    "with torch.no_grad():\n",
    "    greedy_output = model.generate(**inputs, max_length=max_length)\n",
    "    top_k_output = model.generate(**inputs, max_length=max_length, top_k=50)\n",
    "\n",
    "# Decode the outputs\n",
    "generated_texts = {\n",
    "    \"Greedy\": tokenizer.decode(greedy_output[0], skip_special_tokens=True),\n",
    "    \"Top-K\": tokenizer.decode(top_k_output[0], skip_special_tokens=True)\n",
    "}\n",
    "\n",
    "# Clean all generated texts\n",
    "cleaned_texts = {strategy: clean_output(text, prompt) for strategy, text in generated_texts.items()}\n",
    "\n",
    "# Reference text (ground truth)\n",
    "reference = [\n",
    "    \"Supervised learning uses labeled data to learn a mapping from inputs to outputs, while unsupervised learning tries to find patterns or groupings within unlabeled data.\"\n",
    "]\n",
    "\n",
    "results = {}\n",
    "for strategy, text in cleaned_texts.items():\n",
    "    P, R, F1 = score([text], reference, lang=\"en\", model_type=\"microsoft/deberta-xlarge-mnli\")\n",
    "    llm_result = llm_score(text, reference[0])\n",
    "    results[strategy] = {\n",
    "        \"Generated Text\": text,\n",
    "        \"BERTScore P\": P.mean().item(),\n",
    "        \"BERTScore R\": R.mean().item(),\n",
    "        \"BERTScore F1\": F1.mean().item(),\n",
    "        \"LLM Score\": llm_result,\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "dataframe=pd.DataFrame.from_dict(results, orient='index')\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(dataframe.to_html(float_format=\"%.4f\", justify=\"center\", index=True, border=0, classes='dataframe')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercises for Task 3**\n",
    "For the exercises, use the following prompt and reference text:\n",
    "Prompt: \"Describe the process of photosynthesis.\"\n",
    "Reference: \"Photosynthesis is the process by which green plants and some other organisms use sunlight to synthesize foods with the aid of chlorophyll. The process generally involves the absorption of carbon dioxide and the release of oxygen as a by-product.\"\n",
    "\n",
    "1. Add Top-P (temp = 0.7, p = 0.90)  and Beam Search (3 beams) decoding to the model evaluation and comment on the results.\n",
    "2. Now, with a new code cell, evaluate four variations of top-p generation.  Compare the BERTScore and llm_score results for each temperature and explain how temperature impacts the quality of the generated text.\n",
    "    - Low: 0.3 (High coherence, low diversity)\n",
    "    - Medium: 0.7 (Balanced coherence and diversity)\n",
    "    - High: 1.0 (More diverse, but potentially less coherent)\n",
    "    - Very High: 1.5 (Significantly increased randomness, likely decreased coherence)\n",
    "3. Notice that BERTScore matches similar tokens regardless of order.  Explain, with an example, how this could be a weakness of using BERTScore as an evaluation metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Task 4 - Build a Local Chatbot Simulator (10 points)\n",
    "\n",
    "This task requires you to pull together a few pieces from the lesson notebook.  We want you to mimic the Chatbot Simulation from the lesson, but using the local model \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\" or similar.  You'll need to create a function similar to the `chat_with_gemini function`, call it `chat_with_local_model`.  \n",
    "\n",
    "* Instead of passing the name of the model you'll want to pass in the model and tokenizer that you've already loaded.  \n",
    "* Pass a `split_string` to `chat_with_local_model` and use it to split the output of the model so that the assistant response contains only the response and doesn't repeat the input prompt (we did this in the lesson too).  You can set the default `split_string` as needed.\n",
    "* Also allow additional arguments to be passed to `chat_with_local_model` that you pass along to `model.generate()` so you can configure the decoding strategy.\n",
    "* Use the chat_template to format the conversation in the tokenization step.\n",
    "* You should use the helper function `visualize_conversation` to show the conversation after each new prompt.  \n",
    "\n",
    "Your function will look something like this:\n",
    "\n",
    "```python\n",
    "# copy this to a code cell to start\n",
    "\n",
    "def chat_with_local_model( user_input, model, tokenizer, split_string='assistant', **kwargs):\n",
    "\n",
    "# fill in lots of stuff here\n",
    "\n",
    "    outputs = model.generate(input_ids, **kwargs)\n",
    "\n",
    "# more stuff here\n",
    "```\n",
    "\n",
    "Then you could call it like this (after initializing the conversation with the system prompt):\n",
    "```python\n",
    "chat_with_local_model( 'Tell me about overfitting in deep learning', model, tokenizer, do_sample=False, top_p = None, temperature=None)\n",
    "```\n",
    "\n",
    "Your solution should use and mimic code from the lesson and should not be AI-generated.  The point here is for you to understand the code in the leson well enough to do this.\n",
    "\n",
    "Demonstrate that your function works by showing two conversations with at least 3 inputs each.  \n",
    "1.  A creative conversation to generate a jingle, poem, limerick, or similar and refine it.  Use an appropriate decoding strategy.\n",
    "2.  A coding conversation where you ask the chatbot to create some code and refine it.  Use an appropriate decoding strategy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
