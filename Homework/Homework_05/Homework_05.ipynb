{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 - Transfer Learning\n",
    "\n",
    "In this homework you'll experiment with applying transfer learning for fine-grained classification using the Flowers102 dataset in torchvision.datasets.  Fine-grained classification is when you have many categories or classes that are similar like related series of flowers.  Or, for example, trying to distinguish breeds of dogs as opposed to cats, dogs, and foxes.\n",
    "\n",
    "Note: we were able to train all the models described in this homework in about 40 minutes on the T4 Compute Server.  The ConvNext model was the biggest and took the most time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Flowers102 dataset\n",
    "\n",
    "There are 102 classes of flowers each with between 40 and 258 images. The dataset is available in torchvision as `torchvision.datasets.Flowers102`.  You can find more information about the [dataset here](https://www.robots.ox.ac.uk/~vgg/data/flowers/102/).  The labels for the classes are also [available here](https://gist.github.com/JosephKJ/94c7728ed1a8e0cd87fe6a029769cde1).  \n",
    "\n",
    "The dataset has three splits each of which can be accessed with code like this:\n",
    "\n",
    "```python\n",
    "train_dataset = Flowers102(root=DATA_PATH, split='train', download=True, transform = transform_train)\n",
    "```\n",
    "\n",
    "To get the validation and testing splits change split to 'valid' or 'test'.  \n",
    "\n",
    "### Data Exploration (5 pts)\n",
    "\n",
    "In this section you should explore the dataset a bit.  Plot a few examples and find at least two classes that have similar looking flowers.  Also how many images per class in the training and validation sets?  You may want to start with transforms that don't add any augmentation for the purposes of exploring.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation and DataLoaders (5 pts)\n",
    "\n",
    "Build your transforms for training.  Remember that for testing and validation the transforms shouldn't add any augmentation.  The images should be $224 \\times 224$ when transformed since our pretrained models were trained on Imagenet with the same size images.  We used `batch_size = 32` on the T4 Compute Servers.  For normalization use the statistics from Imagenet since the pretrained models we are using expect that normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50 (5 pts)\n",
    "\n",
    "The ResNet models establish good baselines for results.\n",
    "\n",
    "Build a custom model class for ResNet50 (AI may be helpful here) with an adjustable number of output classes.  It should have methods to freeze and unfreeze the backbone.  Apply transfer learning instantiating your model with the default Imagenet weights and training with for 5 epochs followed by training for a suitable number of epochs (you may need to experiment).  Include graphics or display dataframes to show how the model is converging (at least for the unfrozen training).\n",
    "\n",
    "Use the training and validation sets here.  The test set will be reserved for your final best model. \n",
    "\n",
    "What kind of validation accuracy are you able to achieve?  Is the model overfitting?\n",
    "\n",
    "Note: the training dataset is already pretty small so downsampling it to expedite experimentation isn't a good idea, but you could temporarily reduce the size of the images to say 128x128 in your tranforms to get things working, then go back to 224x224 to train your models.  All final results should be done with 224x224."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNet V2 Small (5 pts)\n",
    "\n",
    "EfficientNet models are a modern upgrade to traditional convolutional neural networks, offering improved performance and efficiency.  Repeat what you did for ResNet50 for EfficientNet V2 Small.  Use AI to search for how to load it in torchvision and how to adapt in your custom model class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNeXt Small (5 pts)\n",
    "\n",
    "ConvNeXt models are a family of convolutional neural networks that aim to modernize the design of traditional CNNs by incorporating elements from vision transformers. They provide a strong performance baseline for various computer vision tasks.  Use transfer learning to train a ConvNeXT Small (not Tiny) model on Flowers102."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT Small (5 pts)\n",
    "\n",
    "Vision Transformers (ViTs) are a type of neural network architecture that leverages the transformer model, originally designed for natural language processing, to process image data. Unlike Convolutional Neural Networks (CNNs), which use convolutional layers to capture spatial hierarchies, ViTs divide images into patches and process them as sequences, allowing for global context understanding. ViTs typically require more data to train from scratch compared to CNNs, but they can be effectively used for transfer learning on smaller datasets if the images are similar to those in the Imagenet dataset.  We'll learn more about transformer models in the second half of the course.\n",
    "\n",
    "We'll use the timm library which doesn't seem to be installed in CoCalc.  \n",
    "To use ViT Small from the timm library, you can install timm with the following command:\n",
    "```python\n",
    "!pip install timm\n",
    "```\n",
    "Then, load the pre-trained ViT Small model with:\n",
    "```python\n",
    "import timm\n",
    "model = timm.create_model('vit_small_patch16_224', pretrained=True)\n",
    "```\n",
    "\n",
    "(Note: you'll need to copy this code from this markdown cell to a regular code cell for the installation to work correctly.)\n",
    "\n",
    "The ViT Small model is pretrained on Imagenet and expects the same size images and same normalization as other models.  Typically we fine tune the whole model and don't train with a frozen backbone.  The learning rates used are usually smaller, too.  Do the same kind of fine tuning as you've done above using OneCycleLR with max_lr = 0.0005.  We found that the number of epochs needed was similar to the total number of epochs used in the two-phase training used by our other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Best Model to Test Data and Evaluate (10 pts)\n",
    "\n",
    "Write a brief summary of your investigations above.  Include a graph comparing the training metrics from the fine-tuning phases on the validation data from above.\n",
    "\n",
    "Generate a classification report comparing the predictions of your best model to the ground truth labels on the test dataset.  Summarize the highlights of the report.  A confusion matrix display probably isn't helpful because there are so many classes (set `display_confusion=False` if use `evaluate_classifier` from `introdl.utils`.)  But you can look at slices of the confusion matrix.  Try to identify at least two classes which are being confused by your model and display examples, with proper labels, from those classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
