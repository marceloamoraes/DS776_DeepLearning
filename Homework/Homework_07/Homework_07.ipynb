{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7:  Exploring Hugging Face Pipelines and LLM Prompting\n",
    "\n",
    "In this assignment, you will explore different NLP tasks using Hugging Face's transformers and LLM-based prompting. You will also experiment with different models, zero-shot prompting, and few-shot prompting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODELS_PATH=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\models\n",
      "DATA_PATH=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\data\n",
      "TORCH_HOME=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\n",
      "HF_HOME=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from introdl.utils import get_device, wrap_print_text, config_paths_keys\n",
    "from introdl.nlp import llm_configure, llm_generate, clear_pipeline, print_pipeline_info, display_markdown\n",
    "\n",
    "# overload print to wrap text\n",
    "print = wrap_print_text(print)\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "paths = config_paths_keys()\n",
    "\n",
    "mistral_config = llm_configure('mistral-7B')\n",
    "gemini_config = llm_configure('gemini-flash-lite')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some notes about LLMs and Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get deterministic responses by setting `search_strategy = \"deterministic\".   This can help when using an LLM to produce repeatable results.  We'll learn about how LLMs generate text in Lesson 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi (symbolized by the Greek letter π) is a mathematical constant that represents\n",
      "the ratio of a circle's circumference to its diameter. It is approximately equal\n",
      "to 3.14159 and has been calculated to trillions of decimal places beyond its\n",
      "common representation. The value of pi is an irrational number, meaning it\n",
      "cannot be expressed as a simple fraction and its decimal representation goes on\n",
      "indefinitely without repeating.\n",
      "Pi appears frequently in many areas of mathematics, particularly when dealing\n",
      "with circles or trigonometry. Some notable formulas involving pi include:\n",
      "- Area of a circle: A = πr²\n",
      "- Circumference of a circle: C = 2πr\n",
      "- Volume of a sphere: V = (4/3)πr³\n",
      "In addition to its mathematical significance, pi also holds cultural importance\n",
      "due to its ubiquity across various fields such as physics, engineering, computer\n",
      "science, and\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are an AI assistant who gives helpful, concise answers.\"\n",
    "user_prompt = \"Tell me about the number pi.\"\n",
    "\n",
    "mistral_output = llm_generate(mistral_config, user_prompt, system_prompt=system_prompt, search_strategy='deterministic')\n",
    "print(mistral_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### System and User Prompts\n",
    "\n",
    "* Use the system prompt to set the overall behavior of the LLM.  e.g. 'You are a Named Entity Recognition expert.'\n",
    "* The main prompt is often called the user prompt, especially in the context of chats.\n",
    "* Use the user prompt to give detailed instructions, possibly examples, the particular text you want analyzed.  e.g. 'Input:{text}, Entities:'.\n",
    "\n",
    "These are not rules for prompts, only guidelines.  API based LLMs may actually use or weight the system and user prompts differently but local LLMs generally just concatenate the system and user prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided Texts for Tasks 1 and 2\n",
    "\n",
    "texts = [\n",
    "    \"The new AI technology developed by OpenAI is revolutionizing various industries, from healthcare to finance.\",\n",
    "    \"Marie Curie was a physicist and chemist who conducted research on radioactivity.\",\n",
    "    \"In 2023, NASA successfully landed another rover on Mars, aiming to explore signs of ancient life.\",\n",
    "    \"The recent advancements in quantum computing by IBM have the potential to solve complex problems that are currently unsolvable with classical computers.\",\n",
    "    \"Despite the company's efforts, the new product launch by XYZ Corp was a complete failure, leading to significant financial losses and a drop in stock prices.\",\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Use the default sentiment analysis pipeline from HuggingFace to get determine the sentiment of each of the texts. Note that some HuggingFace pipelines can handle multiple inputs passed as a list or you can write a loop to iterate over the texts.\n",
    " Use clear_pipeline() to free the memory after you're done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Try a different HuggingFace model, pass `model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"` to `pipeline()` to instantiate a different classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  Explore HuggingFace to find a different model for sentiment analysis and apply it each of the texts.  You may have to try more than one model to find which produces relevant classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.  Which of the three models seems to best capture the sentiments of the texts.  EXPLAIN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Named Entity Recognition (NER)\n",
    "\n",
    "1.  Apply the default HuggingFace NER pipeline to each of the texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Now use an LLM  (local or API-based) along with a zero-shot prompt (no examples) to try to get the LLM provide a list of entities in JSON format like this:  [{\"entity\": \"entity_name\", \"type\": \"entity_type\"}].  You'll have to experiment with the system and user prompt to get this to work.  Use `llm_generate` from the introdl package.  You can pass `search_strategy=\"deterministic\"` to get reproducible results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - Text Generation\n",
    "\n",
    "Think of a short creative task like writing text for an advertisement, lyrics for a jingle, etc.  Create a prompt for the task.  Use the default HuggingFace pipeline to generate text for the task.  Use `llm_generate` with two different models to do the tasks.  Compare the results.  Which model or pipeline produced the best result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Translation\n",
    "\n",
    "Pick your own short text of at least 3 sentences and translate it to another language (not Spanish) and back and compare the back-translated result to the original text (or if you're fluent in the other language you can comment directly on the translation).\n",
    "\n",
    "1.  Do this with a HuggingFace pipeline (search HuggingFace for an appropriate model)\n",
    "2.  Do this by using an LLM and llm_generate.\n",
    "\n",
    "Which works better?  The specialized model or the LLM?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Summarization\n",
    "\n",
    "For this task you're going to generate summaries of [\"The Bitter Lesson\"](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) by Rich Sutton.  If you haven't read it already, you should since it's directly related to deep learning.  \n",
    "\n",
    "The next code cell grabs the text of \"The Bitter Lesson\".  You may need to `!pip install bs4` to install BeautifulSoup - a webscraping library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate three different summaries of the text and comment on the differences.  For the three summaries:\n",
    "1.  Use the default HuggingFace pipeline for summarization.\n",
    "2.  Find another model on HuggingFace and use it.\n",
    "3.  Use an LLM with `llm_generate`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6:  Sarcasm Detection with an LLM\n",
    "\n",
    "The code in the next cell loads the Sarcasm News Headlines dataset from HuggingFace (we'll use this again next week) and put the results in a dataframe.  A label of 1 indicates sarcasm and a label of 0 is not sarcastic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>headline</th>\n",
       "      <th>article_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>my white inheritance</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/my-white-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>5 ways to file your taxes with less stress</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/5-ways-to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>richard branson's global-warming donation near...</td>\n",
       "      <td>https://www.theonion.com/richard-bransons-glob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>shadow government getting too large to meet in...</td>\n",
       "      <td>https://politics.theonion.com/shadow-governmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>lots of parents know this scenario</td>\n",
       "      <td>https://www.huffingtonpost.comhttp://pubx.co/6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           headline  \\\n",
       "0      1  thirtysomething scientists unveil doomsday clo...   \n",
       "1      0  dem rep. totally nails why congress is falling...   \n",
       "2      0  eat your veggies: 9 deliciously different recipes   \n",
       "3      1  inclement weather prevents liar from getting t...   \n",
       "4      1  mother comes pretty close to using word 'strea...   \n",
       "5      0                               my white inheritance   \n",
       "6      0         5 ways to file your taxes with less stress   \n",
       "7      1  richard branson's global-warming donation near...   \n",
       "8      1  shadow government getting too large to meet in...   \n",
       "9      0                 lots of parents know this scenario   \n",
       "\n",
       "                                        article_link  \n",
       "0  https://www.theonion.com/thirtysomething-scien...  \n",
       "1  https://www.huffingtonpost.com/entry/donna-edw...  \n",
       "2  https://www.huffingtonpost.com/entry/eat-your-...  \n",
       "3  https://local.theonion.com/inclement-weather-p...  \n",
       "4  https://www.theonion.com/mother-comes-pretty-c...  \n",
       "5  https://www.huffingtonpost.com/entry/my-white-...  \n",
       "6  https://www.huffingtonpost.com/entry/5-ways-to...  \n",
       "7  https://www.theonion.com/richard-bransons-glob...  \n",
       "8  https://politics.theonion.com/shadow-governmen...  \n",
       "9  https://www.huffingtonpost.comhttp://pubx.co/6...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Twitter Sarcasm Dataset\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "dataset = load_dataset(\"raquiba/Sarcasm_News_Headline\")\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "df = df.rename(columns={'is_sarcastic': 'label'})\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task use an LLM to classify the first 10 headlines in dataset as sarcastic or not sarcastic.  Use a local LLM or an API-based LLM (or both).  If you want to take this a step further you can experiment with few-shot prompting by providing some examples in your user prompt to the LLM.  Compare your results to the actual labels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
