{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91f35c78",
   "metadata": {},
   "source": [
    "# Homework 12 - Text Summarization\n",
    "\n",
    "We're going to work with conversational data in this homework.  The `SAMsum` dataset consists of chat-like conversations and summaries like this:\n",
    "\n",
    "Conversation-\n",
    "```\n",
    "Olivia: Who are you voting for in this election?\n",
    "Oliver: Liberals as always.\n",
    "Olivia: Me too!!\n",
    "Oliver: Great\n",
    "```\n",
    "\n",
    "Summary-\n",
    "```\n",
    "Olivia and Olivier are voting for liberals in this election.\n",
    "```\n",
    "\n",
    "Applications for this kind of summarization include generating chat and meeting summaries.\n",
    "\n",
    "Throughout this assignment you'll work with the first 100 conversations and summaries from the validation split of [\"knkarthick/samsum\"](https://huggingface.co/datasets/knkarthick/samsum) on Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9906df4a",
   "metadata": {},
   "source": [
    "## Task 1 - Build a zero-shot LLM conversation summarizer (10 points)\n",
    "\n",
    "Use either an 8B local Llama model or an API-based model like `gemini-2.0-flash-lite` or better to build an `llm_summarizer` function that takes as input a list of conversations and returns a list of extracted summaries.  Your function should be constructed similarly to `llm_classifier` or `llm_ner_extractor` in Lessons 8 and 10, respectively.  \n",
    "\n",
    "Put some effort into the prompt to make it good at generating succinct summaries of converations that identify both the topics and the people.\n",
    "\n",
    "Your list of returned summaries should be cleanly extracted summaries with no additional text such as parts of the input prompt.\n",
    "\n",
    "Give a qualitative evaluation of the first three generated summaries compared to the ground-truth summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0a9157",
   "metadata": {},
   "source": [
    "## Task 2 - Build a few-shot LLM conversation summarizer (6 points)\n",
    "\n",
    "Follow the same instructions as in Task 1, but add a few examples from the training data.  Don't simply pick the first examples, rather take some care to choose diverse conversations and/or conversations that are difficult to summarize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3009ca1c",
   "metadata": {},
   "source": [
    "## Task 3 - Refine the llm_score function (10 points)\n",
    "\n",
    "For this task you can use a local Llama model or an API-based model.  (I personally find the API-based models much easier to use.)\n",
    "\n",
    "Start with the `llm_score` function from last week and refine the prompt to improve the scoring to better reflect similarities in semantic meaning between two texts.  Here are some guidelines that you should incorporate into your prompt:\n",
    "\n",
    "- A score of **100** means the texts have **identical meaning**.\n",
    "- A score of **80–99** means they are **strong paraphrases** or very similar in meaning.\n",
    "- A score of **50–79** means they are **somewhat related**, but not expressing the same idea.\n",
    "- A score of **1–49** means they are **barely or loosely related**.\n",
    "- A score of **0** means **no semantic similarity**.\n",
    "- Take into account word meaning, order, and structure.\n",
    "- Synonyms count as matches.\n",
    "- Do not reward scrambled words unless they convey the same meaning.\n",
    "- Make the prompt few-shot by including several text pairs and the corresponding similarity scores.\n",
    "\n",
    "Demonstrate your `llm_score` function by applying it to the 7 sentence pairs from the lesson.  Comment on the performance of the scoring.  Does it still get fooled by the sixth and seventh pairs like BERTScore did?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b103040",
   "metadata": {},
   "source": [
    "## Task 4 - Evaluate a Pre-trained Model and LLM_summarizer (10 points)\n",
    "\n",
    "For this task you're going to qualitatively and quantitatively compare the generated summaries from:\n",
    "1. The already fine-tuned Hugging Face model - ['philschmid/flan-t5-base-samsum'](https://huggingface.co/philschmid/flan-t5-base-samsum)\n",
    "2. The zero-shot or few shot LLM summarizer from above.\n",
    "\n",
    "If, for some reason, you can't get the specified Hugging Face model to work, then find a different Hugging Face summarization model that has already been fine-tuned on SAMsum.\n",
    "\n",
    "First, qualititavely compare the first three generated summaries from each approach to the ground-truth summaries.  Explain how the the two approaches seem to be working on the three examples.\n",
    "\n",
    "Second, compute ROUGE scores, BERTScore, and llm_score for the first 100 examples in the validation set. \n",
    "\n",
    "What do these scores suggest about the performance of the two approaches?  Is one approach clearly better than the other?  Is llm_score working well as a metric?  Does it agree with the other metrics?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c6dd7f",
   "metadata": {},
   "source": [
    "## Task 5 - Comparison and Reflection (4 points)\n",
    "\n",
    "* Give a brief summary of what you learned in this assignment.\n",
    "\n",
    "* What did you find most difficult to understand?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
