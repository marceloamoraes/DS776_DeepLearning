{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "64b5e8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "%%capture\n",
    "#### RUN THIS IN EVERY NEW COLAB SESSION\n",
    "#### RUN IT if you change runtimes\n",
    "#### shouldn't need to run after a kernel restart in the same session\n",
    "\n",
    "from google.colab import drive\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "COLAB_NOTEBOOKS_DIR = Path(\"/content/drive/MyDrive/Colab Notebooks\")\n",
    "\n",
    "########## MODIFY THIS PATH TO AS NEEDED ##########\n",
    "WORKING_DIR = COLAB_NOTEBOOKS_DIR / \"Homework_12\"\n",
    "################################################### \n",
    "sys.path.append(str(WORKING_DIR))\n",
    "\n",
    "# ✅ Now you can import from helpers.py in the your homework folder\n",
    "\n",
    "# ✅ Install JupyterLab so the nbconvert lab template becomes available\n",
    "%pip install -q jupyterlab jupyterlab_widgets\n",
    "!jupyter nbconvert --to html --template lab --stdout --output dummy /dev/null || true\n",
    "\n",
    "# ✅ Install the introdl course package\n",
    "!wget -q https://github.com/DataScienceUWL/DS776/raw/main/Lessons/Course_Tools/introdl.zip\n",
    "!unzip -q introdl.zip -d introdl_pkg\n",
    "%pip install -q -e introdl_pkg --no-cache-dir\n",
    "\n",
    "src_path = Path(\"introdl_pkg/src\").resolve()\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Reload the introdl package (no kernel restart needed)\n",
    "import importlib\n",
    "try:\n",
    "    import introdl\n",
    "    importlib.reload(introdl)\n",
    "except ImportError:\n",
    "    import introdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "e956db",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#### Run this cell later when you want to export your notebook to HTML\n",
    "# see post @420 in Piazza for how to do this in CoCalc\n",
    "\n",
    "from introdl.utils import convert_nb_to_html\n",
    "my_html_file = (WORKING_DIR / \"Homework_12_MY_NAME.html\").resolve()  # change file name as needed\n",
    "my_notebooks_dir = (WORKING_DIR / \"Homework_12_Colab_Version.ipynb\").resolve() # must include name of this notebook\n",
    "convert_nb_to_html(output_filename = my_html_file, notebook_path = my_notebooks_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f35c78",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Homework 12 - Text Summarization\n",
    "\n",
    "We're going to work with conversational data in this homework.  The `SAMsum` dataset consists of chat-like conversations and summaries like this:\n",
    "\n",
    "Conversation-\n",
    "```\n",
    "Olivia: Who are you voting for in this election?\n",
    "Oliver: Liberals as always.\n",
    "Olivia: Me too!!\n",
    "Oliver: Great\n",
    "```\n",
    "\n",
    "Summary-\n",
    "```\n",
    "Olivia and Olivier are voting for liberals in this election.\n",
    "```\n",
    "\n",
    "Applications for this kind of summarization include generating chat and meeting summaries.\n",
    "\n",
    "Throughout this assignment you'll work with the first 100 conversations and summaries from the validation split of [\"spencer/samsum_reformat\"](https://huggingface.co/datasets/spencer/samsum_reformat) on Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "334401",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9906df4a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 1 - Build a zero-shot LLM conversation summarizer (10 points)\n",
    "\n",
    "Use either an 8B local Llama model or an API-based model like `gemini-2.0-flash-lite` or better to build an `llm_summarizer` function that takes as input a list of conversations and returns a list of extracted summaries.  Your function should be constructed similarly to `llm_classifier` or `llm_ner_extractor` in Lessons 8 and 10, respectively.  \n",
    "\n",
    "Put some effort into the prompt to make it good at generating succinct summaries of converations that identify both the topics and the people.\n",
    "\n",
    "Your list of returned summaries should be cleanly extracted summaries with no additional text such as parts of the input prompt.\n",
    "\n",
    "Give a qualitative evaluation of the first three generated summaries compared to the ground-truth summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0a9157",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 2 - Build a few-shot LLM conversation summarizer (6 points)\n",
    "\n",
    "Follow the same instructions as in Task 1, but add a few examples from the training data.  Don't simply pick the first examples, rather take some care to choose diverse conversations and/or conversations that are difficult to summarize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3009ca1c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 3 - Refine the llm_score function (10 points)\n",
    "\n",
    "For this task you can use a local Llama model or an API-based model.  (I personally find the API-based models much easier to use.)\n",
    "\n",
    "Start with the `llm_score` function from last week and refine the prompt to improve the scoring to better reflect similarities in semantic meaning between two texts.  Here are some guidelines that you should incorporate into your prompt:\n",
    "\n",
    "- A score of **100** means the texts have **identical meaning**.\n",
    "- A score of **80–99** means they are **strong paraphrases** or very similar in meaning.\n",
    "- A score of **50–79** means they are **somewhat related**, but not expressing the same idea.\n",
    "- A score of **1–49** means they are **barely or loosely related**.\n",
    "- A score of **0** means **no semantic similarity**.\n",
    "- Take into account word meaning, order, and structure.\n",
    "- Synonyms count as matches.\n",
    "- Do not reward scrambled words unless they convey the same meaning.\n",
    "- Make the prompt few-shot by including several text pairs and the corresponding similarity scores.\n",
    "\n",
    "Demonstrate your `llm_score` function by applying it to the 7 sentence pairs from the lesson.  Comment on the performance of the scoring.  Does it still get fooled by the sixth and seventh pairs like BERTScore did?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b103040",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 4 - Evaluate a Pre-trained Model and LLM_summarizer (10 points)\n",
    "\n",
    "For this task you're going to qualitatively and quantitatively compare the generated summaries from:\n",
    "1. The already fine-tuned Hugging Face model -   ['philschmid/flan-t5-base-samsum'](https://huggingface.co/philschmid/flan-t5-base-samsum)\n",
    "2. The zero-shot or few shot LLM summarizer from above.\n",
    "\n",
    "If, for some reason, you can't get the specified Hugging Face model to work, then find a different Hugging Face summarization model that has already been fine-tuned on SAMsum.\n",
    "\n",
    "First, qualititavely compare the first three generated summaries from each approach to the ground-truth summaries.  Explain how the the two approaches seem to be working on the three examples.\n",
    "\n",
    "Second, compute ROUGE scores, BERTScore, and llm_score for the first 100 examples in the validation set. \n",
    "\n",
    "What do these scores suggest about the performance of the two approaches?  Is one approach clearly better than the other?  Is llm_score working well as a metric?  Does it agree with the other metrics?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c6dd7f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 5 - Comparison and Reflection (4 points)\n",
    "\n",
    "* Give a brief summary of what you learned in this assignment.\n",
    "\n",
    "* What did you find most difficult to understand?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d651c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exporting to HTML\n",
    "\n",
    "We've added to the course package a helper function to export your notebook to HTML.  This uses the preferred formatting and cleans outputs that sometimes cause errors with that format.\n",
    "\n",
    "To use it, first update the course package (you'll need to do this on home and compute server if you want to use either)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "b34b01",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#!pip install ~/Lessons/Course_Tools/introdl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a07d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Restart the kernel.\n",
    "\n",
    "Add this code cell to your notebook and run it (I don't think it matters where you put in the notebook).  Modify the filename as desired.  This isn't particularly fast so you may need to wait 40 to 60 seconds the first time, and maybe 10-15 seconds thereafter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "467f85",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using notebook: Homework_12_(UPDATED).ipynb\n",
      "[INFO] Temporary copy created: /tmp/tmplmu0lu7h/Homework_12_(UPDATED).ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /tmp/tmplmu0lu7h/Homework_12_(UPDATED).ipynb to html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Writing 282653 bytes to /home/user/Homework/Homework_12/HW12_Jeff_Bagggett.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] HTML export complete: /home/user/Homework/Homework_12/HW12_Jeff_Bagggett.html\n"
     ]
    }
   ],
   "source": [
    "from introdl.utils import convert_nb_to_html\n",
    "convert_nb_to_html(\"HW12_Jeff_Bagggett.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b688",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now your html file should be in the same directory as the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel",
    "--HistoryManager.enabled=False",
    "--matplotlib=inline",
    "-c",
    "%config InlineBackend.figure_formats = set(['retina'])\nimport matplotlib; matplotlib.rcParams['figure.figsize'] = (12, 7)",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (system-wide)",
   "env": {
   },
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}