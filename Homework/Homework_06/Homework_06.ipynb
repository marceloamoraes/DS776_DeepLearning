{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to ensure course package is installed\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "course_tools_path = Path('../../Lessons/Course_Tools/').resolve() # change this to the local path of the course package\n",
    "sys.path.append(str(course_tools_path))\n",
    "\n",
    "from install_introdl import ensure_introdl_installed\n",
    "ensure_introdl_installed(force_update=False, local_path_pkg= course_tools_path / 'introdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODELS_PATH=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\models\n",
      "DATA_PATH=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\data\n",
      "TORCH_HOME=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\n",
      "HF_HOME=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\n"
     ]
    }
   ],
   "source": [
    "# please add all your imports here\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torchvision import tv_tensors\n",
    "from pathlib import Path\n",
    "\n",
    "# import local modules\n",
    "from graphics_and_data_prep import display_yolo_predictions, prepare_penn_fudan_yolo\n",
    "\n",
    "from introdl.utils import config_paths_keys\n",
    "\n",
    "paths = config_paths_keys()\n",
    "DATA_PATH = paths[\"DATA_PATH\"]\n",
    "MODELS_PATH = paths[\"MODELS_PATH\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 6\n",
    "\n",
    "For this assignment there are two primary tasks.  \n",
    "\n",
    "1.  Explore UNet and UNet++ on the nuclei segmentation task described in the textbook.\n",
    "2.  Fine-tune a YOLO model for pedestrian detection and compare the results to the Faster R-CNN model in the lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Nuclei Segmentation (20 points)\n",
    "\n",
    "You're going to use the segmentation models pytorch package as we did in the lesson to fine-tune and evaluate UNet and UNet++ models on the nuclei segmentation task shown in the textbook\n",
    "\n",
    "We've already prepared the data and put it in the directory with this homework.  The next cell contains most of a custom dataset class and some transforms to get get you started.  You'll need to finish the code with \"####\" to read the image and mask and add appropriate augmentation transforms.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\data\\nuclei_data already exists. Skipping download and extraction.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell once to download the Nuclei Segmentation dataset \n",
    "\n",
    "from graphics_and_data_prep import download_and_extract_nuclei_data\n",
    "\n",
    "# Call the function\n",
    "download_and_extract_nuclei_data(DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NucleiDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (str or Path): Path to the dataset (train or val folder).\n",
    "            transform (callable, optional): Optional transforms to apply to both image and mask.\n",
    "        \"\"\"\n",
    "        self.root = Path(root)  # Convert to pathlib Path object\n",
    "        self.transform = transform\n",
    "        self.data = []  # List to store (image_tensor, mask_tensor) tuples\n",
    "\n",
    "        # Load all image and mask files\n",
    "        all_imgs = sorted((self.root / \"images\").iterdir())\n",
    "        all_masks = sorted((self.root / \"masks\").iterdir())\n",
    "\n",
    "        # Ensure that the number of images and masks are the same\n",
    "        assert len(all_imgs) == len(all_masks), \"The number of images and masks must be the same\"        \n",
    "\n",
    "        # Read and store images and masks as tensors in memory\n",
    "        for img_path, mask_path in zip(all_imgs, all_masks):\n",
    "            # Read images and masks as tensors\n",
    "            image =#### read_image from image path, convert to float and scale to [0,1]\n",
    "            mask = #### read image from mask path, any entries bigger than 0 map to 1, rest to 0, convert to float\n",
    "\n",
    "            # Store as tuple\n",
    "            self.data.append((tv_tensors.Image(image), tv_tensors.Mask(mask)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, mask = self.data[idx]\n",
    "\n",
    "        # Apply transforms if provided\n",
    "        if self.transform:\n",
    "            image, mask = self.transform(image, mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Define a set of transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    #### Add your augmentation transforms here\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define a set of transforms for validation (without augmentation)\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = NucleiDataset(root=DATA_PATH / \"nuclei_data/train\", transform=train_transforms)\n",
    "val_dataset = NucleiDataset(root=DATA_PATH / \"nuclei_data/val\", transform=val_transforms)\n",
    "\n",
    "# create dataloaders with batch size 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now setup and train Unet and UnetPlusPlus models with a pretrained resnet50 backbone as we did in the lesson.  Model your code on the code in the \"Better Training\" part of the notebook.  You should set different learning rates for the encoder and decoder and use OneCycleLR as we did.  We found that 12 epochs of fine-tuning worked reasonably well.\n",
    "\n",
    "For each model display convergence graphs of the loss and IOU and sample images along with the ground truth and predicted masks.  \n",
    "\n",
    "Answer the following followup questions:\n",
    "1. Which model performs better?\n",
    "2. Use AI to write a short summary of the difference between Unet and Unet++.\n",
    "3. Report the highest value of the IOU metric on the validation set.  Interpret that value in the context of this problem.  What is it telling you about the predicted masks for the cell nuclei?\n",
    "\n",
    "Please number your responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Fine-Tune a YOLO v11 model for Pedestrian Detection\n",
    "\n",
    "YOLO (You Only Look Once) models are a family of object detection models known for their speed and accuracy. Unlike traditional object detection methods that use a sliding window approach, YOLO models frame object detection as a single regression problem, directly predicting bounding boxes and class probabilities from full images in one evaluation. This makes YOLO models extremely fast, making them suitable for real-time applications.\n",
    "\n",
    "YOLO models consist of a single convolutional network that simultaneously predicts multiple bounding boxes and class probabilities for those boxes. The architecture is divided into several key components:\n",
    "\n",
    "1. **Backbone**: This is typically a convolutional neural network (CNN) that extracts essential features from the input image.\n",
    "2. **Neck**: This part of the network aggregates and combines features from different stages of the backbone. It often includes components like Feature Pyramid Networks (FPN) or Path Aggregation Networks (PAN).\n",
    "3. **Head**: The final part of the network, which predicts the bounding boxes, objectness scores, and class probabilities. It usually consists of convolutional layers that output the final detection results.\n",
    "\n",
    "YOLO models are quite easy to load and train because they provide pre-trained weights and a straightforward API for customization and fine-tuning.  The hardest part may be preparing the data in the format that the API expects, but we've done that for you.  \n",
    "\n",
    "You'll need to install two packages.  Copy this into a code cell and run it once on each server you use:\n",
    "```python\n",
    "!pip install ultralytics torchmetrics\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below once to prepare the Penn Fudan Pedestrian dataset in YOLO format.  This dataset uses the same splits we used in the lesson to allow you to compare the results to the Faster R-CNN model we trained there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only need to run this once per platform, but it's safe to run multiple times\n",
    "prepare_penn_fudan_yolo(DATA_PATH)\n",
    "\n",
    "# the dataset will be here:\n",
    "dataset_path = DATA_PATH / \"PennFudanPedYOLO\"\n",
    "\n",
    "# you may wish to set an output path for the model\n",
    "output_path = MODELS_PATH / \"PennFudanPedYOLO\"\n",
    "\n",
    "# the YAML file for the dataset is here:\n",
    "yaml_path = dataset_path / \"dataset.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visit the ultralytics website to learn about YOLO11.  You can watch short video to learn more about it.  Sample code is provided to show you how to load and train a model (use 'yolo11s.pt').  Pass `project=output_path` to `model.train` to store the output in your models directory.   After training you might want to look at some of the images created in that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the following cell to show selected images and boxes from the validation set.  You can replace `indices=selected_indices` with `num_samples=3` to display 3 randomly selected images.  The selected images we chose should align with the images we showed in the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_indices = [28,29,33]\n",
    "display_yolo_predictions(yaml_path, model, indices=selected_indices, show_confidence=True, conf=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the followup questions:\n",
    "\n",
    "1.  Find and plot an image with a false positive box in the validation data.\n",
    "2. How is the process of fine-tuning the YOLO model different than for the Faster R-CNN model in the lesson?  Is it easier or harder?  Why?\n",
    "3.  What did you get for map50 and map50:95 on the validation data with your YOLO model?\n",
    "4.  How do those values compare to values in the lesson?\n",
    "5.  How do the predicted boxes compare qualitatively to the boxes predicted by Faster R-CNN in the lesson?  Do they align better or worse with the ground truth boxes.\n",
    "6.  Thoroughly explain what your map50 value tells you about the performance of your YOLO model at detecting pedestrians in the validation data.\n",
    "\n",
    "Please number your responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
