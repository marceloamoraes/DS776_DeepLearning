{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "In this notebook we'll\n",
    "\n",
    "* List some common applications of NER\n",
    "* Give a brief history of NER\n",
    "* Demonstrate how to setup and fine-tune a DistilBERT model for NER\n",
    "* Discuss some of the issues with using an LLM for an NER task\n",
    "\n",
    "First, make sure your course package is updated for this lesson and homework.  You need to do this once per server, but not once per notebook.  The exact path will depend on where this notebook is in relation to the folder /Lessons/Course_Tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ../Course_Tools/introdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running that cell, you should restart the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of NER\n",
    "\n",
    "I wasn't really familiar with Named Entity Recognition before building this course.  However, after studying it for a bit I realize it's very similar to object detection and instance segmentation in computer vision where we're trying to \"tag\" individual objects in an image.  Now we're doing it with text.  Now that I know more about it I realize that NER is everywhere:\n",
    "\n",
    "- **Information Extraction from Text**\n",
    "  - Identify names of people, places, organizations, and dates in news articles, legal documents, and academic papers.\n",
    "\n",
    "- **Search and Question Answering**\n",
    "  - Improve retrieval and understanding by recognizing key entities in queries and documents (e.g., “Where was Barack Obama born?”).\n",
    "\n",
    "- **Social Media Monitoring**\n",
    "  - Detect mentions of public figures, brands, products, and locations in tweets, posts, and comments for sentiment analysis or moderation.\n",
    "\n",
    "- **Marketing and Trend Analysis**\n",
    "  - Track mentions of brands, competitors, or topics over time to identify emerging trends and customer interests.\n",
    "\n",
    "- **Content Recommendation**\n",
    "  - Extract entities (e.g., movies, products, places) from reviews and user posts to personalize content or advertisements.\n",
    "\n",
    "- **Customer Support Automation**\n",
    "  - Identify product names, user accounts, and issue types in support chats and emails to assist routing and auto-response systems.\n",
    "\n",
    "- **Financial and Business Intelligence**\n",
    "  - Extract company names, stock tickers, monetary values, and events from reports or articles to support decision-making.\n",
    "\n",
    "- **Medical and Clinical Text Analysis**\n",
    "  - Identify diseases, medications, and procedures in clinical notes for tasks like anonymization, coding, or record analysis.\n",
    "\n",
    "- **Legal and Compliance Monitoring**\n",
    "  - Recognize case names, organizations, and laws in legal documents to support research, auditing, or compliance checks.\n",
    "\n",
    "- **Resume and Job Post Parsing**\n",
    "  - Extract structured information such as skills, education, job titles, and companies to streamline recruitment processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chronology of State-of-the-Art Approaches for Named Entity Recognition (NER)**  \n",
    "\n",
    "The evolution of NER closely parallels the evolution of algorithms for text classification.  Early approaches were based on statistical models, then word embeddings and recurrent neural networks, before transformer architectures revolutionized the field since 2017.  \n",
    "\n",
    "Here's a timeline of some of the key advancements in NER:\n",
    "\n",
    "---\n",
    "\n",
    "### **Pre-2010s: Rule-Based Systems and Feature Engineering**  \n",
    "Early NER systems used **hand-crafted rules**, lookup lists (called **gazetteers**), and basic statistical models like **Hidden Markov Models (HMMs)** and **Conditional Random Fields (CRFs)**.  \n",
    "- **HMMs** modeled sequences by predicting the most likely tag (e.g., PERSON, LOCATION) for each word based on probabilities.\n",
    "- **CRFs** improved on HMMs by allowing more flexible features and considering the entire sequence when making predictions.\n",
    "\n",
    "These approaches required heavy manual feature engineering—like marking whether a word is capitalized, its part of speech, or its prefix/suffix.\n",
    "\n",
    "- **1990s–2000s**: Rule-based systems and statistical models dominated tasks like newswire NER.\n",
    "- **2003**: The CoNLL-2003 shared task standardized benchmarks and boosted interest in developing better NER models.\n",
    "\n",
    "---\n",
    "\n",
    "### **2010s: Word Embeddings and Neural Sequence Models**  \n",
    "NER systems improved significantly with the introduction of **word embeddings** like **Word2Vec** and **GloVe**, which represented words in continuous vector space based on context. These embeddings replaced sparse, manual features.\n",
    "\n",
    "- **2013–2015**: **Word2Vec** and **GloVe** made it easier to train neural models for NER.\n",
    "- **2015–2016**: **BiLSTM-CRF** architectures became popular—combining bidirectional LSTMs (which read sentences both forward and backward) with a CRF layer to model dependencies between entity tags.\n",
    "- **2015**: **spaCy** launched as a fast, practical NLP library with built-in NER support, making NER accessible for developers and educators.\n",
    "- **2016–2017**: Character-level embeddings and CNNs were added to improve robustness to spelling variation and rare words.\n",
    "\n",
    "---\n",
    "\n",
    "### **Late 2010s: Contextual Embeddings and Transformers**  \n",
    "NER took a major leap with **contextualized embeddings** from transformer-based models.\n",
    "\n",
    "- **2018**: **ELMo** introduced deep contextualized word representations that vary based on sentence context.\n",
    "- **2018**: **BERT** achieved state-of-the-art NER results by treating NER as a token classification problem using bidirectional transformer layers.\n",
    "- **2019**: **Flair** added character-level contextual embeddings to further improve performance on small or domain-specific datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### **2020s: Prompting and Large Language Models (LLMs)**  \n",
    "Recent NER approaches increasingly use **LLMs** like **GPT-4**, **Claude**, and **Gemini**, which can extract entities using **natural language prompts** instead of token-level supervision.\n",
    "\n",
    "- **2020–2022**: Models like **RoBERTa**, **SpanBERT**, and **LUKE** fine-tuned transformer architectures for better span detection and entity-aware representations.\n",
    "- **spaCy** added support for transformer-based pipelines (e.g., `en_core_web_trf`) to make state-of-the-art NER accessible for production use.\n",
    "- **2023–2025**: Instruction-tuned models like **GLiNER** and general-purpose LLMs now handle **zero-shot or few-shot NER** using prompts like *\"Find all organizations and people in this sentence.\"* These models reduce the need for annotated datasets and allow rapid prototyping for new entity types.\n",
    "\n",
    "  While LLMs offer flexibility and ease of use, they may be less precise than traditional models. Hybrid systems often combine LLMs with structured postprocessing or constrained decoding to improve accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "We'll focus on two of these tools.  We'll fine-tune a BERT model for NER and we'll look at some of the hurdles to using LLMs for NER.  You'll explore both of these topics further in the homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's our main import cell before we dive into the rest of the material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODELS_PATH=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\models\n",
      "DATA_PATH=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\data\n",
      "TORCH_HOME=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\n",
      "HF_HOME=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\n",
      "HF_HUB_CACHE=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\n",
      "Successfully logged in to Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import evaluate # Hugging Face library for evaluation\n",
    "from IPython.display import display\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (AutoTokenizer, AutoModelForTokenClassification, \n",
    "                          TrainingArguments, Trainer, DataCollatorForTokenClassification)\n",
    "\n",
    "# local packages\n",
    "from helpers import (display_ner_html, format_ner_eval_results, evaluate_ner,\n",
    "                     extract_gold_entities) \n",
    "from introdl.utils import config_paths_keys, wrap_print_text\n",
    "from introdl.nlp import llm_generate, llm_configure, llm_list_models\n",
    "\n",
    "print = wrap_print_text(print, width=120) # you can specify the wrap width for all print statements\n",
    "\n",
    "paths = config_paths_keys() # import paths and keys\n",
    "MODELS_PATH = paths['MODELS_PATH']\n",
    "DATA_PATH = paths['DATA_PATH']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset - CoNLL2003 for NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our examples, well use the CoNLL2003 dataset.  It is one of the first widely used benchmarks for Named Entity Recognition (NER). It was introduced as part of the CoNLL-2003 shared task and contains annotated text for four entity types: **PER** (person), **LOC** (location), **ORG** (organization), and **MISC** (miscellaneous). The dataset is derived from Reuters news articles and is structured in the BIO format, making it a standard for evaluating NER models.\n",
    "\n",
    "Multiple versions of the dataset are available in Hugging Face.  We chose \"tomaarsen/conll2003\" because the NER tags are available in BIO format and because the list of possible labels is easy to extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible BIO tags ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load CoNLL2003 dataset (this is not the most well known version of teh dataset, but it is the one that is easiest to load with the datasets library)\n",
    "dataset = load_dataset(\"tomaarsen/conll2003\")\n",
    "BIO_tags_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "print(\"Possible BIO tags\", BIO_tags_list)\n",
    "\n",
    "# delete the pos_tags and chunk_tags columns, as we don't need them\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].remove_columns([\"pos_tags\", \"chunk_tags\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample in the dataset consists of a single sentence or headline.  Here is how it's stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '12', 'document_id': 1, 'sentence_id': 12, 'tokens': ['Only', 'France', 'and', 'Britain', 'backed', 'Fischler',\n",
      "\"'s\", 'proposal', '.'], 'ner_tags': [0, 5, 0, 5, 0, 1, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the tokens are the words in sentence split up by whitespace and punctuation.  The ner_tags correspond to indices of the entity tags in our list.  The next bit of code also shows you how to get the BIO tags corresponding to each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>NER Tags (IDs)</th>\n",
       "      <th>BIO Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Only</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>France</td>\n",
       "      <td>5</td>\n",
       "      <td>B-LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Britain</td>\n",
       "      <td>5</td>\n",
       "      <td>B-LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>backed</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fischler</td>\n",
       "      <td>1</td>\n",
       "      <td>B-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>'s</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>proposal</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Tokens  NER Tags (IDs) BIO Tags\n",
       "0      Only               0        O\n",
       "1    France               5    B-LOC\n",
       "2       and               0        O\n",
       "3   Britain               5    B-LOC\n",
       "4    backed               0        O\n",
       "5  Fischler               1    B-PER\n",
       "6        's               0        O\n",
       "7  proposal               0        O\n",
       "8         .               0        O"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract tokens and ner_tags from dataset[\"train\"][12]\n",
    "tokens = dataset[\"train\"][12][\"tokens\"]\n",
    "ner_tags = dataset[\"train\"][12][\"ner_tags\"]\n",
    "\n",
    "# Map ner_tags to their corresponding BIO tags using label_list\n",
    "bio_tags = [BIO_tags_list[tag] for tag in ner_tags]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\"Tokens\": tokens, \"NER Tags (IDs)\": ner_tags, \"BIO Tags\": bio_tags})\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[spaCy is a whole ecosystem](https://spacy.io/) of tools for NLP that we won't really dive into much in this course, but it's worth a look if you're going to be working in this area.  They provide some great tools for visualization of tagged text.  We've use their package to make a little function called `display_ner_html` which takes lists of tokens, tag IDs, and the list of labels to produce HTML visualizations of the tags.  The function is in helper.py if you're curious.  Here's how we can use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"\n",
       "        line-height: 1.6;\n",
       "        max-width: 120ch;\n",
       "        white-space: normal;\n",
       "        word-wrap: break-word;\n",
       "        font-family: 'Segoe UI', sans-serif;\n",
       "    \"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Only \n",
       "<mark class=\"entity\" style=\"background: #66c2a5; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    France\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-LOC</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #66c2a5; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Britain\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-LOC</span>\n",
       "</mark>\n",
       " backed \n",
       "<mark class=\"entity\" style=\"background: #e78ac3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Fischler\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PER</span>\n",
       "</mark>\n",
       " 's proposal .</div></div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokens and ner_tags were defined in the previous code cell\n",
    "\n",
    "display_ner_html(tokens, ner_tags, BIO_tags_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"\n",
       "        line-height: 1.6;\n",
       "        max-width: 120ch;\n",
       "        white-space: normal;\n",
       "        word-wrap: break-word;\n",
       "        font-family: 'Segoe UI', sans-serif;\n",
       "    \"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #66c2a5; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Germany\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-LOC</span>\n",
       "</mark>\n",
       " 's representative to the \n",
       "<mark class=\"entity\" style=\"background: #8da0cb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    European Union\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-ORG</span>\n",
       "</mark>\n",
       " 's veterinary committee \n",
       "<mark class=\"entity\" style=\"background: #e78ac3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Werner Zwingmann\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PER</span>\n",
       "</mark>\n",
       " said on Wednesday consumers should buy sheepmeat from countries other than \n",
       "<mark class=\"entity\" style=\"background: #66c2a5; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Britain\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-LOC</span>\n",
       "</mark>\n",
       " until the scientific advice was clearer .</div></div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here's another example\n",
    "display_ner_html(dataset[\"train\"][4][\"tokens\"], dataset[\"train\"][4][\"ner_tags\"], BIO_tags_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune DistilBERT for ConNLL2003\n",
    "\n",
    "#### L10_1_Fine-tune_BERT Video\n",
    "\n",
    "<iframe \n",
    "    src=\"https://media.uwex.edu/content/ds/ds776/ds776_l10_1_fine-tune_bert/\" \n",
    "    width=\"800\" \n",
    "    height=\"450\" \n",
    "    style=\"border: 5px solid cyan;\"  \n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "<br>\n",
    "<a href=\"https://media.uwex.edu/content/ds/ds776/ds776_l10_1_fine-tune_bert/\" target=\"_blank\">Open UWEX version of video in new tab</a>\n",
    "<br>\n",
    "<a href=\"https://share.descript.com/view/EgBF1mreyjw\" target=\"_blank\">Open Descript version of video in new tab</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to fine-tune a BERT model so that it can provide similar tagging for new text.  First we'll load a model and its tokenizer.\n",
    "`distilbert-base-cased` is a smaller, faster, and lighter version of BERT that retains 97% of its language understanding capabilities while being 40% smaller. It is case-sensitive, meaning it distinguishes between \"Apple\" and \"apple\" which is useful for NER tasks. It was trained using masked language modeling on the same data as BERT, including the English Wikipedia and BookCorpus, but with a reduced architecture to improve efficiency. \n",
    "\n",
    "Note that we make use of `AutoModelForTokenClassification` which adds a classification head to the backbone the same way we did for transfer learning applications in image classification.  The backbone uses pretrained weights while the classification head weights are randomly initialized and learned during fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"distilbert-base-cased\", num_labels=len(BIO_tags_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "One of the main issues we'll need to deal with is to map the BIO tags to the tokens that are produced by tokenizer that comes with our selected BERT model.  That tokenizer will break some of our words into subwords.  For those subwords we'll introduce an ID of -100 that tells the model not to predict tags for those tokens.\n",
    "\n",
    "The function, `tokenize_and_align_labels` below takes care of aligning the ID tags from the input sequence in the dataset to the output tokens in the tokenizer.  We've included some comments in the code if you want to study it, or you can use an AI to help you walk through the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to align labels with tokens\n",
    "def tokenize_and_align_labels(examples):\n",
    "    # Tokenize the input text (list of tokens) while keeping track of word-to-token alignment\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    \n",
    "    # Initialize a list to store the aligned labels for each example\n",
    "    labels = []\n",
    "    \n",
    "    # Iterate over each example in the batch\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        # Get the word-to-token mapping for the current example\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        \n",
    "        # Initialize variables to track the previous word index and the label IDs\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        # Iterate over the word IDs corresponding to the tokens\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # If the token is a special token (e.g., [CLS], [SEP]), ignore it by assigning -100\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # If the token corresponds to a new word, assign the label of that word\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                # If the token is part of the same word (e.g., subword tokens), ignore it by assigning -100\n",
    "                label_ids.append(-100)\n",
    "            \n",
    "            # Update the previous word index to the current one\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        # Append the aligned label IDs for the current example\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    # Add the aligned labels to the tokenized inputs\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    \n",
    "    # Return the tokenized inputs with aligned labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell demonstrates how our tokenizer works the alignment function to get the tokenization expected by the model and to introduce IDs of -100 for each of the subwords introduced by the tokenizer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before model tokenization:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"\n",
       "        line-height: 1.6;\n",
       "        max-width: 120ch;\n",
       "        white-space: normal;\n",
       "        word-wrap: break-word;\n",
       "        font-family: 'Segoe UI', sans-serif;\n",
       "    \"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">He said a proposal last month by \n",
       "<mark class=\"entity\" style=\"background: #8da0cb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    EU\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-ORG</span>\n",
       "</mark>\n",
       " Farm Commissioner \n",
       "<mark class=\"entity\" style=\"background: #e78ac3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Franz Fischler\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PER</span>\n",
       "</mark>\n",
       " to ban sheep brains , spleens and spinal cords from the human and animal food chains was a highly specific and precautionary move to protect human health .</div></div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After model tokenization:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"\n",
       "        line-height: 1.6;\n",
       "        max-width: 120ch;\n",
       "        white-space: normal;\n",
       "        word-wrap: break-word;\n",
       "        font-family: 'Segoe UI', sans-serif;\n",
       "    \"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #cccccc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    [CLS]\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">IGNORE</span>\n",
       "</mark>\n",
       " He said a proposal last month by \n",
       "<mark class=\"entity\" style=\"background: #8da0cb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    EU\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-ORG</span>\n",
       "</mark>\n",
       " Farm Commissioner \n",
       "<mark class=\"entity\" style=\"background: #e78ac3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Franz Fi\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PER</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #cccccc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ##sch\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">IGNORE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #cccccc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ##ler\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">IGNORE</span>\n",
       "</mark>\n",
       " to ban sheep brains , s \n",
       "<mark class=\"entity\" style=\"background: #cccccc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ##ple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">IGNORE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #cccccc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ##ens\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">IGNORE</span>\n",
       "</mark>\n",
       " and spinal cord \n",
       "<mark class=\"entity\" style=\"background: #cccccc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ##s\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">IGNORE</span>\n",
       "</mark>\n",
       " from the human and animal food chains was a highly specific and pre \n",
       "<mark class=\"entity\" style=\"background: #cccccc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ##ca\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">IGNORE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #cccccc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ##ution\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">IGNORE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #cccccc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ##ary\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">IGNORE</span>\n",
       "</mark>\n",
       " move to protect human health . \n",
       "<mark class=\"entity\" style=\"background: #cccccc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    [SEP]\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">IGNORE</span>\n",
       "</mark>\n",
       "</div></div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the example\n",
    "example = dataset[\"train\"][7]\n",
    "\n",
    "# Wrap in a batch of one for compatibility with tokenize_and_align_labels\n",
    "batch = {\"tokens\": [example[\"tokens\"]], \"ner_tags\": [example[\"ner_tags\"]]}\n",
    "\n",
    "# Apply the tokenization and alignment function\n",
    "tokenized = tokenize_and_align_labels(batch)\n",
    "\n",
    "# Extract and display results\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized[\"input_ids\"][0])\n",
    "labels = tokenized[\"labels\"][0]\n",
    "\n",
    "print((\"Before model tokenization:\\n\"))\n",
    "display_ner_html(dataset[\"train\"][7][\"tokens\"], dataset[\"train\"][7][\"ner_tags\"], BIO_tags_list)\n",
    "print((\"\\nAfter model tokenization:\\n\"))\n",
    "display_ner_html(tokens, labels, BIO_tags_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the tokenizer divided some of the original words into subwords which get assigned an ID of -100 to be ignored by the model.  During training those tokens are ignored by the loss function and the outputs corresponding to those tokens are ignored during model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we fine-tune the model we define a custom metrics function that does two things:\n",
    "1. Uses the `seqeval` package to evaluate entire entity spans (e.g, e.g., `B-LOC`, `I-LOC`, etc. forming `\"New York\"`) instead of evaluating individual labels as we'd do with the scikit-learn metrics.\n",
    "2. Ignores the tokens with IDs of -100 for the evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load seqeval metric\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "# Note if you have a different list of possible tags, you'll need to change the default value of label_list\n",
    "def compute_metrics(p, label_list=BIO_tags_list):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return metric.compute(predictions=true_predictions, references=true_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the actual fine-tuning we use a similar setup to what we did for text classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2634' max='2634' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2634/2634 05:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Loc</th>\n",
       "      <th>Misc</th>\n",
       "      <th>Org</th>\n",
       "      <th>Per</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall F1</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>0.056684</td>\n",
       "      <td>{'precision': 0.9189046866771985, 'recall': 0.949918345127926, 'f1': 0.934154175588865, 'number': 1837}</td>\n",
       "      <td>{'precision': 0.8034744842562432, 'recall': 0.8026030368763557, 'f1': 0.8030385241454151, 'number': 922}</td>\n",
       "      <td>{'precision': 0.8747268754552076, 'recall': 0.8956002982848621, 'f1': 0.8850405305821666, 'number': 1341}</td>\n",
       "      <td>{'precision': 0.9753363228699552, 'recall': 0.9446254071661238, 'f1': 0.9597352454495311, 'number': 1842}</td>\n",
       "      <td>0.907813</td>\n",
       "      <td>0.913161</td>\n",
       "      <td>0.910479</td>\n",
       "      <td>0.984580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.048850</td>\n",
       "      <td>{'precision': 0.9451907576571735, 'recall': 0.9575394665215025, 'f1': 0.9513250405624661, 'number': 1837}</td>\n",
       "      <td>{'precision': 0.8494623655913979, 'recall': 0.8568329718004338, 'f1': 0.8531317494600432, 'number': 922}</td>\n",
       "      <td>{'precision': 0.8949329359165424, 'recall': 0.8956002982848621, 'f1': 0.8952664927320164, 'number': 1341}</td>\n",
       "      <td>{'precision': 0.9627027027027028, 'recall': 0.9668838219326819, 'f1': 0.9647887323943664, 'number': 1842}</td>\n",
       "      <td>0.924453</td>\n",
       "      <td>0.930831</td>\n",
       "      <td>0.927631</td>\n",
       "      <td>0.987598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.046552</td>\n",
       "      <td>{'precision': 0.959061135371179, 'recall': 0.9564507348938487, 'f1': 0.9577541564458981, 'number': 1837}</td>\n",
       "      <td>{'precision': 0.8449531737773153, 'recall': 0.8806941431670282, 'f1': 0.8624535315985129, 'number': 922}</td>\n",
       "      <td>{'precision': 0.894659839063643, 'recall': 0.9120059656972409, 'f1': 0.9032496307237814, 'number': 1341}</td>\n",
       "      <td>{'precision': 0.9678824169842134, 'recall': 0.9652551574375678, 'f1': 0.966567001902691, 'number': 1842}</td>\n",
       "      <td>0.928798</td>\n",
       "      <td>0.937395</td>\n",
       "      <td>0.933076</td>\n",
       "      <td>0.988552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2634, training_loss=0.06429638274879554, metrics={'train_runtime': 302.6591, 'train_samples_per_second': 139.176, 'train_steps_per_second': 8.703, 'total_flos': 525319502290632.0, 'train_loss': 0.06429638274879554, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= MODELS_PATH / \"distilbert-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    "    seed=42,\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [216/216 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set evaluation results:\n",
      "{'eval_loss': 0.12091150879859924, 'eval_LOC': {'precision': 0.9082240762812872, 'recall': 0.9136690647482014, 'f1':\n",
      "0.9109384339509861, 'number': 1668}, 'eval_MISC': {'precision': 0.7157622739018088, 'recall': 0.7891737891737892, 'f1':\n",
      "0.7506775067750678, 'number': 702}, 'eval_ORG': {'precision': 0.8491555037856727, 'recall': 0.8777844671884407, 'f1':\n",
      "0.8632326820603909, 'number': 1661}, 'eval_PER': {'precision': 0.9567126725219574, 'recall': 0.943104514533086, 'f1':\n",
      "0.9498598567424478, 'number': 1617}, 'eval_overall_precision': 0.8781884435190005, 'eval_overall_recall':\n",
      "0.8960694050991501, 'eval_overall_f1': 0.8870388221891157, 'eval_overall_accuracy': 0.977969204264025, 'eval_runtime':\n",
      "6.8877, 'eval_samples_per_second': 501.332, 'eval_steps_per_second': 31.36, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate on test set\n",
    "results_BERT = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(\"\\nTest set evaluation results:\")\n",
    "print(results_BERT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's some ugly output!  Let's put it in a data frame with some formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Number</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.9082</td>\n",
       "      <td>0.9137</td>\n",
       "      <td>0.9109</td>\n",
       "      <td>1668.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.7158</td>\n",
       "      <td>0.7892</td>\n",
       "      <td>0.7507</td>\n",
       "      <td>702.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.8492</td>\n",
       "      <td>0.8778</td>\n",
       "      <td>0.8632</td>\n",
       "      <td>1661.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.9567</td>\n",
       "      <td>0.9431</td>\n",
       "      <td>0.9499</td>\n",
       "      <td>1617.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Overall</td>\n",
       "      <td>0.8782</td>\n",
       "      <td>0.8961</td>\n",
       "      <td>0.8870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Entity  Precision  Recall      F1  Number  Accuracy\n",
       "0      LOC     0.9082  0.9137  0.9109  1668.0       NaN\n",
       "1     MISC     0.7158  0.7892  0.7507   702.0       NaN\n",
       "2      ORG     0.8492  0.8778  0.8632  1661.0       NaN\n",
       "3      PER     0.9567  0.9431  0.9499  1617.0       NaN\n",
       "4  Overall     0.8782  0.8961  0.8870     NaN     0.978"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_results_BERT = format_ner_eval_results(results_BERT)\n",
    "display(df_results_BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better!  You can think of f1 as a balanced version of accuracy.  We can see that the model does a great job on identifying people and is also good at identifying locations and organizations.  It doesn't do quite as well as identifying miscellaneous entities (but I'm not sure what those are supposed to be either ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the model to do inference by making predictions on new text.  The function below is also included in the helpers.py file, but we include it here so you can study it and see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ner_tags(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenizes and predicts NER tags for the given text using a Hugging Face model.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input sentence (e.g., \"Barack Obama was born in Hawaii\").\n",
    "        model: A Hugging Face token classification model (e.g., DistilBERT).\n",
    "        tokenizer: The tokenizer corresponding to the model.\n",
    "\n",
    "    Returns:\n",
    "        tokens (List[str]): Original word tokens from the input text.\n",
    "        predicted_tag_ids (List[int]): One predicted tag index per word (subwords/specials skipped).\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Split the input text into whitespace-separated words\n",
    "    words = text.split()\n",
    "\n",
    "    # Step 2: Tokenize the list of words and retain word alignment\n",
    "    inputs = tokenizer(words, return_tensors=\"pt\", is_split_into_words=True).to(model.device)\n",
    "\n",
    "    # Step 3: Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Step 4: Convert logits to predicted label indices\n",
    "    predictions = torch.argmax(outputs.logits, dim=2)[0].cpu().numpy()\n",
    "\n",
    "    # Step 5: Get word IDs for each token\n",
    "    word_ids = inputs.word_ids(batch_index=0)\n",
    "\n",
    "    # Step 6: Extract one prediction per word (first subword only)\n",
    "    predicted_tag_ids = []\n",
    "    seen_words = set()\n",
    "    for token_idx, word_idx in enumerate(word_ids):\n",
    "        if word_idx is not None and word_idx not in seen_words:\n",
    "            predicted_tag_ids.append(int(predictions[token_idx]))\n",
    "            seen_words.add(word_idx)\n",
    "        # skip subwords and special tokens\n",
    "\n",
    "    # Step 7: Return the original words and corresponding predicted tags\n",
    "    return words, predicted_tag_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll apply it to some example text we copied from the internet (about GPT-4o's new image generation capability).  First we'll load the model from our last checkpoint so that we don't need to retrain the model to make predictions.  You may have to adjust the path to the checkpoint if you changed the traiing or save path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model from the final checkpoint\n",
    "final_checkpoint_path = MODELS_PATH / \"distilbert-ner\" / \"checkpoint-2634\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(final_checkpoint_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(final_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It’s', 'only', 'been', 'a', 'day', 'since', 'ChatGPT’s', 'new', 'AI', 'image', 'generator', 'went', 'live,', 'and',\n",
      "'social', 'media', 'feeds', 'are', 'already', 'flooded', 'with', 'AI-generated', 'memes', 'in', 'the', 'style', 'of',\n",
      "'Studio', 'Ghibli,', 'the', 'cult-favorite', 'Japanese', 'animation', 'studio', 'behind', 'blockbuster', 'films',\n",
      "'such', 'as', '“My', 'Neighbor', 'Totoro”', 'and', '“Spirited', 'Away.”', 'In', 'the', 'last', '24', 'hours,', 'we’ve',\n",
      "'seen', 'AI-generated', 'images', 'representing', 'Studio', 'Ghibli', 'versions', 'of', 'Elon', 'Musk,', '“The', 'Lord',\n",
      "'of', 'the', 'Rings“,', 'and', 'President', 'Donald', 'Trump.', 'OpenAI', 'CEO', 'Sam', 'Altman', 'even', 'seems', 'to',\n",
      "'have', 'made', 'his', 'new', 'profile', 'picture', 'a', 'Studio', 'Ghibli-style', 'image,', 'presumably', 'made',\n",
      "'with', 'GPT-4o’s', 'native', 'image', 'generator.', 'Users', 'seem', 'to', 'be', 'uploading', 'existing', 'images',\n",
      "'and', 'pictures', 'into', 'ChatGPT', 'and', 'asking', 'the', 'chatbot', 'to', 're-create', 'it', 'in', 'new',\n",
      "'styles.'] [0, 0, 0, 0, 0, 0, 3, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 7, 0, 0, 0, 0,\n",
      "0, 0, 0, 7, 8, 8, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 7, 8, 0, 0, 1, 2, 7, 8, 8, 8, 8, 0, 0, 1, 2, 3, 0, 1, 2, 0, 0,\n",
      "0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "example_text = \"\"\"\n",
    "It’s only been a day since ChatGPT’s new AI image generator went live, and social media feeds are already flooded with AI-generated memes in the style of Studio Ghibli, the cult-favorite Japanese animation studio behind blockbuster films such as “My Neighbor Totoro” and “Spirited Away.”\n",
    "\n",
    "In the last 24 hours, we’ve seen AI-generated images representing Studio Ghibli versions of Elon Musk, “The Lord of the Rings“, and President Donald Trump. OpenAI CEO Sam Altman even seems to have made his new profile picture a Studio Ghibli-style image, presumably made with GPT-4o’s native image generator. Users seem to be uploading existing images and pictures into ChatGPT and asking the chatbot to re-create it in new styles.\n",
    "\"\"\"\n",
    "\n",
    "tokens, tags = predict_ner_tags(example_text, model, tokenizer)\n",
    "print(tokens,tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the raw output is kind of difficult to interpret, but we can easily visualize it with `display_ner_html`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"\n",
       "        line-height: 1.6;\n",
       "        max-width: 120ch;\n",
       "        white-space: normal;\n",
       "        word-wrap: break-word;\n",
       "        font-family: 'Segoe UI', sans-serif;\n",
       "    \"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">It’s only been a day since \n",
       "<mark class=\"entity\" style=\"background: #8da0cb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ChatGPT’s\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-ORG</span>\n",
       "</mark>\n",
       " new \n",
       "<mark class=\"entity\" style=\"background: #fc8d62; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
       "</mark>\n",
       " image generator went live, and social media feeds are already flooded with AI-generated memes in the style of \n",
       "<mark class=\"entity\" style=\"background: #8da0cb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Studio Ghibli,\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-ORG</span>\n",
       "</mark>\n",
       " the cult-favorite \n",
       "<mark class=\"entity\" style=\"background: #fc8d62; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Japanese\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
       "</mark>\n",
       " animation studio behind blockbuster films such as \n",
       "<mark class=\"entity\" style=\"background: #fc8d62; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    “My Neighbor Totoro”\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #fc8d62; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    “Spirited Away.”\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
       "</mark>\n",
       " In the last 24 hours, we’ve seen \n",
       "<mark class=\"entity\" style=\"background: #fc8d62; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI-generated\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
       "</mark>\n",
       " images representing \n",
       "<mark class=\"entity\" style=\"background: #fc8d62; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Studio Ghibli\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
       "</mark>\n",
       " versions of \n",
       "<mark class=\"entity\" style=\"background: #e78ac3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Elon Musk,\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PER</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #fc8d62; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    “The Lord of the Rings“,\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
       "</mark>\n",
       " and President \n",
       "<mark class=\"entity\" style=\"background: #e78ac3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Donald Trump.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PER</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #8da0cb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    OpenAI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-ORG</span>\n",
       "</mark>\n",
       " CEO \n",
       "<mark class=\"entity\" style=\"background: #e78ac3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sam Altman\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PER</span>\n",
       "</mark>\n",
       " even seems to have made his new profile picture a \n",
       "<mark class=\"entity\" style=\"background: #fc8d62; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Studio Ghibli-style\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
       "</mark>\n",
       " image, presumably made with \n",
       "<mark class=\"entity\" style=\"background: #fc8d62; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    GPT-4o’s\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
       "</mark>\n",
       " native image generator. Users seem to be uploading existing images and pictures into \n",
       "<mark class=\"entity\" style=\"background: #8da0cb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ChatGPT\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-ORG</span>\n",
       "</mark>\n",
       " and asking the chatbot to re-create it in new styles.</div></div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_ner_html(tokens, tags, BIO_tags_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems pretty amazing for entity recognition on text the model has never seen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER by Zero-Shot LLM Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L10_1_LLM_NER Video\n",
    "\n",
    "<iframe \n",
    "    src=\"https://media.uwex.edu/content/ds/ds776/ds776_l10_1_llm_ner/\" \n",
    "    width=\"800\" \n",
    "    height=\"450\" \n",
    "    style=\"border: 5px solid cyan;\"  \n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "<br>\n",
    "<a href=\"https://media.uwex.edu/content/ds/ds776/ds776_l10_1_llm_ner/\" target=\"_blank\">Open UWEX version of video in new tab</a>\n",
    "<br>\n",
    "<a href=\"https://share.descript.com/view/EgBF1mreyjw\" target=\"_blank\">Open Descript version of video in new tab</a>\n",
    "\n",
    "In this section we'll explore using LLMs for NER.  LLMs can do this quite well, but there are some differences to be aware of though.  LLMs are naturally better at extracting spans (the relevant words for each identified entity) or structured output, not token-level labeling, because:\n",
    "\n",
    "* The process text holistically, not token-by-token.\n",
    "* There's no inherent token alignment.\n",
    "* They can hallucinate or skip tokens when generating lists.\n",
    "* The extracted spans may not exactly match the strings in the text, e.g. \"ChatGPT's\" gets extracted as \"ChatGPT\"\n",
    "\n",
    "When we use an LLM to extract entities, we'll get lists of spans of each type.  You'll need to prompt carefully:\n",
    "* try to get the LLM to extract the entities as they appear in the text\n",
    "* you may need to provide examples or explanations of the entity types\n",
    "\n",
    "When we evaluate the results, we won't be able to compare token by token as we did above for the output of our BERT model (that kind of evaluation is similar to evaluating semantic segmentation results where we can compare every pixel in the image to every pixel in the mask).  Instead we can just determine if each found each entity and whether it had the correct entity type.  It will help to use \"fuzzy\" matching which doesn't require exacty matching of strings to accout for misspellings and different presentations of words.\n",
    "\n",
    "**Note:**  It's possible to use an LLM to produce token-level tags for each token through a combination of careful prompting and post-processing, but we'll stick with the simpler problem of identifying entities without identifying their positions in the text which is adequate for many applications.\n",
    "\n",
    "We'll use `llm_generate` as we've done previously.    Here's the list of models that are easy to use with `llm_generate`.  You can adjust the code below to use other models, or the Groq or Together.AI APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      " llama-3p2-3B => HuggingFace: unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit\n",
      " llama-3p1-8B => HuggingFace: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\n",
      " mistral-7B => HuggingFace: unsloth/mistral-7b-instruct-v0.3-bnb-4bit\n",
      " qwen-2p5-3B => HuggingFace: unsloth/Qwen2.5-3B-Instruct-bnb-4bit\n",
      " qwen-2p5-7B => HuggingFace: unsloth/Qwen2.5-7B-Instruct-bnb-4bit\n",
      " gemini-flash-lite => needs GEMINI_API_KEY\n",
      " gemini-flash => needs GEMINI_API_KEY\n",
      " gpt-4o => needs OPENAI_API_KEY\n",
      " gpt-4o-mini => needs OPENAI_API_KEY\n"
     ]
    }
   ],
   "source": [
    "llm_list_models();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an LLM for NER - The Basics\n",
    "\n",
    "We'll start by crafting a prompt and asking a local model to identify the CoNLL entities (PER, LOC, ORG, MISC) in the example text from the last section.  We're going to specify the entity types in the prompt and try to get the model to produce JSON ouput.  JSON is output that's been formatted like a Python dictionary.  Let's see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Loading model: unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit (this may take a while)...\n",
      "🟢 Model unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit loaded successfully.\n",
      "\n",
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant for named entity recognition. You return entity spans in JSON.user\n",
      "\n",
      "Extract the following named entities from the text below, if they appear:\n",
      "- PER (Person)\n",
      "- ORG (Organization)\n",
      "- LOC (Location)\n",
      "- MISC (Miscellaneous)\n",
      "\n",
      "Only include named entities that are explicitly mentioned in the text — do not infer or guess.\n",
      "Return each entity **exactly as it appears in the text**, preserving casing and punctuation.\n",
      "\n",
      "Return the result as a JSON object in the format:\n",
      "{{\n",
      "  \"PER\": [...],\n",
      "  \"ORG\": [...],\n",
      "  \"LOC\": [...],\n",
      "  \"MISC\": [...]\n",
      "}}\n",
      "\n",
      "Return only the JSON object, nothing else.\n",
      "\n",
      "Text: It’s only been a day since ChatGPT’s new AI image generator went live, and social media feeds\n",
      "are already flooded with AI-generated memes in the style of Studio Ghibli, the cult-favorite\n",
      "Japanese animation studio behind blockbuster films such as “My Neighbor Totoro” and “Spirited Away.”\n",
      "\n",
      "In the last 24 hours, we’ve seen AI-generated images representing Studio Ghibli versions of Elon Musk,\n",
      "“The Lord of the Rings“, and President Donald Trump. OpenAI CEO Sam Altman even seems to have made his\n",
      "new profile picture a Studio Ghibli-style image, presumably made with GPT-4o’s native image generator.\n",
      "Users seem to be uploading existing images and pictures into ChatGPT and asking the chatbot to re-create\n",
      "it in new styles.\n",
      "The Entities JSON:assistant\n",
      "\n",
      "{\"PER\":[\"ChatGPT\", \"Elon Musk\", \"Sam Altman\", \"Donald Trump\"], \"ORG\":[\"Studio Ghibli\", \"OpenAI\"], \"LOC\":[], \"MISC\":[]}\n"
     ]
    }
   ],
   "source": [
    "llm_config = llm_configure(\"llama-3p1-8B\")\n",
    "\n",
    "# System instruction for the model\n",
    "system_instruct = \"You are a helpful assistant for named entity recognition. You return entity spans in JSON.\"\n",
    "\n",
    "# Example Text\n",
    "example_text = \"\"\"It’s only been a day since ChatGPT’s new AI image generator went live, and social media feeds \n",
    "are already flooded with AI-generated memes in the style of Studio Ghibli, the cult-favorite \n",
    "Japanese animation studio behind blockbuster films such as “My Neighbor Totoro” and “Spirited Away.”\n",
    "\n",
    "In the last 24 hours, we’ve seen AI-generated images representing Studio Ghibli versions of Elon Musk, \n",
    "“The Lord of the Rings“, and President Donald Trump. OpenAI CEO Sam Altman even seems to have made his \n",
    "new profile picture a Studio Ghibli-style image, presumably made with GPT-4o’s native image generator. \n",
    "Users seem to be uploading existing images and pictures into ChatGPT and asking the chatbot to re-create \n",
    "it in new styles.\"\"\"\n",
    "\n",
    "# Prompt for CoNLL2003-style entity extraction\n",
    "prompt = \"\"\"\n",
    "Extract the following named entities from the text below, if they appear:\n",
    "- PER (Person)\n",
    "- ORG (Organization)\n",
    "- LOC (Location)\n",
    "- MISC (Miscellaneous)\n",
    "\n",
    "Only include named entities that are explicitly mentioned in the text — do not infer or guess. \n",
    "Return each entity **exactly as it appears in the text**, preserving casing and punctuation.\n",
    "\n",
    "Return the result as a JSON object in the format:\n",
    "{{\n",
    "  \"PER\": [...],\n",
    "  \"ORG\": [...],\n",
    "  \"LOC\": [...],\n",
    "  \"MISC\": [...]\n",
    "}}\n",
    "\n",
    "Return only the JSON object, nothing else.\n",
    "\n",
    "Text: \"\"\" + example_text + \" \\nThe Entities JSON:\"\n",
    "\n",
    "response = llm_generate(llm_config, prompt, system_prompt = system_instruct, search_strategy='deterministic', remove_input_prompt=False)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note some things about the output:\n",
    "* `llm_generate` may fail to remove the prompt from the output. We forced it to keep it above by passing `remove_input_prompt = False`, but sometimes it fails because our cleaning algorithm doesn't correctly detect the input prompt in the output.  You should generally use `remove_input_prompt=True` or just leave it out since it defaults to True.\n",
    "* It mis-identified \"ChatGPT\" as a person.\n",
    "* It also returned the the span as \"ChatGPT\" instead of \"ChatGPT's\" as it occurs in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fix the first issue by passing a `split_string` to `llm_generate` which will delete all the text up to the string.  We might be able to fix the second issue by providing examples (few-shot prompting) or more careful instructions to the LLM.  The third issue is why we'll need to use some inexact matching to match predicted spans with the input text.  \n",
    "\n",
    "First let's see how to get rid of that input prompt if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"PER\":[\"ChatGPT\", \"Elon Musk\", \"Sam Altman\", \"Donald Trump\"], \"ORG\":[\"Studio Ghibli\", \"OpenAI\"], \"LOC\":[], \"MISC\":[]}\n"
     ]
    }
   ],
   "source": [
    "response = llm_generate(llm_config, prompt, system_prompt = system_instruct, \n",
    "                        search_strategy='deterministic', split_string='JSON:assistant')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better.  You may not need the split_string with some LLMs (particularly the API-based LLMs) or you may need to adjust it for different models.  \n",
    "\n",
    "Finally, the output is still a string, but we'd like to load that string as an actual dictionary. We can use `json.loads` to load the JSON formatted string as a dictionary in Python.  Some LLMs, like Gemini, will return the output with Markdown formatting like this:\n",
    "\n",
    "<pre>\n",
    "```json\n",
    "{\"PER\":[\"ChatGPT\", \"Elon Musk\", \"Sam Altman\", \"Donald Trump\"], \"ORG\":[\"Studio Ghibli\", \"OpenAI\"], \"LOC\":[], \"MISC\":[]}\n",
    "```\n",
    "</pre>\n",
    "\n",
    "So we may need to strip those extra characters away before using `json.loads`.  Here's a little function to do both of those things.  It's also in helpers.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_extractor(text):\n",
    "    # Extract the JSON object from the response\n",
    "    try:\n",
    "        text = text.strip(\"```json\").strip(\"```\").strip()\n",
    "        json_object = json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        json_object = {\"error\": \"Could not parse JSON\"}\n",
    "    return json_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PER': ['ChatGPT', 'Elon Musk', 'Sam Altman', 'Donald Trump'],\n",
       " 'ORG': ['Studio Ghibli', 'OpenAI'],\n",
       " 'LOC': [],\n",
       " 'MISC': []}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = json_extractor(response)\n",
    "entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we review our example_text with the tags predicted by our BERT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"\n",
       "        line-height: 1.6;\n",
       "        max-width: 120ch;\n",
       "        white-space: normal;\n",
       "        word-wrap: break-word;\n",
       "        font-family: 'Segoe UI', sans-serif;\n",
       "    \"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">It’s only been a day since \n",
       "<mark class=\"entity\" style=\"background: #8da0cb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ChatGPT’s\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-ORG</span>\n",
       "</mark>\n",
       " new \n",
       "<mark class=\"entity\" style=\"background: #fc8d62; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
       "</mark>\n",
       " image generator went live, and social media feeds are already flooded with AI-generated memes in the style of \n",
       "<mark class=\"entity\" style=\"background: #8da0cb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Studio Ghibli,\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-ORG</span>\n",
       "</mark>\n",
       " the cult-favorite \n",
       "<mark class=\"entity\" style=\"background: #fc8d62; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Japanese\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
       "</mark>\n",
       " animation studio behind blockbuster films such as \n",
       "<mark class=\"entity\" style=\"background: #fc8d62; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    “My Neighbor Totoro”\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #fc8d62; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    “Spirited Away.”\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
       "</mark>\n",
       " In the last 24 hours, we’ve seen \n",
       "<mark class=\"entity\" style=\"background: #fc8d62; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI-generated\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
       "</mark>\n",
       " images representing \n",
       "<mark class=\"entity\" style=\"background: #fc8d62; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Studio Ghibli\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
       "</mark>\n",
       " versions of \n",
       "<mark class=\"entity\" style=\"background: #e78ac3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Elon Musk,\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PER</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #fc8d62; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    “The Lord of the Rings“,\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
       "</mark>\n",
       " and President \n",
       "<mark class=\"entity\" style=\"background: #e78ac3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Donald Trump.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PER</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #8da0cb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    OpenAI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-ORG</span>\n",
       "</mark>\n",
       " CEO \n",
       "<mark class=\"entity\" style=\"background: #e78ac3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sam Altman\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PER</span>\n",
       "</mark>\n",
       " even seems to have made his new profile picture a \n",
       "<mark class=\"entity\" style=\"background: #fc8d62; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Studio Ghibli-style\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
       "</mark>\n",
       " image, presumably made with \n",
       "<mark class=\"entity\" style=\"background: #fc8d62; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    GPT-4o’s\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
       "</mark>\n",
       " native image generator. Users seem to be uploading existing images and pictures into \n",
       "<mark class=\"entity\" style=\"background: #8da0cb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ChatGPT\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-ORG</span>\n",
       "</mark>\n",
       " and asking the chatbot to re-create it in new styles.</div></div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# you must have run the BERT model above on the example_text\n",
    "display_ner_html(tokens, tags, BIO_tags_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our LLM approach didn't tag any \"MISC\" entities but the BERT model identified a bunch.  The BERT model also incorrectly identifed \"Elon\" as a \"MISC\" entity instead of identify \"Elon Musk\" as a \"PER\".  We may be able to improve our prompt by providing examples of \"MISC\" or by giving better instructions.  We'll leave that a homework exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an LLM for NER - Streamlining the Process\n",
    "\n",
    "Similar to the way we made `llm_text_classifier` for text classification, we'll put our pipeline together here in a single function that expects us to input a list of texts to be tagged and outputs a list of entity dictionaries.  \n",
    "\n",
    "If you have to do a lot of this sort of work you should explore [LangChain](https://www.langchain.com/) which in ecosystem of tools for developing applications powered by LLMs.  If you're curious check out the [documentation here](https://python.langchain.com/docs/introduction/).  Look at the tutorial for text classification to see how it compares to what we did in Lesson 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_ner_extractor(llm_config,\n",
    "                      texts,\n",
    "                      system_prompt,\n",
    "                      prompt_template,\n",
    "                      batch_size=1,\n",
    "                      estimate_cost=False,\n",
    "                      rate_limit=None,\n",
    "                      split_string=None,\n",
    "                      return_raw=False):\n",
    "    \"\"\"\n",
    "    Extract named entities using a Large Language Model (LLM) in zero-shot fashion.\n",
    "\n",
    "    Args:\n",
    "        llm_config (ModelConfig): Configuration for the LLM.\n",
    "        texts (list of str): List of input texts to process.\n",
    "        system_prompt (str): System prompt guiding the LLM behavior.\n",
    "        prompt_template (str): Template to construct the user prompt for each text.\n",
    "        batch_size (int, optional): Batch size for local LLMs. Defaults to 1.\n",
    "        estimate_cost (bool, optional): Estimate LLM cost. Defaults to False.\n",
    "        rate_limit (int, optional): Throttle requests for API models. Defaults to None.\n",
    "        split_string (str, optional): String to split the LLM output. Defaults to None.\n",
    "        return_raw (bool, optional): Whether to return raw LLM outputs. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        list of dict: List of JSON objects containing extracted entities for each input text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Create user prompts by formatting the prompt template with each input text.\n",
    "    # This ensures that each text is passed to the LLM with the same structure.\n",
    "    user_prompts = [prompt_template.format(text=text) for text in texts]\n",
    "\n",
    "    # Step 2: Generate raw outputs from the LLM using the provided configuration and prompts.\n",
    "    # The `llm_generate` function sends the prompts to the LLM and retrieves the responses.\n",
    "    raw_outputs = llm_generate(llm_config,\n",
    "                               user_prompts,\n",
    "                               system_prompt=system_prompt,\n",
    "                               search_strategy='deterministic',  # Ensures consistent outputs.\n",
    "                               batch_size=batch_size,  # Number of prompts processed at once.\n",
    "                               estimate_cost=estimate_cost,  # Optionally estimate the cost of LLM usage.\n",
    "                               rate_limit=rate_limit,  # Throttle requests if needed.\n",
    "                               split_string=split_string)  # Optionally split the output.\n",
    "\n",
    "    # Step 3: If the user wants raw outputs, return them directly without further processing.\n",
    "    if return_raw:\n",
    "        return raw_outputs\n",
    "\n",
    "    # Step 4: Process the raw outputs to extract JSON objects.\n",
    "    # Initialize an empty list to store the processed JSON outputs.\n",
    "    json_outputs = []\n",
    "    for output in raw_outputs:\n",
    "        try:\n",
    "            # Step 4.1: Clean the output by removing any extra formatting (e.g., Markdown code blocks).\n",
    "            output = output.strip(\"```json\").strip(\"```\").strip()\n",
    "\n",
    "            # Step 4.2: Parse the cleaned output as a JSON object and append it to the list.\n",
    "            json_outputs.append(json.loads(output))\n",
    "        except json.JSONDecodeError:\n",
    "            # Step 4.3: If parsing fails, append an error message along with the raw output.\n",
    "            json_outputs.append({\"Error\": \"Could not parse JSON\", \"raw_output\": output})\n",
    "\n",
    "    # Step 5: Return the list of JSON objects containing the extracted entities.\n",
    "    return json_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll apply `llm_ner_extractor` to the first 100 texts in the validation set to extract the entity dictionaries.  We'll use the `gemini-flash-lite` model which requires a Gemini API key.  If you're using the free version you may need to pass `rate_limit=30` to `llm_ner_extractor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: CRICKET - LEICESTERSHIRE TAKE OVER AT TOP AFTER INNINGS VICTORY .\n",
      "The Entities JSON:\n",
      "{'PER': [], 'ORG': ['LEICESTERSHIRE'], 'LOC': [], 'MISC': []}\n",
      "\n",
      "\n",
      "Text: LONDON 1996-08-30\n",
      "The Entities JSON:\n",
      "{'PER': [], 'ORG': [], 'LOC': ['LONDON'], 'MISC': []}\n",
      "\n",
      "\n",
      "Text: West Indian all-rounder Phil Simmons took four for 38 on Friday as Leicestershire beat Somerset by an innings and\n",
      "39 runs in two days to take over at the head of the county championship .\n",
      "The Entities JSON:\n",
      "{'PER': ['Phil Simmons'], 'ORG': ['Leicestershire', 'Somerset'], 'LOC': [], 'MISC': []}\n",
      "\n",
      "\n",
      "Text: Their stay on top , though , may be short-lived as title rivals Essex , Derbyshire and Surrey all closed in on\n",
      "victory while Kent made up for lost time in their rain-affected match against Nottinghamshire .\n",
      "The Entities JSON:\n",
      "{'PER': [], 'ORG': ['Essex', 'Derbyshire', 'Surrey', 'Kent', 'Nottinghamshire'], 'LOC': [], 'MISC': []}\n",
      "\n",
      "\n",
      "Text: After bowling Somerset out for 83 on the opening morning at Grace Road , Leicestershire extended their first\n",
      "innings by 94 runs before being bowled out for 296 with England discard Andy Caddick taking three for 83 .\n",
      "The Entities JSON:\n",
      "{'PER': ['Andy Caddick'], 'ORG': ['Somerset', 'Leicestershire', 'England'], 'LOC': ['Grace Road'], 'MISC': []}\n",
      "\n",
      "\n",
      "Text: Trailing by 213 , Somerset got a solid start to their second innings before Simmons stepped in to bundle them out\n",
      "for 174 .\n",
      "The Entities JSON:\n",
      "{'PER': ['Simmons'], 'ORG': ['Somerset'], 'LOC': [], 'MISC': []}\n",
      "\n",
      "\n",
      "Text: Essex , however , look certain to regain their top spot after Nasser Hussain and Peter Such gave them a firm grip\n",
      "on their match against Yorkshire at Headingley .\n",
      "The Entities JSON:\n",
      "{'PER': ['Nasser Hussain', 'Peter Such'], 'ORG': ['Essex', 'Yorkshire'], 'LOC': ['Headingley'], 'MISC': []}\n",
      "\n",
      "\n",
      "Text: Hussain , considered surplus to England 's one-day requirements , struck 158 , his first championship century of\n",
      "the season , as Essex reached 372 and took a first innings lead of 82 .\n",
      "The Entities JSON:\n",
      "{'PER': ['Hussain'], 'ORG': ['England', 'Essex'], 'LOC': [], 'MISC': []}\n",
      "\n",
      "\n",
      "Text: By the close Yorkshire had turned that into a 37-run advantage but off-spinner Such had scuttled their hopes ,\n",
      "taking four for 24 in 48 balls and leaving them hanging on 119 for five and praying for rain .\n",
      "The Entities JSON:\n",
      "{'PER': ['Such'], 'ORG': ['Yorkshire'], 'LOC': [], 'MISC': []}\n",
      "\n",
      "\n",
      "Text: At the Oval , Surrey captain Chris Lewis , another man dumped by England , continued to silence his critics as he\n",
      "followed his four for 45 on Thursday with 80 not out on Friday in the match against Warwickshire .\n",
      "The Entities JSON:\n",
      "{'PER': ['Chris Lewis'], 'ORG': ['Surrey', 'England', 'Warwickshire'], 'LOC': ['Oval'], 'MISC': []}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm_config = llm_configure('gemini-flash-lite')\n",
    "\n",
    "# Extract N examples from the validation split of CoNLL2003\n",
    "N = 100\n",
    "subset = dataset[\"validation\"].select(range(N))\n",
    "\n",
    "texts = [' '.join(tokens) for tokens in subset[\"tokens\"]] # Convert tokens to text\n",
    "\n",
    "# System instruction for the model\n",
    "system_instruct = \"You are a helpful assistant for named entity recognition. You return entity spans in JSON.\"\n",
    "\n",
    "# Prompt template adapted for CoNLL2003-style entity extraction.  \n",
    "# You must keep {text} in the template for the text to be inserted.\n",
    "prompt_template = \"\"\"\n",
    "Extract the following named entities from the text below, if they appear:\n",
    "- PER (Person)\n",
    "- ORG (Organization)\n",
    "- LOC (Location)\n",
    "- MISC (Miscellaneous)\n",
    "\n",
    "Only include named entities that are explicitly mentioned in the text — do not infer or guess. \n",
    "Return each entity **exactly as it appears in the text**, preserving casing and punctuation.\n",
    "\n",
    "Return the result as a JSON object in the format:\n",
    "{{\n",
    "  \"PER\": [...],\n",
    "  \"ORG\": [...],\n",
    "  \"LOC\": [...],\n",
    "  \"MISC\": [...]\n",
    "}}\n",
    "\n",
    "Return only the JSON object, nothing else.\n",
    "\n",
    "Text: {text}\n",
    "The Entities JSON:\n",
    "\"\"\"\n",
    "\n",
    "# Used to split off the assistant output from the JSON (if needed)\n",
    "split_string = \"JSON:assistant\"\n",
    "\n",
    "# Call the LLM-based NER extractor\n",
    "predicted_entities = llm_ner_extractor(\n",
    "    llm_config,\n",
    "    texts,\n",
    "    system_instruct,\n",
    "    prompt_template,\n",
    "    batch_size=10,\n",
    "    estimate_cost=False,\n",
    "    rate_limit=None,\n",
    "    split_string=split_string\n",
    ")\n",
    "\n",
    "# Display the first few predictions for inspection\n",
    "for i, text in enumerate(texts[:10]):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(\"The Entities JSON:\")\n",
    "    print(predicted_entities[i])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look to see if there were any problems extracting JSON from the LLM output.  We can count the number of output dictionaries that include 'Error' as a key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dictionaries with 'Error' as a key: 0\n"
     ]
    }
   ],
   "source": [
    "error_count = sum(1 for prediction in predicted_entities if 'Error' in prediction)\n",
    "print(f\"Number of dictionaries with 'Error' as a key: {error_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great.  We were able to successfully extract JSON from every response.  Let's now evaluate the performance.  Since we're not comparing tags token-by-token what we'll do is:\n",
    "\n",
    "1.  Use the token-by-token tags in the dataset to compute an entity dictionary for each input text.\n",
    "\n",
    "2.  Compare the predicted entity dictionary to the \"gold\" entity dictionary for each example using fuzzy matching (inexact string matches).  In the context of NER the ground-truth labels are sometime called the \"gold\" labels!\n",
    "\n",
    "You can learn more about fuzzy string matching and the package in the [RapidFuzz Documentation](https://rapidfuzz.github.io/RapidFuzz/).\n",
    "\n",
    "We built a helper function called `extract_gold_entities` which takes an example from our dataset and extracts the \"gold\" dictionary.  For example, here's an example from the validation set:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2',\n",
       " 'document_id': 1,\n",
       " 'sentence_id': 2,\n",
       " 'tokens': ['West',\n",
       "  'Indian',\n",
       "  'all-rounder',\n",
       "  'Phil',\n",
       "  'Simmons',\n",
       "  'took',\n",
       "  'four',\n",
       "  'for',\n",
       "  '38',\n",
       "  'on',\n",
       "  'Friday',\n",
       "  'as',\n",
       "  'Leicestershire',\n",
       "  'beat',\n",
       "  'Somerset',\n",
       "  'by',\n",
       "  'an',\n",
       "  'innings',\n",
       "  'and',\n",
       "  '39',\n",
       "  'runs',\n",
       "  'in',\n",
       "  'two',\n",
       "  'days',\n",
       "  'to',\n",
       "  'take',\n",
       "  'over',\n",
       "  'at',\n",
       "  'the',\n",
       "  'head',\n",
       "  'of',\n",
       "  'the',\n",
       "  'county',\n",
       "  'championship',\n",
       "  '.'],\n",
       " 'ner_tags': [7,\n",
       "  8,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  0,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the extracted gold or ground-truth entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MISC': ['West Indian'],\n",
       " 'PER': ['Phil Simmons'],\n",
       " 'ORG': ['Leicestershire', 'Somerset']}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_entities = extract_gold_entities(subset[2], BIO_tags_list)\n",
    "gold_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While here are the predicted entities from our LLM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PER': ['Phil Simmons'],\n",
       " 'ORG': ['Leicestershire', 'Somerset'],\n",
       " 'LOC': [],\n",
       " 'MISC': []}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_entities[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate Named Entity Recognition (NER), we compare the entities predicted by the model with the **gold (true)** entities from the dataset.\n",
    "\n",
    "We compute the following metrics **for each entity type** (e.g., PER, LOC, ORG):\n",
    "\n",
    "- **Precision** = Correct predictions / All predictions  \n",
    "- **Recall** = Correct predictions / All gold (true) entities  \n",
    "- **F1 score** = Harmonic mean of precision and recall  \n",
    "- **Accuracy** = Correct predictions / (Correct + Wrong + Missed predictions)\n",
    "\n",
    "We include the function `evaluate_ner` in `helpers.py` to do the computations.  It's imported above.  We show you how to use it in the next cell assuming that `subset` from above for which our LLM NER model gave us the entity `predicted_entities`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Number</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PER</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>58.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.8154</td>\n",
       "      <td>0.7571</td>\n",
       "      <td>0.7852</td>\n",
       "      <td>70.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.8393</td>\n",
       "      <td>0.8103</td>\n",
       "      <td>0.8246</td>\n",
       "      <td>58.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.2667</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Overall</td>\n",
       "      <td>0.8791</td>\n",
       "      <td>0.8081</td>\n",
       "      <td>0.8421</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Entity  Precision  Recall      F1  Number  Accuracy\n",
       "0      PER     1.0000  1.0000  1.0000    58.0       NaN\n",
       "1      ORG     0.8154  0.7571  0.7852    70.0       NaN\n",
       "2      LOC     0.8393  0.8103  0.8246    58.0       NaN\n",
       "3     MISC     0.6667  0.1667  0.2667    12.0       NaN\n",
       "4  Overall     0.8791  0.8081  0.8421     NaN    0.7273"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract gold entities\n",
    "gold_entities = [extract_gold_entities(ex, BIO_tags_list) for ex in subset]\n",
    "\n",
    "# Evaluate\n",
    "results_llm = evaluate_ner(predicted_entities, gold_entities, labels = [\"PER\", \"ORG\", \"LOC\", \"MISC\"])\n",
    "\n",
    "# Format the evaluation results\n",
    "df_results_llm = format_ner_eval_results(results_llm)\n",
    "display(df_results_llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLM NER results are terrific for people, and pretty good for locations and organizations, but only find about 17% the true MISC entities in the texts.  Maybe you can get it to work better by providing examples of MISC entities and additional instructions in the prompt.\n",
    "\n",
    "Here are the results from the BERT model (applied to the whole test set) for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Number</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.9082</td>\n",
       "      <td>0.9137</td>\n",
       "      <td>0.9109</td>\n",
       "      <td>1668.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.7158</td>\n",
       "      <td>0.7892</td>\n",
       "      <td>0.7507</td>\n",
       "      <td>702.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.8492</td>\n",
       "      <td>0.8778</td>\n",
       "      <td>0.8632</td>\n",
       "      <td>1661.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.9567</td>\n",
       "      <td>0.9431</td>\n",
       "      <td>0.9499</td>\n",
       "      <td>1617.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Overall</td>\n",
       "      <td>0.8782</td>\n",
       "      <td>0.8961</td>\n",
       "      <td>0.8870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Entity  Precision  Recall      F1  Number  Accuracy\n",
       "0      LOC     0.9082  0.9137  0.9109  1668.0       NaN\n",
       "1     MISC     0.7158  0.7892  0.7507   702.0       NaN\n",
       "2      ORG     0.8492  0.8778  0.8632  1661.0       NaN\n",
       "3      PER     0.9567  0.9431  0.9499  1617.0       NaN\n",
       "4  Overall     0.8782  0.8961  0.8870     NaN     0.978"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_results_BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The accuracies are very different, in part, because they're computed differently.  In the case of the BERT model we are able to include all the tokens tagged as 'O' (other) which is most of the tokens.  This inflates the accuracy just like computing accuracy for a segmentation model in which most of the pixels are background.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
