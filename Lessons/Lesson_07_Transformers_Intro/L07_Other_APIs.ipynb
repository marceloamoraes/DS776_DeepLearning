{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional APIs for LLMs\n",
    "\n",
    "Running LLMs locally is great for data security, but they can be painfully slow without significant computing resources.  We've identified a couple of services that allow us to run some of the same open source LLMs that are available on HuggingFace at little to no cost.  I've been using these services because they're so much faster than running the models locally, even on a good Nvidia gaming GPU and they're super cheap.\n",
    "\n",
    "First you'll need to update the course package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DO THIS FIRST\n",
    "\n",
    "We made some updates to our text generation help functions in the course package to use a couple of additional APIs.\n",
    "\n",
    "Change `force_update=True` in the last line and run the next cell to install an updated course package.  Once it's done restart your kernel and change back to `force_update=False`.  You only need to do this once per server (not once per notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Force update requested. Uninstalling `introdl`...\n",
      "Installing `introdl` from local directory: C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\DS776\\Lessons\\Course_Tools\\introdl\n",
      "The `introdl` module is now installed.\n"
     ]
    }
   ],
   "source": [
    "# run this cell to ensure course package is installed\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "course_tools_path = Path('../Course_Tools/').resolve() # change this to the local path of the course package\n",
    "sys.path.append(str(course_tools_path))\n",
    "\n",
    "from install_introdl import ensure_introdl_installed\n",
    "ensure_introdl_installed(force_update=True, local_path_pkg= course_tools_path / 'introdl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groq \n",
    "\n",
    "[Groq](https://groq.com/) is a service for running generative AI models in the cloud via an API.  (We'll look at the APIs in more detail in Lesson 11).  Go to [Groq](https://groq.com/) and sign up for an account.  (Start with Developers -> Free API Key).  Their models are free to use on a limited basis for testing and development which may be enough for this class, but you could spend a dollar or two and use their models for the rest of the semester for everything.  The Groq API is fast!  Groq allows you to pay for ony what you use and I can't imagine it would cost more than $1 or $2 for everthing we do in this class unless go wild.  Even then it wouldn't be much. Poke around in the developer console to see what models are available.\n",
    "\n",
    "Get an API key and add it to your api_keys.env file as GROQ_API_KEY. \n",
    "\n",
    "In this example we use Mistral AI's Mixtral 8x7b mixture of experts model ($0.24 per million tokens input and output):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "from introdl.utils import config_paths_keys, wrap_print_text\n",
    "from introdl.nlp import llm_configure, llm_generate\n",
    "\n",
    "paths = config_paths_keys(); # set API keys and paths\n",
    "print = wrap_print_text(print) # set print to wrap text for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’° Estimated Cost: $0.000021 (Input: 24.0 tokens, Output: 64.0 tokens)\n",
      "\n",
      "Pi (Ï€) is an irrational number, which means its decimal representation goes on\n",
      "forever without repeating. It is approximately equal to 3.14159. Pi is the ratio\n",
      "of a circle's circumference to its diameter and is widely used in mathematics,\n",
      "physics, and engineering.\n"
     ]
    }
   ],
   "source": [
    "# set the model name and configure the LLM\n",
    "# you can use any model name supported by the provider\n",
    "model_name = 'Mixtral-8x7b-32768'\n",
    "\n",
    "# optionally estimate costs for the LLM usage by setting the cost per million tokens\n",
    "# and passing to llm_configure() as cost_per_M_input and cost_per_M_output\n",
    "# the cost per million tokens can be found on the provider's website\n",
    "cost_per_M_input = 0.24 # cost per million input tokens in USD\n",
    "cost_per_M_output = 0.24 # cost per million output tokens in USD\n",
    "groq_config = llm_configure(model_name, llm_provider='groq', \n",
    "                            cost_per_M_input=cost_per_M_input, \n",
    "                            cost_per_M_output=cost_per_M_output)\n",
    "\n",
    "# generate a response using the LLM\n",
    "prompt = \"Tell me about the number pi.\"\n",
    "response = llm_generate(groq_config, prompt, max_new_tokens=200, estimate_cost=True) # estimate_cost defaults to False\n",
    "print(\"\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'd have to repeat that over 45,000 times to spend a $1!  It's also MUCH faster than running the model locally unless you have access to some serious hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Together.AI\n",
    "\n",
    "[Together.AI](https://groq.com/) is similar to Groq but has a wider variety of models available.  Including a couple that are always free (but throttled so they're not that fast).  The prices are similar to Groq, but without free testing.  But they give you $1 credit for signing up.  Click on Get Started to set up an account and get an API key.  I think Together.AI requires you to buy in credits in chunks of $25 so Groq is probably a better option, but you may as well spend the $1 credit and use the free models.  Groq also seems to be significantly faster in my limited testing.\n",
    "\n",
    "Get an API key and add it to your api_keys.env file as TOGETHER_API_KEY. \n",
    "\n",
    "In this example we use Meta's Llama 3.3 70 billion parameter which seems to be quite good ($0.88 per million tokens input and output):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "from introdl.utils import config_paths_keys, wrap_print_text\n",
    "from introdl.nlp import llm_configure, llm_generate\n",
    "\n",
    "paths = config_paths_keys(); # set API keys and paths\n",
    "print = wrap_print_text(print) # set print to wrap text for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’° Estimated Cost: $0.000092 (Input: 33.0 tokens, Output: 71.0 tokens)\n",
      "\n",
      "Pi (Ï€) is a mathematical constant representing the ratio of a circle's\n",
      "circumference to its diameter. It's approximately 3.14159, but it's an\n",
      "irrational number, meaning it can't be expressed exactly as a finite decimal or\n",
      "fraction. Pi is essential in mathematics and appears in many mathematical\n",
      "formulas, particularly in geometry and trigonometry.\n"
     ]
    }
   ],
   "source": [
    "# set the model name and configure the LLM\n",
    "# you can use any model name supported by the provider\n",
    "model_name = 'meta-llama/Meta-Llama-3-70B-Instruct-Turbo'\n",
    "\n",
    "cost_per_M_input = 0.88 # cost per million input tokens in USD\n",
    "cost_per_M_output = 0.88 # cost per million output tokens in USD\n",
    "together_config = llm_configure(model_name, llm_provider='together', \n",
    "                            cost_per_M_input=cost_per_M_input, \n",
    "                            cost_per_M_output=cost_per_M_output)\n",
    "\n",
    "# generate a response using the LLM\n",
    "prompt = \"Tell me about the number pi.\"\n",
    "response = llm_generate(together_config, prompt, max_new_tokens=200, estimate_cost=True) # estimate_cost defaults to False\n",
    "print(\"\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
