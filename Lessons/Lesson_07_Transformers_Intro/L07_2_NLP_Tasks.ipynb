{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install ../Course_Tools/introdl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from introdl.utils import get_device, wrap_print_text, config_paths_keys\n",
    "from introdl.nlp import llm_configure, llm_generate, clear_pipeline, print_pipeline_info, display_markdown\n",
    "\n",
    "# overload print to wrap text\n",
    "print = wrap_print_text(print)\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "paths = config_paths_keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP Tasks with Transformer Models\n",
    "\n",
    "In this notebook we'll demonstrate soutions to some common Natural Langauge Processing (NLP) tasks that use transformer models.  We expand on the material in our NLP textbook Chapter 1 - Hello Transformers.  We'll add a little background about the underlying models.  We'll also demonstrate how these same tasks come be done using a large language model with either \"zero-shot prompting\" or \"few-shot prompting\".\n",
    "\n",
    "Using LLMs for various NLP tasks is common when there isn't much labeled data avaialable.  Zero-shot prompting means that no examples are provided to the LLM.  Few-shot prompting means that a small number of examples are provided to the LLM.\n",
    "\n",
    "Over the next five lesons we'll go into some of these NLP tasks in detail and a learn a bit about the transfomer neural network architecture.\n",
    "\n",
    "Here is a customer feedback message that we'll use as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Text\n",
    "text = \"\"\"I ordered the Samsung Galaxy S24 Ultra from Tech Haven, expecting next-day delivery, \n",
    "but after three days, I hadnâ€™t even received a shipping update. After waiting 45 minutes on hold, \n",
    "customer service told me there was a stock issueâ€”yet no one had informed me! \n",
    "\n",
    "When the package finally arrived a week late, it contained a Google Pixel 8 Pro instead. \n",
    "The support rep was apologetic but said an exchange would take another two weeks.  \n",
    "\n",
    "I paid $1,200 for the wrong phone, dealt with delays and poor communication, and now have to wait even longer. \n",
    "Tech Haven, you need to do better! Sincerely, Jamie.\n",
    "\n",
    "To add insult to injury, the customer service representative I spoke with seemed indifferent to my frustration. \n",
    "I had to explain my situation multiple times before they even acknowledged the mistake. \n",
    "The entire experience has been incredibly disappointing and has left me questioning whether I should ever shop with Tech Haven again. \n",
    "It's baffling how a company can operate with such a lack of transparency and efficiency. \n",
    "I hope this feedback reaches someone who can make a difference, as no customer should have to go through what I did.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task - Text Classification\n",
    "\n",
    "Text classification is the process of assigning predefined categories to text. It involves analyzing the content of the text and categorizing it based on its subject, sentiment, or other criteria. One common application of text classification is sentiment analysis, which determines the sentiment expressed in a piece of text, such as positive, negative, or neutral. Sentiment analysis is widely used in customer feedback analysis, social media monitoring, and market research to gauge public opinion and customer satisfaction.\n",
    "\n",
    "### Sentiment Analysis with a Specialized Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will let the HuggingFace transformers library provide its default model for sentiment analysis and apply it to our customer feedback.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Sentiment Analysis**\n",
      "[{'label': 'NEGATIVE', 'score': 0.9990748167037964}]\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Analysis\n",
    "print(\"\\n**Sentiment Analysis**\")\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", device=device)\n",
    "print_pipeline_info(sentiment_pipeline)\n",
    "sentiment_result = sentiment_pipeline(text)\n",
    "print(sentiment_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, a \"BERT\" model correctly classified the customer feedback as negative. BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model developed by Google. It is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. This allows BERT to understand the context of a word based on its surroundings, making it highly effective for various NLP tasks. The particular model used here is a distilled BERT model that has been fine-tuned on a sentiment dataset. A distilled model is a smaller, faster, and more efficient version of a larger model, trained using knowledge distillation, where the smaller model learns to mimic the outputs of the larger one while retaining most of its performance. In Lesson 9, we'll learn more about the family of transformer models called encoders, which include BERT models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's never a bad idea to remove models from memory when they aren't being used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline cleared.  Freed 0.00 MB of CUDA memory\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(sentiment_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis with a Zero-Shot Prompt\n",
    "\n",
    "A system prompt is used to give instructions to an LLM while a user prompt is the specific input you want the LLM to respond to.  Here we define a system prompt for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"You are an expert sentiment analysis model. Analyze the sentiment of the following text. \n",
    "Give only a one word response: positive, negative, or neutral.\"\"\"\n",
    "user_prompt = f\"Text: {text}\\nSentiment:\"\n",
    "\n",
    "llm_config = llm_configure(\"mistral-7B\")\n",
    "response_zero_shot = llm_generate(llm_config, user_prompt, system_prompt=system_prompt)\n",
    "print(response_zero_shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also handle batches of inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The product quality is amazing, exceeded my expectations!\n",
      "Sentiment:\n",
      "Positive\n",
      "Text: Terrible service, I will never shop here again.\n",
      "Sentiment:\n",
      "Negative\n",
      "Text: Fast shipping and great customer support. Highly recommend!\n",
      "Sentiment:\n",
      "Positive\n",
      "Text: The item arrived damaged and the return process was a nightmare.\n",
      "Sentiment:\n",
      "Negative\n",
      "Text: I'm very satisfied with my purchase. Will buy again.\n",
      "Sentiment:\n",
      "Positive\n",
      "Text: The website is user-friendly and the prices are unbeatable.\n",
      "Sentiment:\n",
      "Positive\n",
      "Text: Received the wrong item and customer service was unhelpful.\n",
      "Sentiment:\n",
      "Negative\n",
      "Text: Fantastic experience from start to finish.\n",
      "Sentiment:\n",
      "Positive\n",
      "Text: The product is okay, but not worth the price.\n",
      "Sentiment:\n",
      "Negative\n",
      "Text: Excellent quality and quick delivery. Very happy!\n",
      "Sentiment:\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "customer_comments = [\n",
    "    \"The product quality is amazing, exceeded my expectations!\",\n",
    "    \"Terrible service, I will never shop here again.\",\n",
    "    \"Fast shipping and great customer support. Highly recommend!\",\n",
    "    \"The item arrived damaged and the return process was a nightmare.\",\n",
    "    \"I'm very satisfied with my purchase. Will buy again.\",\n",
    "    \"The website is user-friendly and the prices are unbeatable.\",\n",
    "    \"Received the wrong item and customer service was unhelpful.\",\n",
    "    \"Fantastic experience from start to finish.\",\n",
    "    \"The product is okay, but not worth the price.\",\n",
    "    \"Excellent quality and quick delivery. Very happy!\"\n",
    "]\n",
    "\n",
    "user_prompts = [f\"Text: {comment}\\nSentiment:\" for comment in customer_comments]\n",
    "\n",
    "responses_zero_shot = llm_generate(llm_config, user_prompts, system_prompt=system_prompt)\n",
    "\n",
    "for user_prompt, response_zero_shot in zip(user_prompts, responses_zero_shot):\n",
    "    print(user_prompt)\n",
    "    print(response_zero_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Named Entity Recognition**\n",
      "Model: dbmdz/bert-large-cased-finetuned-conll03-english, Size: 332,538,889\n",
      "parameters\n",
      "[{'entity_group': 'MISC', 'score': 0.9910073, 'word': 'Samsung Galaxy S24\n",
      "Ultra', 'start': 14, 'end': 38}, {'entity_group': 'ORG', 'score': 0.99012053,\n",
      "'word': 'Tech Haven', 'start': 44, 'end': 54}, {'entity_group': 'MISC', 'score':\n",
      "0.992503, 'word': 'Google Pixel 8 Pro', 'start': 325, 'end': 343},\n",
      "{'entity_group': 'ORG', 'score': 0.96618426, 'word': 'Tech Haven', 'start': 551,\n",
      "'end': 561}, {'entity_group': 'PER', 'score': 0.98039377, 'word': 'Jamie',\n",
      "'start': 597, 'end': 602}]\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition (NER)\n",
    "print(\"\\n**Named Entity Recognition**\")\n",
    "ner_pipeline = pipeline(\"ner\", aggregation_strategy=\"simple\", device=device)\n",
    "print_pipeline_info(ner_pipeline)\n",
    "ner_result = ner_pipeline(text)\n",
    "print(ner_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline cleared.  Freed 1330.93 MB of CUDA memory\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(ner_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Question Answering**\n",
      "Model: distilbert/distilbert-base-cased-distilled-squad, Size: 65,192,450\n",
      "parameters\n",
      "{'score': 0.11689740419387817, 'start': 222, 'end': 233, 'answer': 'stock\n",
      "issue'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Question Answering\n",
    "print(\"\\n**Question Answering**\")\n",
    "qa_pipeline = pipeline(\"question-answering\", device=device)\n",
    "print_pipeline_info(qa_pipeline)\n",
    "question = \"What is the defect?\"\n",
    "qa_result = qa_pipeline(question=question, context=text)\n",
    "print(qa_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline cleared.  Freed 260.77 MB of CUDA memory\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(qa_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Translation**\n",
      "Model: Helsinki-NLP/opus-mt-en-es, Size: 77,943,296 parameters\n",
      "[{'translation_text': 'PedÃ­ el Samsung Galaxy S24 Ultra de Tech Haven, esperando\n",
      "la entrega del dÃ­a siguiente, pero despuÃ©s de tres dÃ­as, ni siquiera habÃ­a\n",
      "recibido una actualizaciÃ³n de envÃ­o. DespuÃ©s de esperar 45 minutos en espera, el\n",
      "servicio al cliente me dijo que habÃ­a un problema de existencias â€” sin embargo,\n",
      "nadie me habÃ­a informado! Cuando el paquete finalmente llegÃ³ una semana tarde,\n",
      "que contenÃ­a un Google Pixel 8 Pro en su lugar. El representante de apoyo era\n",
      "apologÃ©tico, pero dijo que un intercambio tomarÃ­a otras dos semanas. PaguÃ©\n",
      "$1,200 por el telÃ©fono equivocado, tratÃ³ con retrasos y mala comunicaciÃ³n, y\n",
      "ahora tienen que esperar aÃºn mÃ¡s. Tech Haven, usted necesita hacer mejor!\n",
      "Sinceramente, Jamie.'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Translation (English to Spanish)\n",
    "print(\"\\n**Translation**\")\n",
    "translation_pipeline = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\", device=device)\n",
    "print_pipeline_info(translation_pipeline)\n",
    "translation_result = translation_pipeline(text, max_length=200)\n",
    "print(translation_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline cleared.  Freed 312.03 MB of CUDA memory\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(translation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Text Generation**\n",
      "Model: openai-community/gpt2, Size: 124,439,808 parameters\n",
      "I ordered the Samsung Galaxy S24 Ultra from Tech Haven, expecting next-day\n",
      "delivery,\n",
      "but after three days, I hadnâ€™t even received a shipping update. After waiting 45\n",
      "minutes on hold,\n",
      "customer service told me there was a stock issueâ€”yet no one had informed me!\n",
      "\n",
      "When the package finally arrived a week late, it contained a Google Pixel 8 Pro\n",
      "instead.\n",
      "The support rep was apologetic but said an exchange would take another two\n",
      "weeks.\n",
      "\n",
      "I paid $1,200 for the wrong phone, dealt with delays and poor communication, and\n",
      "now have to wait even longer.\n",
      "Tech Haven, you need to do better! Sincerely, Jamie.\n",
      "\n",
      "Customer service response:\n",
      "Dear Jamie, I am sorry to hear that your order was mixed up.\n",
      "\n",
      "However, due to the delay, the issue was resolved within 2 weeks, and the order\n",
      "has been delivered to its original address.\n",
      "\n",
      "Customer service response:\n",
      "Dear Jamie, I am sorry to hear that your order was mixed up.\n",
      "\n",
      "However, due to the delay, the issue was resolved within 2 weeks, and the order\n",
      "has been delivered to its original address.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n**Text Generation**\")\n",
    "generator_pipeline = pipeline(\"text-generation\", device=device)\n",
    "print_pipeline_info(generator_pipeline)\n",
    "response = \"Dear Jamie, I am sorry to hear that your order was mixed up.\"\n",
    "prompt = text + \"\\n\\nCustomer service response:\\n\" + response\n",
    "outputs = generator_pipeline(prompt, max_length=200)\n",
    "generated_text = outputs[0]['generated_text']\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline cleared.  Freed 511.15 MB of CUDA memory\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(generator_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Summarization**\n",
      "Model: sshleifer/distilbart-cnn-12-6, Size: 305,510,400 parameters\n",
      "[{'summary_text': ' Tech Haven sent a Samsung Galaxy S24 Ultra to Tech Haven,\n",
      "expecting next-day delivery . The package arrived a week late, it contained a\n",
      "Google Pixel 8 Pro instead . An exchange would take another two weeks .'}]\n"
     ]
    }
   ],
   "source": [
    "# Summarization\n",
    "print(\"\\n**Summarization**\")\n",
    "summarization_pipeline = pipeline(\"summarization\", device=device)\n",
    "print_pipeline_info(summarization_pipeline)\n",
    "summarization_result = summarization_pipeline(text, max_length=50, min_length=25, do_sample=False)\n",
    "print(summarization_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline cleared.  Freed 1222.24 MB of CUDA memory\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(summarization_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ›‘ Unloading model: openai-community/gpt2 from GPU...\n",
      "âœ… Model openai-community/gpt2 has been fully unloaded.\n",
      "ðŸš€ Loading model: unsloth/mistral-7b-instruct-v0.3-bnb-4bit (this may take a while)...\n",
      "ðŸŸ¢ Model unsloth/mistral-7b-instruct-v0.3-bnb-4bit loaded successfully.\n",
      "\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from introdl.nlp import llm_configure, llm_generate\n",
    "from introdl.utils import wrap_print_text, get_device\n",
    "\n",
    "print = wrap_print_text(print)\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "mistral_config = llm_configure(\"mistral\")\n",
    "response = llm_generate(mistral_config, \"What is the capital of France?\")\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "# Sample Text\n",
    "text = \"\"\"I ordered the Samsung Galaxy S24 Ultra from Tech Haven, expecting next-day delivery, \n",
    "but after three days, I hadnâ€™t even received a shipping update. After waiting 45 minutes on hold, \n",
    "customer service told me there was a stock issueâ€”yet no one had informed me! \n",
    "\n",
    "When the package finally arrived a week late, it contained a Google Pixel 8 Pro instead. \n",
    "The support rep was apologetic but said an exchange would take another two weeks.  \n",
    "\n",
    "I paid $1,200 for the wrong phone, dealt with delays and poor communication, and now have to wait even longer. \n",
    "Tech Haven, you need to do better! Sincerely, Jamie.\"\"\"\n",
    "\n",
    "\n",
    "system_prompt_sentiment = \"\"\"You are an expert sentiment analysis model. Analyze the sentiment of the following text. \n",
    "Give only a one word response: positive, negative, or neutral.\"\"\"\n",
    "user_prompt = f\"Text: {text}\\nSentiment:\"\n",
    "\n",
    "response = llm_generate(mistral_config, user_prompt, system_prompt=system_prompt_sentiment)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"name\": \"Samsung Galaxy S24 Ultra\",\n",
      "      \"type\": \"Product\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Tech Haven\",\n",
      "      \"type\": \"Organization\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"$1,200\",\n",
      "      \"type\": \"CurrencyAmount\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Google Pixel 8 Pro\",\n",
      "      \"type\": \"Product\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "system_prompt_ner = \"\"\"You are an expert named entity recognition model. Identify and classify the entities in the following text. \n",
    "Provide the entities and their types in a JSON format.\"\"\"\n",
    "user_prompt_ner = f\"Text: {text}\\nEntities:\"\n",
    "\n",
    "response_ner = llm_generate(mistral_config, user_prompt_ner, system_prompt=system_prompt_ner)\n",
    "print(response_ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The issue involves incorrect order delivery (Google Pixel 8 Pro instead of\n",
      "Samsung Galaxy S24 Ultra), delay in shipment, lack of timely communication about\n",
      "the stock issue, and further prolonged time for an exchange.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"What is the issue?\"\n",
    "system_prompt_qa = \"\"\"You are an expert question answering model. Answer the question based on the context provided. Be succinct\"\"\"\n",
    "user_prompt_qa = f\"Context: {text}\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "response_qa = llm_generate(mistral_config, user_prompt_qa, system_prompt=system_prompt_qa)\n",
    "print(response_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimado Tech Haven, ordenÃ© el Samsung Galaxy S24 Ultra esperando entrega en un\n",
      "dÃ­a,\n",
      "sin embargo, despuÃ©s de tres dÃ­as, aÃºn no habÃ­a recibido una actualizaciÃ³n de\n",
      "envÃ­o. DespuÃ©s de pasar cuarenta y cinco minutos en llamada por lÃ­nea\n",
      "telefÃ³nica,\n",
      "el servicio al cliente me informÃ³ que existÃ­a un problema de stock â€” sin\n",
      "embargo, nadie me lo habÃ­a comunicado previamente!\n",
      "Cuando finalmente llegÃ³ la paqueterÃ­a retrasada por una semana, contenÃ­a un\n",
      "Google Pixel 8 Pro en su lugar. El representante de soporte se disculpÃ³ pero\n",
      "dijo que para realizar una intercambio necesitarÃ­an otras dos semanas.\n",
      "Pague $1,200 por el telÃ©fono equivocado, tuve que lidiar con retras\n"
     ]
    }
   ],
   "source": [
    "system_prompt_translation = \"\"\"You are an expert translation model. Translate the following text from English to Spanish.\"\"\"\n",
    "user_prompt_translation = f\"Text: {text}\\nTranslation:\"\n",
    "\n",
    "response_translation = llm_generate(mistral_config, user_prompt_translation, system_prompt=system_prompt_translation)\n",
    "print(response_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer Jamie experienced delayed delivery of the Samsung Galaxy S24 Ulta from\n",
      "Tech Haven, receiving a Google Pixel 8 Pro instead due to a stock issue that\n",
      "wasn't communicated. The customer service representative offered an exchange,\n",
      "but this process will also take additional time. Overall, Jamie is dissatisfied\n",
      "with the service provided by Tech Haven, citing issues with communication,\n",
      "delays, and incorrect product shipment.\n"
     ]
    }
   ],
   "source": [
    "system_prompt_summarization = \"\"\"You are an expert summarization model. Summarize the following text in a concise manner.\"\"\"\n",
    "user_prompt_summarization = f\"Text: {text}\\nSummary:\"\n",
    "\n",
    "response_summarization = llm_generate(mistral_config, user_prompt_summarization, system_prompt=system_prompt_summarization)\n",
    "print(response_summarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Jamie,\n",
      "Thank you for taking the time to share your experience with us at Tech Haven. We\n",
      "deeply regret the inconvenience you've encountered during your recent purchase\n",
      "of the Samsung Galaxy S24 Ultra.\n",
      "We understand that prompt delivery and clear communication are crucial aspects\n",
      "in maintaining trust with our customers, and we sincerely apologize for falling\n",
      "short of these expectations. The mix-up with the order is unacceptable, and we\n",
      "should have kept you updated about the stock issues earlier.\n",
      "To address this matter, we will immediately process a full refund for the\n",
      "incorrectly shipped device as well as cover any additional costs related to\n",
      "returning the Google Pixel 8 Pro back to us. Furthermore, we will prioritize\n",
      "sending out the correct product (Samsung Galaxy S24 Ultra) without delay, along\n",
      "with expedited shipping to ensure timely arrival.\n",
      "As a token of appreciation for your patience throughout this ordeal, we will\n"
     ]
    }
   ],
   "source": [
    "system_prompt_generation = \"\"\"You are an expert text generation model. Generate a customer service response based on the provided context.\"\"\"\n",
    "user_prompt_generation = f\"Context: {text}\\n\\nCustomer service response:\\n{response}\"\n",
    "\n",
    "response_generation = llm_generate(mistral_config, user_prompt_generation, system_prompt=system_prompt_generation)\n",
    "print(response_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\bagge\\my drive\\python_projects\\ds776_develop_project\\ds776\\lessons\\course_tools\\introdl\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: IPython in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (8.28.0)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (8.1.5)\n",
      "Requirement already satisfied: ipycanvas in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (0.13.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (3.9.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (2.2.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (1.14.1)\n",
      "Requirement already satisfied: seaborn in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (0.13.2)\n",
      "Requirement already satisfied: torch in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (2.4.1)\n",
      "Requirement already satisfied: torchinfo in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (1.8.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (0.19.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (4.66.5)\n",
      "Requirement already satisfied: pillow>=6.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from ipycanvas->introdl==1.0) (10.4.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from ipywidgets->introdl==1.0) (0.2.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from ipywidgets->introdl==1.0) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from ipywidgets->introdl==1.0) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from ipywidgets->introdl==1.0) (3.0.13)\n",
      "Requirement already satisfied: decorator in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (2.18.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from matplotlib->introdl==1.0) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from matplotlib->introdl==1.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from matplotlib->introdl==1.0) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from matplotlib->introdl==1.0) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from matplotlib->introdl==1.0) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from matplotlib->introdl==1.0) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from matplotlib->introdl==1.0) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from pandas->introdl==1.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from pandas->introdl==1.0) (2024.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from torch->introdl==1.0) (3.16.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from torch->introdl==1.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from torch->introdl==1.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from torch->introdl==1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from torch->introdl==1.0) (2024.6.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from jedi>=0.16->IPython->introdl==1.0) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->IPython->introdl==1.0) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->introdl==1.0) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from jinja2->torch->introdl==1.0) (3.0.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from stack-data->IPython->introdl==1.0) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from stack-data->IPython->introdl==1.0) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from stack-data->IPython->introdl==1.0) (0.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from sympy->torch->introdl==1.0) (1.3.0)\n",
      "Building wheels for collected packages: introdl\n",
      "  Building wheel for introdl (pyproject.toml): started\n",
      "  Building wheel for introdl (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for introdl: filename=introdl-1.0-py3-none-any.whl size=41669 sha256=ef0876a17990433a5c0cf7239eec100df7f67d9a9e3cbccbe83cea9d2b439e2b\n",
      "  Stored in directory: C:\\Users\\bagge\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-2_41wy4l\\wheels\\f5\\d5\\0f\\11f1d5af64d00defb23fa33cf51b2946a0899888d73571e687\n",
      "Successfully built introdl\n",
      "Installing collected packages: introdl\n",
      "  Attempting uninstall: introdl\n",
      "    Found existing installation: introdl 1.0\n",
      "    Uninstalling introdl-1.0:\n",
      "      Successfully uninstalled introdl-1.0\n",
      "Successfully installed introdl-1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install ../Course_Tools/introdl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_config = llm_configure(\"gemini-flash-lite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't scientists trust atoms?\n",
      "Because they make up everything!\n"
     ]
    }
   ],
   "source": [
    "response = llm_generate(gemini_config, \"Tell me a dad joke.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three interesting facts about space:\n",
      "1.  **There's a planet made of diamond:** Astronomers discovered a planet called\n",
      "55 Cancri e that is twice the size of Earth and made almost entirely of diamond.\n",
      "2.  **Space is completely silent:** Sound waves need a medium to travel, and\n",
      "space is a vacuum.\n",
      "3.  **The universe is still expanding:** Scientists have observed that the\n",
      "universe is not only expanding but that the expansion is accelerating.\n"
     ]
    }
   ],
   "source": [
    "response = llm_generate(gemini_config, \"I am testing the audio transcription abilities of Gemini. Please tell me three interesting facts about space.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am testing the audio transcription abilities of Gemini.\n",
      "Please tell me three interesting facts about space.\n"
     ]
    }
   ],
   "source": [
    "gemini_config = llm_configure(\"gemini-flash-lite\")\n",
    "\n",
    "system_prompt = \"\"\"You are an AI assistant that refines transcriptions.  \n",
    "The input is the transcription of an audio file.  \n",
    "Do not answer questions in the input or add any content beyond the original input.  \n",
    "Do edit the input transcript for clarity but keep topics in roughly the same order.  \n",
    "Use bullets if appropriate.\n",
    "\"\"\"\n",
    "input_prompt = \"\"\"I am testing the audio transcription abilities of Gemini. \n",
    "Please tell me three interesting facts about space.\n",
    "\"\"\"\n",
    "response = llm_generate(gemini_config, input_prompt, system_prompt=system_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
