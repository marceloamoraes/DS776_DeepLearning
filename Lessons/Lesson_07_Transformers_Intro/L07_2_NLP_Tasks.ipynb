{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from introdl.utils import get_device, wrap_print_text, config_paths_keys\n",
    "from introdl.nlp import llm_configure, llm_generate, clear_pipeline, print_pipeline_info, display_markdown, llm_list_models\n",
    "\n",
    "# overload print to wrap text\n",
    "print = wrap_print_text(print)\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "paths = config_paths_keys()\n",
    "\n",
    "# we use the Mistral 7B model for this notebook, but you can any supported model, e.g. llama-3p1-8B, llama-3p2-3B, gemini-flash-lite, etc.\n",
    "# use llm_list_models() to see all available models\n",
    "LLM_MODEL = 'mistral-7B'\n",
    "llm_config = llm_configure(LLM_MODEL) # load model configuration for use throughout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L07_2_NLP_Tasks Video\n",
    "\n",
    "<iframe \n",
    "    src=\"https://media.uwex.edu/content/ds/ds776/ds776_l07_2_nlp_tasks\" \n",
    "    width=\"800\" \n",
    "    height=\"450\" \n",
    "    style=\"border: 5px solid cyan;\"  \n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "<br>\n",
    "<a href=\"https://media.uwex.edu/content/ds/ds776/ds776_l07_2_nlp_tasks\" target=\"_blank\">Open UWEX version of video in new tab</a>\n",
    "<br>\n",
    "<a href=\"https://share.descript.com/view/omj2ldze713\" target=\"_blank\">Open Descript version of video in new tab</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP Tasks with Transformer Models\n",
    "\n",
    "In this notebook we'll demonstrate solutions to some common Natural Language Processing (NLP) tasks that use transformer models.  We expand on the material in our NLP textbook Chapter 1 - Hello Transformers.  We'll add a little background about the underlying models.  We'll also demonstrate how these same tasks come be done using a large language model with either \"zero-shot prompting\" or \"few-shot prompting\".\n",
    "\n",
    "Over the next five lessons we'll go into some of these NLP tasks in detail and a learn a bit about the transformer neural network architecture.  For each of the NLP tasks that follows we'll demonstrate how to do the task two ways.  The first is by using a pre-trained transformer-based model downloaded from HuggingFace.  In the second approach we'll use a a large language model and prompting.\n",
    "\n",
    "Using LLMs for various NLP tasks is common when there isn't much labeled data available.  Zero-shot prompting means that no examples are provided to the LLM.  Few-shot prompting means that a small number of examples are provided to the LLM.  In this notebook we'll demonstrate zero-shot prompting, but in the lessons to come we'll include few-shot prompting examples.\n",
    "\n",
    "Throughout this notebook we'll use the following customer feedback message that as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ordered the Samsung Galaxy S24 Ultra from Tech Haven, expecting next-day\n",
      "delivery, but after three days, I hadn’t even received a shipping update. After\n",
      "waiting 45 minutes on hold, customer service told me there was a stock issue—yet\n",
      "no one had informed me!\n",
      "\n",
      "When the package finally arrived a week late, it contained a Google Pixel 8 Pro\n",
      "instead. The support rep was apologetic but said an exchange would take another\n",
      "two weeks.\n",
      "\n",
      "I paid $1,200 for the wrong phone, dealt with delays and poor communication, and\n",
      "now have to wait even longer. To add insult to injury, the customer service\n",
      "representative I spoke with seemed indifferent to my frustration. I had to\n",
      "explain my situation multiple times before they even acknowledged the mistake.\n",
      "The entire experience has been incredibly disappointing and has left me\n",
      "questioning whether I should ever shop with Tech Haven again.\n",
      "\n",
      "It's baffling how a company can operate with such a lack of transparency and\n",
      "efficiency. I hope this feedback reaches someone who can make a difference, as\n",
      "no customer should have to go through what I did. Tech Haven, you need to do\n",
      "better! Sincerely, Jamie.\n"
     ]
    }
   ],
   "source": [
    "# Sample Text\n",
    "text = \"\"\"I ordered the Samsung Galaxy S24 Ultra from Tech Haven, expecting next-day delivery, but after three days, I hadn’t even received a shipping update. After waiting 45 minutes on hold, customer service told me there was a stock issue—yet no one had informed me! \n",
    "\n",
    "When the package finally arrived a week late, it contained a Google Pixel 8 Pro instead. The support rep was apologetic but said an exchange would take another two weeks.  \n",
    "\n",
    "I paid $1,200 for the wrong phone, dealt with delays and poor communication, and now have to wait even longer. To add insult to injury, the customer service representative I spoke with seemed indifferent to my frustration. I had to explain my situation multiple times before they even acknowledged the mistake. The entire experience has been incredibly disappointing and has left me questioning whether I should ever shop with Tech Haven again. \n",
    "\n",
    "It's baffling how a company can operate with such a lack of transparency and efficiency. I hope this feedback reaches someone who can make a difference, as no customer should have to go through what I did. Tech Haven, you need to do better! Sincerely, Jamie.\"\"\"\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task - Text Classification\n",
    "\n",
    "Text classification is the process of assigning predefined categories to text. It involves analyzing the content of the text and categorizing it based on its subject, sentiment, or other criteria. One common application of text classification is sentiment analysis, which determines the sentiment expressed in a piece of text, such as positive, negative, or neutral. Sentiment analysis is widely used in customer feedback analysis, social media monitoring, and market research to gauge public opinion and customer satisfaction.\n",
    "\n",
    "### Sentiment Analysis with a Specialized Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will let the HuggingFace transformers library provide its default model for sentiment analysis and apply it to our customer feedback.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Sentiment Analysis**\n",
      "Model: distilbert/distilbert-base-uncased-finetuned-sst-2-english, Size: 66,955,010 parameters\n",
      "[{'label': 'NEGATIVE', 'score': 0.9989207983016968}]\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Analysis\n",
    "print(\"\\n**Sentiment Analysis**\")\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", device=device)\n",
    "print_pipeline_info(sentiment_pipeline)\n",
    "sentiment_result = sentiment_pipeline(text)\n",
    "print(sentiment_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, a \"BERT\" model correctly classified the customer feedback as negative. BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model developed by Google. It is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. This allows BERT to understand the context of a word based on its surroundings, making it highly effective for various NLP tasks. The particular model used here is a distilled BERT model that has been fine-tuned on a sentiment dataset. A distilled model is a smaller, faster, and more efficient version of a larger model, trained using knowledge distillation, where the smaller model learns to mimic the outputs of the larger one while retaining most of its performance. In Lesson 9, we'll learn more about the family of transformer models called encoders, which include BERT models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's never a bad idea to remove models from memory when they aren't being used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(sentiment_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Sentiment Analysis with an LLM and a Zero-Shot Prompt\n",
    "\n",
    "A system prompt is used to give instructions to an LLM while a user prompt is the specific input you want the LLM to respond to.  Here we define a system prompt for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"You are an expert sentiment analysis model. Analyze the sentiment of the following text. \n",
    "Give only a one word response: positive, negative, or neutral.\"\"\"\n",
    "user_prompt = f\"Text: {text}\\nSentiment:\"\n",
    "\n",
    "response_zero_shot = llm_generate(llm_config, user_prompt, system_prompt=system_prompt)\n",
    "print(response_zero_shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also handle batches of inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Fast shipping and great customer support. Highly recommend!\n",
      "Sentiment: Positive\n",
      "\n",
      "Text: The item arrived damaged and the return process was a nightmare.\n",
      "Sentiment: Negative\n",
      "\n",
      "Text: I'm very satisfied with my purchase. Will buy again.\n",
      "Sentiment: Positive\n",
      "\n",
      "Text: The website is user-friendly and the prices are unbeatable.\n",
      "Sentiment: Positive\n",
      "\n",
      "Text: Received the wrong item and customer service was unhelpful.\n",
      "Sentiment: Negative\n",
      "\n",
      "Text: Fantastic experience from start to finish.\n",
      "Sentiment: Positive\n",
      "\n",
      "Text: The product is okay, but not worth the price.\n",
      "Sentiment: Negative\n",
      "\n",
      "Text: Excellent quality and quick delivery. Very happy!\n",
      "Sentiment: Positive\n",
      "\n",
      "Text: The product works as expected, nothing more, nothing less.\n",
      "Sentiment: Neutral\n",
      "\n",
      "Text: I have mixed feelings about the service; it was both good and bad.\n",
      "Sentiment: Neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "customer_comments = [\n",
    "    \"Fast shipping and great customer support. Highly recommend!\",\n",
    "    \"The item arrived damaged and the return process was a nightmare.\",\n",
    "    \"I'm very satisfied with my purchase. Will buy again.\",\n",
    "    \"The website is user-friendly and the prices are unbeatable.\",\n",
    "    \"Received the wrong item and customer service was unhelpful.\",\n",
    "    \"Fantastic experience from start to finish.\",\n",
    "    \"The product is okay, but not worth the price.\",\n",
    "    \"Excellent quality and quick delivery. Very happy!\",\n",
    "    \"The product works as expected, nothing more, nothing less.\",\n",
    "    \"I have mixed feelings about the service; it was both good and bad.\"\n",
    "]\n",
    "\n",
    "user_prompts = [f\"Text: {comment}\\nSentiment:\" for comment in customer_comments]\n",
    "\n",
    "responses_zero_shot = llm_generate(llm_config, user_prompts, system_prompt=system_prompt)\n",
    "\n",
    "for comment, response_zero_shot in zip(customer_comments, responses_zero_shot):\n",
    "    print(f\"Text: {comment}\\nSentiment: {response_zero_shot}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll study text classification more in Lesson 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning to Write Better Prompts\n",
    "\n",
    "There are many prompt engineering resources available on the internet.  I encourage you to look at those as needed.  I've also had good luck asking ChatGPT how to craft prompts.  One particularly useful resource is [ChatGPT Prompt Engineering for Developers](https://app.datacamp.com/learn/courses/chatgpt-prompt-engineering-for-developers) on DataCamp.  It's not free (though I'm trying to see if we can get free acess - I'll announce it in Piazza if we are able to), but it is cheap.  I've viewed parts of this course and found it to be a very good introduction to programatic prompt writing.  It's tailored to ChatGPT, but you can use the OpenAI API with Gemini as well.  Working through this short course (they say it takes 4 hours) is likely worthwhile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task - Named Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.\n",
    "\n",
    "Practical examples of NER include:\n",
    "- **Business Application**: Extracting company names, dates, and monetary amounts from financial reports to automate data entry and analysis.\n",
    "- **Healthcare**: Identifying patient names, medical conditions, and treatment dates from clinical notes to improve patient record management.\n",
    "- **News Aggregation**: Categorizing and tagging entities like people, places, and events in news articles to enhance search and recommendation systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will let the HuggingFace transformers library provide its default model for NER and apply it to our customer feedback.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Named Entity Recognition**\n",
      "\n",
      "Model: dbmdz/bert-large-cased-finetuned-conll03-english, Size: 332,538,889 parameters\n",
      "\n",
      "[{'entity_group': 'MISC', 'score': 0.990973, 'word': 'Samsung Galaxy S24 Ultra',\n",
      "'start': 14, 'end': 38}, {'entity_group': 'ORG', 'score': 0.994846, 'word':\n",
      "'Tech Haven', 'start': 44, 'end': 54}, {'entity_group': 'MISC', 'score':\n",
      "0.9928634, 'word': 'Google Pixel 8 Pro', 'start': 323, 'end': 341},\n",
      "{'entity_group': 'ORG', 'score': 0.9964845, 'word': 'Tech Haven', 'start': 863,\n",
      "'end': 873}, {'entity_group': 'ORG', 'score': 0.9887396, 'word': 'Tech Haven',\n",
      "'start': 1089, 'end': 1099}, {'entity_group': 'PER', 'score': 0.9780477, 'word':\n",
      "'Jamie', 'start': 1135, 'end': 1140}]\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition (NER)\n",
    "print(\"\\n**Named Entity Recognition**\\n\")\n",
    "ner_pipeline = pipeline(\"ner\", aggregation_strategy=\"simple\", device=device)\n",
    "print_pipeline_info(ner_pipeline)\n",
    "print(\"\")\n",
    "ner_result = ner_pipeline(text)\n",
    "print(ner_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That output is hard to read, but we can easily convert it to a Pandas data frame for display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.990973</td>\n",
       "      <td>Samsung Galaxy S24 Ultra</td>\n",
       "      <td>14</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.994846</td>\n",
       "      <td>Tech Haven</td>\n",
       "      <td>44</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.992863</td>\n",
       "      <td>Google Pixel 8 Pro</td>\n",
       "      <td>323</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.996485</td>\n",
       "      <td>Tech Haven</td>\n",
       "      <td>863</td>\n",
       "      <td>873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.988740</td>\n",
       "      <td>Tech Haven</td>\n",
       "      <td>1089</td>\n",
       "      <td>1099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.978048</td>\n",
       "      <td>Jamie</td>\n",
       "      <td>1135</td>\n",
       "      <td>1140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score                      word  start   end\n",
       "0         MISC  0.990973  Samsung Galaxy S24 Ultra     14    38\n",
       "1          ORG  0.994846                Tech Haven     44    54\n",
       "2         MISC  0.992863        Google Pixel 8 Pro    323   341\n",
       "3          ORG  0.996485                Tech Haven    863   873\n",
       "4          ORG  0.988740                Tech Haven   1089  1099\n",
       "5          PER  0.978048                     Jamie   1135  1140"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "df = pd.DataFrame(ner_result)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"BERT\" model used here is the `dbmdz/bert-large-cased-finetuned-conll03-english` model, which has been fine-tuned on the CoNLL-2003 dataset for Named Entity Recognition (NER). This fine-tuning process allows the model to accurately identify and classify entities such as names of persons, organizations, locations, and more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(ner_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER with an LLM and a Zero-Shot Prompt\n",
    "\n",
    "If we don't have much training data or just want something quick and easy we can also use an LLM to for NER.  Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"text\": \"Samsung Galaxy S24 Ultra\",\n",
      "      \"type\": \"Product\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"Tech Haven\",\n",
      "      \"type\": \"Organization\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"Google Pixel 8 Pro\",\n",
      "      \"type\": \"Product\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"$1,200\",\n",
      "      \"type\": \"CurrencyAmount\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"Jamie\",\n",
      "      \"type\": \"Person\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "system_prompt = \"\"\"You are an expert named entity recognition model. Identify and classify the entities in the following text. \n",
    "Provide the entities and their types in a JSON format.\"\"\"\n",
    "user_prompt = f\"Text: {text}\\nEntities:\"\n",
    "\n",
    "llm_config = llm_configure(LLM_MODEL)\n",
    "response_ner = llm_generate(llm_config, user_prompt, system_prompt=system_prompt)\n",
    "print(response_ner)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Mistral-7B LLM returned a string containing json. Below we convert it to json (a dictionary -like format) and use a dataframe to display it.  Different LLM's will likely return different versions of the output which would need to be processed differently.  Some LLMs are capable of producing JSON with a particular structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Samsung Galaxy S24 Ultra</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tech Haven</td>\n",
       "      <td>Organization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Google Pixel 8 Pro</td>\n",
       "      <td>Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$1,200</td>\n",
       "      <td>CurrencyAmount</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jamie</td>\n",
       "      <td>Person</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       text            type\n",
       "0  Samsung Galaxy S24 Ultra         Product\n",
       "1                Tech Haven    Organization\n",
       "2        Google Pixel 8 Pro         Product\n",
       "3                    $1,200  CurrencyAmount\n",
       "4                     Jamie          Person"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Clean the response_ner string\n",
    "response_ner = response_ner.strip('```json\\n').strip('\\n```')\n",
    "\n",
    "# Convert the cleaned response to a DataFrame for display\n",
    "ner_result = json.loads(response_ner)\n",
    "\n",
    "df = pd.DataFrame(ner_result['entities'])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the LLM is similar to that of the specialized model from HuggingFace.  If we want different output from the LLM we could include instructions for that in our system prompt.\n",
    "\n",
    "In Lesson 10 we'll learn more about Named Entity Recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task - Question Answering\n",
    "\n",
    "Question Answering (QA) is a subtask of information retrieval and natural language understanding that involves automatically answering questions posed by humans in a natural language. QA systems can be designed to answer questions based on a given context or a large corpus of documents. The goal is to provide accurate and relevant answers to user queries.\n",
    "\n",
    "Practical examples of QA include:\n",
    "- **Customer Support**: Providing instant answers to customer queries based on a knowledge base or FAQ, improving response times and customer satisfaction.\n",
    "- **Education**: Assisting students by answering questions related to their coursework or providing explanations for complex topics.\n",
    "- **Healthcare**: Offering medical professionals quick access to information from medical literature or patient records to support clinical decision-making.\n",
    "- **Search Engines**: Enhancing search results by directly providing answers to user queries, rather than just a list of relevant documents.\n",
    "\n",
    "Here we will let the HuggingFace transformers library provide its default model for QA and apply it to our customer feedback.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Question Answering**\n",
      "\n",
      "Model: distilbert/distilbert-base-cased-distilled-squad, Size: 65,192,450 parameters\n",
      "\n",
      "{'score': 0.40413808822631836, 'start': 218, 'end': 231, 'answer': 'a stock\n",
      "issue'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Question Answering\n",
    "print(\"\\n**Question Answering**\\n\")\n",
    "qa_pipeline = pipeline(\"question-answering\", device=device)\n",
    "print_pipeline_info(qa_pipeline)\n",
    "print(\"\")\n",
    "question = \"What is the main issue?\"\n",
    "qa_result = qa_pipeline(question=question, context=text)\n",
    "print(qa_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(qa_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA with an LLM and a Zero-Shot Prompt\n",
    "\n",
    "If we don't have much training data or just want something quick and easy we can also use an LLM to for QA.  Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main issue is poor service quality at Tech Haven, including delayed\n",
      "delivery, inadequate communication about order issues, sending the incorrect\n",
      "product, and unresponsive customer service that failed to acknowledge and\n",
      "address the customer's (Jamie) concerns promptly.\n"
     ]
    }
   ],
   "source": [
    "system_prompt_qa = \"\"\"You are an expert question answering model. Answer the question based on the context provided. Be succinct.\"\"\"\n",
    "user_prompt_qa = f\"Context: {text}\\nQuestion: What is the main issue?\\nAnswer:\"\n",
    "\n",
    "response_qa = llm_generate(llm_config, user_prompt_qa, system_prompt=system_prompt_qa)\n",
    "print(response_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Issue: Poor Customer Service & Delivery Mishap (wrong product delivered\n",
      "late)\n"
     ]
    }
   ],
   "source": [
    "### MAKE THIS AN EXERCISE TO GET A VERY CONCISE ANSWER\n",
    "\n",
    "system_prompt_qa = \"\"\"You are an expert question answering model. Answer the question based on the context provided. \n",
    "Your answer should be a few words at most.\"\"\"\n",
    "user_prompt_qa = f\"Context: {text}\\nQuestion: What is the main issue? Try to identify a single issue.\\nAnswer:\"\n",
    "\n",
    "response_qa = llm_generate(llm_config, user_prompt_qa, system_prompt=system_prompt_qa)\n",
    "print(response_qa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have a lesson dedicated to question answering, but it's discussed in our NLP textbook in Chapter 7.  You could investigate this topic further in a project if you're interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task - Translation\n",
    "\n",
    "The first transformer model in the paper \"Attention is All You Need\" was designed for the task of language translation. This model, known as the Transformer, introduced a novel architecture that relies entirely on self-attention mechanisms to process input sequences, making it highly effective for translation tasks. The Transformer model has since become the foundation for many state-of-the-art NLP models, including BERT, GPT, and others.\n",
    "\n",
    "Here we demonstrate how to use a pre-trained model from HuggingFace for translating English to Spanish.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Translation**\n",
      "\n",
      "Model: Helsinki-NLP/opus-mt-en-es, Size: 77,943,296 parameters\n",
      "\n",
      "Pedí el Samsung Galaxy S24 Ultra de Tech Haven, esperando la entrega del día\n",
      "siguiente, pero después de tres días, yo ni siquiera había recibido una\n",
      "actualización de envío. Después de esperar 45 minutos en espera, el servicio al\n",
      "cliente me dijo que había un problema de existencias — sin embargo nadie me\n",
      "había informado! Cuando el paquete finalmente llegó una semana tarde, que\n",
      "contenía un Google Pixel 8 Pro en su lugar. El representante de apoyo era\n",
      "apologético, pero dijo que un intercambio tomaría otras dos semanas. Pagué\n",
      "$1.200 por el teléfono equivocado, trató con retrasos y mala comunicación, y\n",
      "ahora tienen que esperar incluso más tiempo. Para añadir insulto a la lesión, el\n",
      "representante de servicio al cliente con el que hablé parecía indiferente a mi\n",
      "frustración. Tuve que explicar mi situación varias veces antes de que incluso\n",
      "reconocieron el error. Toda la experiencia ha sido increíblemente decepcionante\n",
      "y me ha dejado cuestionando si alguna vez debería comprar con Tech Haven de\n",
      "nuevo. Es desconcertante cómo una empresa puede funcionar con tal falta de\n",
      "transparencia y eficiencia. Espero que esta retroalimentación llega a alguien\n",
      "que puede hacer una diferencia, ya que ningún cliente debe ir a través de lo que\n",
      "hice. Tech Haven!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Translation (English to Spanish)\n",
    "print(\"\\n**Translation**\\n\")\n",
    "translation_pipeline = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\", device=device)\n",
    "print_pipeline_info(translation_pipeline)\n",
    "print(\"\")\n",
    "translation_result = translation_pipeline(text, max_length=300)\n",
    "print(translation_result[0]['translation_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps you're better than I am at Spanish, but I can't read that well enough to know if it's a good translation.  However, let's now use a similar model to translate it from Spanish back into English and compare it to the orginal text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Helsinki-NLP/opus-mt-es-en, Size: 77,943,296 parameters\n",
      "Back Translation to English: I ordered the Samsung Galaxy S24 Ultra from Tech\n",
      "Haven, waiting for delivery the next day, but after three days, I had not even\n",
      "received a shipping update. After waiting 45 minutes on hold, the customer\n",
      "service told me that there was a stock problem — yet no one had informed me!\n",
      "When the package finally arrived a week late, containing a Google Pixel 8 Pro\n",
      "instead. The support representative was apologetic, but he said an exchange\n",
      "would take another two weeks. I paid $1,200 for the wrong phone, tried with\n",
      "delays and bad communication, and now they have to wait even longer. To add\n",
      "insult to the injury, the customer service representative with whom I spoke\n",
      "seemed indifferent to my frustration. I had to explain my situation several\n",
      "times before they even recognized the error. All the experience has been\n",
      "incredibly disappointing and has left me wondering if I should ever buy with\n",
      "Tech Haven again. It is disconcerting how a company can function with such a\n",
      "lack of transparency and efficiency. I hope this feedback comes to someone who\n",
      "can make a difference, as no customer should go through what I did. Tech Haven!\n",
      "\n",
      "Original Text: I ordered the Samsung Galaxy S24 Ultra from Tech Haven, expecting\n",
      "next-day delivery, but after three days, I hadn’t even received a shipping\n",
      "update. After waiting 45 minutes on hold, customer service told me there was a\n",
      "stock issue—yet no one had informed me!\n",
      "\n",
      "When the package finally arrived a week late, it contained a Google Pixel 8 Pro\n",
      "instead. The support rep was apologetic but said an exchange would take another\n",
      "two weeks.\n",
      "\n",
      "I paid $1,200 for the wrong phone, dealt with delays and poor communication, and\n",
      "now have to wait even longer. To add insult to injury, the customer service\n",
      "representative I spoke with seemed indifferent to my frustration. I had to\n",
      "explain my situation multiple times before they even acknowledged the mistake.\n",
      "The entire experience has been incredibly disappointing and has left me\n",
      "questioning whether I should ever shop with Tech Haven again.\n",
      "\n",
      "It's baffling how a company can operate with such a lack of transparency and\n",
      "efficiency. I hope this feedback reaches someone who can make a difference, as\n",
      "no customer should have to go through what I did. Tech Haven, you need to do\n",
      "better! Sincerely, Jamie.\n"
     ]
    }
   ],
   "source": [
    "# Extract the Spanish translation from the previous result\n",
    "spanish_translation = translation_result[0]['translation_text']\n",
    "\n",
    "# Translate the Spanish text back to English\n",
    "back_translation_pipeline = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\", device=device)\n",
    "print_pipeline_info(back_translation_pipeline)\n",
    "back_translation_result = back_translation_pipeline(spanish_translation, max_length=300)\n",
    "print(f\"Back Translation to English: {back_translation_result[0]['translation_text']}\\n\")\n",
    "print(f\"Original Text: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The Helsinki-NLP models are part of the OPUS-MT project, which provides pre-trained neural machine translation models for many language pairs. These models are based on the MarianMT architecture, a transformer-based model optimized for translation tasks. The MarianMT architecture leverages self-attention mechanisms to effectively process and translate text, making these models highly accurate and efficient for translation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline cleared.\n",
      "✅ Pipeline cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(translation_pipeline)\n",
    "clear_pipeline(back_translation_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation with an LLM and a Zero-shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimado/a Tech Haven, ordené el Samsung Galaxy S24 Ultra esperando una entrega\n",
      "en un día hábil, pero después de tres días, aún no había recibido actualización\n",
      "sobre la envío. Después de haber estado por casi medio hora en cola, el servicio\n",
      "al cliente me informó que existía un problema con las reservas —sin embargo,\n",
      "nadie me lo había comunicado previamente!\n",
      "Cuando finalmente llegó el paquete un semana más tarde, contenía un Google Pixel\n",
      "8 Pro en su lugar. El representante de soporte se mostró arrepentida, pero dijo\n",
      "que para hacer un intercambio serían necesarias otras dos semanas.\n",
      "Pague $1,200 por el teléfono equivocado, tuve problemas y malas comunicaciones,\n",
      "y ahora tengo que esperar incluso más tiempo. Para agravar la situación, el\n",
      "representante de soporte con quien hablé pareció despreocupado ante mi\n",
      "frustración. Tuve que explicar mi situación varias veces antes de que\n",
      "reconocieran el error. La experiencia ha sido totalmente decepcionante e incita\n",
      "a preguntarse si alguna vez volveré a comprar en Tech Haven.\n",
      "Es increíble cómo una empresa puede operar con tal falta de transparencia y\n",
      "eficacia. Espero que esta información alcance a alguien capaz de cambiar algo,\n",
      "ya que ningún cliente debería pasar por lo que acabo de vivir. Tech Haven, debes\n",
      "mejorar! Atentamente, Jamie.\n"
     ]
    }
   ],
   "source": [
    "system_prompt_translation = \"\"\"You are an expert translation model. Translate the following text from English to Spanish.\"\"\"\n",
    "user_prompt_translation = f\"Text: {text}\\nTranslation:\"\n",
    "\n",
    "response_translation = llm_generate(llm_config, \n",
    "                                    user_prompt_translation, \n",
    "                                    system_prompt=system_prompt_translation,\n",
    "                                    max_new_tokens=500)\n",
    "print(response_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I requested the Samsung Galaxy S24 Ultra from Tech Haven, expecting delivery the\n",
      "next day, but after three days, I hadn't even received a shipping update. After\n",
      "waiting for 45 minutes on hold, customer service told me there was an inventory\n",
      "issue - however no one had informed me about it! When the package finally\n",
      "arrived a week late, it contained a Google Pixel 8 Pro instead. The support\n",
      "representative was apologetic, but said that an exchange would take another two\n",
      "weeks. I paid $1,200 for the wrong phone, dealt with delays and poor\n",
      "communication, and now have to wait even more time. To add injury to insult, the\n",
      "customer service representative I spoke with seemed uncaring towards my\n",
      "frustration. I had to explain my situation several times before they even\n",
      "acknowledged the error. Overall, this experience has been incredibly\n",
      "disappointing and left me questioning whether I should ever shop at Tech Haven\n",
      "again. It is concerning how a company can operate with such lack of transparency\n",
      "and efficiency. I hope this feedback reaches someone who can make a difference,\n",
      "as no customer should go through what I did. Tech Haven!\n"
     ]
    }
   ],
   "source": [
    "system_prompt_back_translation = \"\"\"You are an expert translation model. Translate the following text from Spanish to English.\"\"\"\n",
    "user_prompt_back_translation = f\"Text: {spanish_translation}\\nTranslation:\"\n",
    "\n",
    "response_back_translation = llm_generate(llm_config, \n",
    "                                         user_prompt_back_translation, \n",
    "                                         system_prompt=system_prompt_back_translation,\n",
    "                                         max_new_tokens=500)\n",
    "print(response_back_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't specifically study translation models in one of our lessons, the transformer models used for text summarization are similar in that they take an input sequence of text and produce an output sequence of text.  These models are called sequence to sequence transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task - Text Generation\n",
    "\n",
    "Of all the models we'll study, text-generation models are perhaps the most familiar since they are the machines that drive today's chatbots like ChatGPT, Gemini, Claude, and others. Given an input sequence that provides context, a text generation model predicts a likely next word, then does it again and again to generate a hopefully sensible response. These models are particularly useful for tasks such as drafting emails, writing code, creating conversational agents, and generating creative content like stories and poems.\n",
    "\n",
    "HuggingFace makes it simple to create a text generation pipeline.  Here we provide the original customer comment plus the beginning of customer service response and ask the modlel to generate 200 new tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Text Generation**\n",
      "\n",
      "Model: openai-community/gpt2, Size: 124,439,808 parameters\n",
      "I ordered the Samsung Galaxy S24 Ultra from Tech Haven, expecting next-day\n",
      "delivery, but after three days, I hadn’t even received a shipping update. After\n",
      "waiting 45 minutes on hold, customer service told me there was a stock issue—yet\n",
      "no one had informed me!\n",
      "\n",
      "When the package finally arrived a week late, it contained a Google Pixel 8 Pro\n",
      "instead. The support rep was apologetic but said an exchange would take another\n",
      "two weeks.\n",
      "\n",
      "I paid $1,200 for the wrong phone, dealt with delays and poor communication, and\n",
      "now have to wait even longer. To add insult to injury, the customer service\n",
      "representative I spoke with seemed indifferent to my frustration. I had to\n",
      "explain my situation multiple times before they even acknowledged the mistake.\n",
      "The entire experience has been incredibly disappointing and has left me\n",
      "questioning whether I should ever shop with Tech Haven again.\n",
      "\n",
      "It's baffling how a company can operate with such a lack of transparency and\n",
      "efficiency. I hope this feedback reaches someone who can make a difference, as\n",
      "no customer should have to go through what I did. Tech Haven, you need to do\n",
      "better! Sincerely, Jamie.\n",
      "\n",
      "Customer service response:\n",
      "Dear Jamie, I am sorry to hear that your order was mixed up. You ordered the\n",
      "first product from a trusted company; however, a delivery receipt says I did not\n",
      "receive the Samsung Galaxy S24, so there's no way for you to confirm that I've\n",
      "got the other.\n",
      "\n",
      "Unfortunately, I couldn't find the correct address for the customer support rep\n",
      "because her personal service number is incorrect, and she couldn't locate and\n",
      "call me back to say she had the order sent to her.\n",
      "\n",
      "Additionally, it is unclear by which company the order was sent, as I can't\n",
      "identify the phone number. It is possible that you bought it from a company\n",
      "which didn't have an internal rep, or that this order is an act of piracy to get\n",
      "your Google Pixel 8 phone. If so, it's also possible that because the company\n",
      "did not send a copy of the order to send to your email address, you didn't buy\n",
      "the Pixel phone from that company.\n",
      "\n",
      "Your company could have sent me an email advising that when I asked for an\n",
      "order, I was told that an error occurred, but there was no answer.\n",
      "\n",
      "I want to add the fact that my experience has forced me to face new challenges\n",
      "as a first-time customer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n**Text Generation**\\n\")\n",
    "generator_pipeline = pipeline(\"text-generation\", device=device)\n",
    "print_pipeline_info(generator_pipeline)\n",
    "response = \"Dear Jamie, I am sorry to hear that your order was mixed up.\"\n",
    "prompt = text + \"\\n\\nCustomer service response:\\n\" + response\n",
    "outputs = generator_pipeline(prompt, max_length=500)\n",
    "generated_text = outputs[0]['generated_text']\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the response includes the input prompt.  This is typical of text-generation models in HuggingFace, but it's easy to remove the input prompt from the output.  If you read the customer service response you can see that it's not very good.  GPT2, released by OpenAI in 2019, is a large transformer-based language model with 1.5 billion parameters. It was designed to generate coherent and contextually relevant text, but it can sometimes produce outputs that are not entirely accurate or appropriate.  Now there are much better text-generation models available in HuggingFace.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(generator_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation with Other LLMs\n",
    "\n",
    "`llm_generate` makes it simple to experiment with different models for text generation.  Here's a generated customer service response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Jamie,\n",
      "Thank you for taking the time to share your recent experience with us at Tech\n",
      "Haven. We sincerely apologize for the inconvenience and frustration caused by\n",
      "the events detailed in your message.\n",
      "Firstly, we deeply regret that the expected next-day delivery for your order of\n",
      "the Samsung Galaxy S24 Ultrawas not met, followed by incorrect item delivery and\n",
      "delayed communication about these matters. In addition, we understand your\n",
      "dissatisfaction with the extended period required for an exchange, which further\n",
      "compounded your disappointment.\n",
      "We want to assure you that our aim is always to deliver exceptional experiences\n",
      "to all customers, and unfortunately, in this instance, we fell short of meeting\n",
      "those standards. Upon investigating your case, we discovered that an oversight\n",
      "occurred during processing your order, leading to the shipment error. This\n",
      "incident does not reflect our usual procedures or commitment to providing\n",
      "accurate information and efficient services.\n",
      "To rectify the situation, we will process an immediate return for the Google\n",
      "Pixel 8 Pro and expedite the dispatch of your correct order for the Samsung\n",
      "Galaxy S24 Ultra at no additional cost to you. Furthermore, as a gesture of\n",
      "goodwill and to compensate for the inconveniences experienced, we shall credit\n",
      "back the full amount charged ($1,200) into your account within 3 business days.\n",
      "Regarding communication, we acknowledge that our representatives could have\n",
      "handled your concerns more promptly and sensitively, causing unnecessary stress\n",
      "and confusion. Rest assured, we will address this internally to ensure improved\n",
      "responses when dealing with similar situations in the future. Your valuable\n",
      "feedback serves as an important reminder that every interaction with our valued\n",
      "customers deserves attention, respect, and clear communication.\n",
      "As a token of appreciation for your patience throughout this ordeal, we invite\n",
      "you to contact our Customer Support Team\n",
      "([support@techhaven.com](mailto:support@techhaven.com)) to claim a complimentary\n",
      "accessory bundle of your choice, worth up to $100, to be applied upon successful\n",
      "completion of the exchange.\n",
      "Again, please accept our apologies for any distress caused, and thank you once\n",
      "more for bringing this matter to our attention so that we may learn from it and\n",
      "strive towards continuous improvement. It remains our priority to earn and\n",
      "maintain your trust, and we look forward to serving you better in the future.\n",
      "W\n"
     ]
    }
   ],
   "source": [
    "system_prompt_generation = \"\"\"You are an expert customer service representative. Generate a professional and empathetic response to the following customer feedback. Address the issues mentioned and provide a resolution.\"\"\"\n",
    "user_prompt_generation = f\"Customer Feedback: {text}\\n\\nCustomer service response:\"\n",
    "\n",
    "response_generation = llm_generate(llm_config, \n",
    "                                   user_prompt_generation, \n",
    "                                   system_prompt=system_prompt_generation,\n",
    "                                   max_new_tokens=500)\n",
    "print(response_generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll study text generation models in Lesson 11.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task - Summarization\n",
    "\n",
    "**Natural Language Processing (NLP) summarization** is the process of condensing a longer text into a shorter, more concise version while retaining its key information. There are two main types of summarization: **extractive** and **abstractive**. **Extractive summarization** selects and highlights the most important sentences or phrases directly from the original text without altering their wording. In contrast, **abstractive summarization** generates a new, rephrased summary that conveys the core meaning of the original content in a more natural and human-like manner. While extractive methods rely on ranking techniques, abstractive approaches often leverage deep learning models for text generation.\n",
    "\n",
    "We'll focus on abstractive summarization using HuggingFace pipelines.  Here we use a summarization model to create a summary of our customer complaint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Summarization**\n",
      "Model: sshleifer/distilbart-cnn-12-6, Size: 305,510,400 parameters\n",
      "[{'summary_text': ' Tech Haven sent a Samsung Galaxy S24 Ultra to Tech Haven,\n",
      "expecting next-day delivery . The package arrived a week late and contained a\n",
      "Google Pixel 8 Pro instead . The customer service rep was apologetic but said an\n",
      "exchange would take two weeks .'}]\n"
     ]
    }
   ],
   "source": [
    "# Summarization\n",
    "print(\"\\n**Summarization**\")\n",
    "summarization_pipeline = pipeline(\"summarization\", device=-1)\n",
    "print_pipeline_info(summarization_pipeline)\n",
    "summarization_result = summarization_pipeline(text, max_length=100, min_length=25, do_sample=False)\n",
    "print(summarization_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(summarization_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BART (Bidirectional and Auto-Regressive Transformers) is a denoising autoencoder for pretraining sequence-to-sequence models. It combines the bidirectional context of BERT with the autoregressive nature of GPT, making it highly effective for various NLP tasks, including text generation and summarization (Don't worry, we're going to make sense of many of those terms in future lessons...). `sshleifer/distilbart-cnn-12-6` is a distilled version of the BART model, specifically fine-tuned on the CNN/DailyMail dataset for abstractive summarization tasks. This model is designed to be smaller and faster than the original BART model while retaining most of its performance, making it efficient for generating concise summaries of longer texts.`sshleifer/distilbart-cnn-12-6` is a distilled version of the BART model, specifically fine-tuned on the CNN/DailyMail dataset for abstractive summarization tasks. This model is designed to be smaller and faster than the original BART model while retaining most of its performance, making it efficient for generating concise summaries of longer texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer Jamie purchased the Samsung Galaxy S24 Ulta from Tech Haven with\n",
      "expected next-day delivery. However, after three days without any updates, they\n",
      "contacted customer service to find out about a stock issue that wasn't\n",
      "communicated earlier. Upon receiving the package a week later, it contained a\n",
      "different device (Google Pixel 8 Pro). The promised exchange will also take\n",
      "additional two weeks. Despite apologies from the support team, Jamie experienced\n",
      "significant delays, poor communication, and felt unacknowledged throughout their\n",
      "interaction. They found the overall experience highly dissatisfying and question\n",
      "future transactions with Tech Haven due to perceived inefficiency and lack of\n",
      "transparency.\n"
     ]
    }
   ],
   "source": [
    "system_prompt_summarization = \"\"\"You are an expert summarization model. Summarize the following customer feedback in a concise manner.\"\"\"\n",
    "user_prompt_summarization = f\"Customer Feedback: {text}\\n\\nSummary:\"\n",
    "\n",
    "response_summarization = llm_generate(llm_config, \n",
    "                                      user_prompt_summarization, \n",
    "                                      system_prompt=system_prompt_summarization,\n",
    "                                      max_new_tokens=150)\n",
    "print(response_summarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Notes on Using LLMs Programatically\n",
    "\n",
    "While LLMs can make great all-purpose NLP tools, their use has some drawbacks as well:\n",
    "\n",
    "1.  They're usually configured to give **human sounding responses** which may not be what you want depending on the task.  You'll often have to experiment with the system prompt to get closer to what you want.\n",
    "\n",
    "2.  **LLMs don't always generate the same output.**  We'll learn more about text-generation in Lesson 11, but by default LLMs include some randomness in the generated text.  You can usually configure the LLM to use a deterministic next-word search to get repeatable results.  In `llm_generate` you can do this by passing `search_strategy = 'deterministic'`.\n",
    "\n",
    "3.  It can be **difficult to get an LLM to format the output** in the way that you want.  Carefully crafting the system prompt can help, but often some post-processing of the generated text is also necessary.  Here's a [4 minute video on DataCamp](https://campus.datacamp.com/courses/chatgpt-prompt-engineering-for-developers/introduction-to-prompt-engineering-best-practices?ex=9) discussing prompting to get structured outputs. (I was able to view the video without paying, but had to sign up for an account to do much else.)  The entire class \"ChatGPT Prompt Engineering for Developers\" looks worthwhile.  Recent LLMs such as GPT-4o and Gemini can produce output following JSON schema through their APIs.\n",
    "\n",
    "4.  **LLMs are usually slower than a specialized model.**  Especially if you're running the LLM locally.  While LLMs continue to improve, often fine-tuning a specialized model is still preferable if you have enough data and resources to do so, but if you don't have much training data or just need something quick using an LLM programatically can be beneficial.\n",
    "\n",
    "5.  **Few-shot prompting can improve the results** from an LLM.  Providing a one or more examples in the prompt can improve the LLM response.  You'll explore this a bit in the homework.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suggestion:** If you have a Gemini or OpenAI api key, go to the first code cell above and change the LLM_MODEL to 'gpt-4o-mini' or 'gemini-flash-lite' and rerun that cell and the subsequent LLM cells to see how the responses differ.  (I think the API based models give better results, but if had the hardware we could probably get similar results running locally as well.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
