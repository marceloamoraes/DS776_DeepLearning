{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install ../Course_Tools/introdl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from introdl.utils import get_device, wrap_print_text, config_paths_keys\n",
    "from introdl.nlp import llm_configure, llm_generate, clear_pipeline, print_pipeline_info, display_markdown\n",
    "\n",
    "# overload print to wrap text\n",
    "print = wrap_print_text(print)\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "paths = config_paths_keys()\n",
    "\n",
    "os.environ[\"GEMINI_API_KEY\"] = 'AIzaSyB1fE6BH4ZHF4nFbWVvoErTrVzvEAumD7s'\n",
    "LLM_MODEL = 'mistral-7B'\n",
    "LLM_MODEL = 'gemini-flash-lite'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP Tasks with Transformer Models\n",
    "\n",
    "In this notebook we'll demonstrate solutions to some common Natural Language Processing (NLP) tasks that use transformer models.  We expand on the material in our NLP textbook Chapter 1 - Hello Transformers.  We'll add a little background about the underlying models.  We'll also demonstrate how these same tasks come be done using a large language model with either \"zero-shot prompting\" or \"few-shot prompting\".\n",
    "\n",
    "Over the next five lessons we'll go into some of these NLP tasks in detail and a learn a bit about the transformer neural network architecture.  For each of the NLP tasks that follows we'll demonstrate how to do the task two ways.  The first is by using a pre-trained transformer-based model downloaded from HuggingFace.  In the second approach we'll use a a large language model and prompting.\n",
    "\n",
    "Using LLMs for various NLP tasks is common when there isn't much labeled data available.  Zero-shot prompting means that no examples are provided to the LLM.  Few-shot prompting means that a small number of examples are provided to the LLM.  In this notebook we'll demonstrate zero-shot prompting, but in the lessons to come we'll include few-shot prompting examples.\n",
    "\n",
    "Throughout this notebook we'll use the following customer feedback message that as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Text\n",
    "text = \"\"\"I ordered the Samsung Galaxy S24 Ultra from Tech Haven, expecting next-day delivery, \n",
    "but after three days, I hadn‚Äôt even received a shipping update. After waiting 45 minutes on hold, \n",
    "customer service told me there was a stock issue‚Äîyet no one had informed me! \n",
    "\n",
    "When the package finally arrived a week late, it contained a Google Pixel 8 Pro instead. \n",
    "The support rep was apologetic but said an exchange would take another two weeks.  \n",
    "\n",
    "I paid $1,200 for the wrong phone, dealt with delays and poor communication, and now have to wait even longer. \n",
    "Tech Haven, you need to do better! Sincerely, Jamie.\n",
    "\n",
    "To add insult to injury, the customer service representative I spoke with seemed indifferent to my frustration. \n",
    "I had to explain my situation multiple times before they even acknowledged the mistake. \n",
    "The entire experience has been incredibly disappointing and has left me questioning whether I should ever shop with Tech Haven again. \n",
    "It's baffling how a company can operate with such a lack of transparency and efficiency. \n",
    "I hope this feedback reaches someone who can make a difference, as no customer should have to go through what I did.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task - Text Classification\n",
    "\n",
    "Text classification is the process of assigning predefined categories to text. It involves analyzing the content of the text and categorizing it based on its subject, sentiment, or other criteria. One common application of text classification is sentiment analysis, which determines the sentiment expressed in a piece of text, such as positive, negative, or neutral. Sentiment analysis is widely used in customer feedback analysis, social media monitoring, and market research to gauge public opinion and customer satisfaction.\n",
    "\n",
    "### Sentiment Analysis with a Specialized Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will let the HuggingFace transformers library provide its default model for sentiment analysis and apply it to our customer feedback.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Sentiment Analysis**\n",
      "Model: distilbert/distilbert-base-uncased-finetuned-sst-2-english, Size: 66,955,010 parameters\n",
      "[{'label': 'NEGATIVE', 'score': 0.9990748167037964}]\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Analysis\n",
    "print(\"\\n**Sentiment Analysis**\")\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", device=device)\n",
    "print_pipeline_info(sentiment_pipeline)\n",
    "sentiment_result = sentiment_pipeline(text)\n",
    "print(sentiment_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, a \"BERT\" model correctly classified the customer feedback as negative. BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model developed by Google. It is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. This allows BERT to understand the context of a word based on its surroundings, making it highly effective for various NLP tasks. The particular model used here is a distilled BERT model that has been fine-tuned on a sentiment dataset. A distilled model is a smaller, faster, and more efficient version of a larger model, trained using knowledge distillation, where the smaller model learns to mimic the outputs of the larger one while retaining most of its performance. In Lesson 9, we'll learn more about the family of transformer models called encoders, which include BERT models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's never a bad idea to remove models from memory when they aren't being used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline cleared from CPU.\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(sentiment_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Sentiment Analysis with an LLM and a Zero-Shot Prompt\n",
    "\n",
    "A system prompt is used to give instructions to an LLM while a user prompt is the specific input you want the LLM to respond to.  Here we define a system prompt for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"You are an expert sentiment analysis model. Analyze the sentiment of the following text. \n",
    "Give only a one word response: positive, negative, or neutral.\"\"\"\n",
    "user_prompt = f\"Text: {text}\\nSentiment:\"\n",
    "\n",
    "llm_config = llm_configure(LLM_MODEL)\n",
    "response_zero_shot = llm_generate(llm_config, user_prompt, system_prompt=system_prompt)\n",
    "print(response_zero_shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also handle batches of inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Fast shipping and great customer support. Highly recommend!\n",
      "Sentiment: Positive\n",
      "\n",
      "Text: The item arrived damaged and the return process was a nightmare.\n",
      "Sentiment: Negative\n",
      "\n",
      "Text: I'm very satisfied with my purchase. Will buy again.\n",
      "Sentiment: Positive\n",
      "\n",
      "Text: The website is user-friendly and the prices are unbeatable.\n",
      "Sentiment: Positive\n",
      "\n",
      "Text: Received the wrong item and customer service was unhelpful.\n",
      "Sentiment: Negative\n",
      "\n",
      "Text: Fantastic experience from start to finish.\n",
      "Sentiment: Positive\n",
      "\n",
      "Text: The product is okay, but not worth the price.\n",
      "Sentiment: Negative\n",
      "\n",
      "Text: Excellent quality and quick delivery. Very happy!\n",
      "Sentiment: Positive\n",
      "\n",
      "Text: The product works as expected, nothing more, nothing less.\n",
      "Sentiment: Neutral\n",
      "\n",
      "Text: I have mixed feelings about the service; it was both good and bad.\n",
      "Sentiment: Neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "customer_comments = [\n",
    "    \"Fast shipping and great customer support. Highly recommend!\",\n",
    "    \"The item arrived damaged and the return process was a nightmare.\",\n",
    "    \"I'm very satisfied with my purchase. Will buy again.\",\n",
    "    \"The website is user-friendly and the prices are unbeatable.\",\n",
    "    \"Received the wrong item and customer service was unhelpful.\",\n",
    "    \"Fantastic experience from start to finish.\",\n",
    "    \"The product is okay, but not worth the price.\",\n",
    "    \"Excellent quality and quick delivery. Very happy!\",\n",
    "    \"The product works as expected, nothing more, nothing less.\",\n",
    "    \"I have mixed feelings about the service; it was both good and bad.\"\n",
    "]\n",
    "\n",
    "user_prompts = [f\"Text: {comment}\\nSentiment:\" for comment in customer_comments]\n",
    "\n",
    "responses_zero_shot = llm_generate(llm_config, user_prompts, system_prompt=system_prompt)\n",
    "\n",
    "for comment, response_zero_shot in zip(customer_comments, responses_zero_shot):\n",
    "    print(f\"Text: {comment}\\nSentiment: {response_zero_shot}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll study text classification more in Lesson 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task - Named Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.\n",
    "\n",
    "Practical examples of NER include:\n",
    "- **Business Application**: Extracting company names, dates, and monetary amounts from financial reports to automate data entry and analysis.\n",
    "- **Healthcare**: Identifying patient names, medical conditions, and treatment dates from clinical notes to improve patient record management.\n",
    "- **News Aggregation**: Categorizing and tagging entities like people, places, and events in news articles to enhance search and recommendation systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will let the HuggingFace transformers library provide its default model for NER and apply it to our customer feedback.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Named Entity Recognition**\n",
      "\n",
      "Model: dbmdz/bert-large-cased-finetuned-conll03-english, Size: 332,538,889 parameters\n",
      "\n",
      "[{'entity_group': 'MISC', 'score': np.float32(0.9907838), 'word': 'Samsung\n",
      "Galaxy S24 Ultra', 'start': 14, 'end': 38}, {'entity_group': 'ORG', 'score':\n",
      "np.float32(0.9947492), 'word': 'Tech Haven', 'start': 44, 'end': 54},\n",
      "{'entity_group': 'MISC', 'score': np.float32(0.9926708), 'word': 'Google Pixel 8\n",
      "Pro', 'start': 325, 'end': 343}, {'entity_group': 'ORG', 'score':\n",
      "np.float32(0.97864664), 'word': 'Tech Haven', 'start': 551, 'end': 561},\n",
      "{'entity_group': 'PER', 'score': np.float32(0.9833628), 'word': 'Jamie',\n",
      "'start': 597, 'end': 602}, {'entity_group': 'ORG', 'score':\n",
      "np.float32(0.9962701), 'word': 'Tech Haven', 'start': 923, 'end': 933}]\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition (NER)\n",
    "print(\"\\n**Named Entity Recognition**\\n\")\n",
    "ner_pipeline = pipeline(\"ner\", aggregation_strategy=\"simple\", device=device)\n",
    "print_pipeline_info(ner_pipeline)\n",
    "print(\"\")\n",
    "ner_result = ner_pipeline(text)\n",
    "print(ner_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That output is hard to read, but we can easily convert it to a Pandas data frame for display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "entity_group",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "score",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "word",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "start",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "end",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "27e6c2b6-6fe0-4d6a-8d7b-bd84b5ffb284",
       "rows": [
        [
         "0",
         "MISC",
         "0.9907838",
         "Samsung Galaxy S24 Ultra",
         "14",
         "38"
        ],
        [
         "1",
         "ORG",
         "0.9947492",
         "Tech Haven",
         "44",
         "54"
        ],
        [
         "2",
         "MISC",
         "0.9926708",
         "Google Pixel 8 Pro",
         "325",
         "343"
        ],
        [
         "3",
         "ORG",
         "0.97864664",
         "Tech Haven",
         "551",
         "561"
        ],
        [
         "4",
         "PER",
         "0.9833628",
         "Jamie",
         "597",
         "602"
        ],
        [
         "5",
         "ORG",
         "0.9962701",
         "Tech Haven",
         "923",
         "933"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.990784</td>\n",
       "      <td>Samsung Galaxy S24 Ultra</td>\n",
       "      <td>14</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.994749</td>\n",
       "      <td>Tech Haven</td>\n",
       "      <td>44</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.992671</td>\n",
       "      <td>Google Pixel 8 Pro</td>\n",
       "      <td>325</td>\n",
       "      <td>343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.978647</td>\n",
       "      <td>Tech Haven</td>\n",
       "      <td>551</td>\n",
       "      <td>561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.983363</td>\n",
       "      <td>Jamie</td>\n",
       "      <td>597</td>\n",
       "      <td>602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.996270</td>\n",
       "      <td>Tech Haven</td>\n",
       "      <td>923</td>\n",
       "      <td>933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score                      word  start  end\n",
       "0         MISC  0.990784  Samsung Galaxy S24 Ultra     14   38\n",
       "1          ORG  0.994749                Tech Haven     44   54\n",
       "2         MISC  0.992671        Google Pixel 8 Pro    325  343\n",
       "3          ORG  0.978647                Tech Haven    551  561\n",
       "4          PER  0.983363                     Jamie    597  602\n",
       "5          ORG  0.996270                Tech Haven    923  933"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "df = pd.DataFrame(ner_result)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"BERT\" model used here is the `dbmdz/bert-large-cased-finetuned-conll03-english` model, which has been fine-tuned on the CoNLL-2003 dataset for Named Entity Recognition (NER). This fine-tuning process allows the model to accurately identify and classify entities such as names of persons, organizations, locations, and more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline cleared from CPU.\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(ner_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER with an LLM and a Zero-Shot Prompt\n",
    "\n",
    "If we don't have much training data or just want something quick and easy we can also use an LLM to for NER.  Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "  {\n",
      "    \"entity\": \"Samsung Galaxy S24 Ultra\",\n",
      "    \"type\": \"PRODUCT\"\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"Tech Haven\",\n",
      "    \"type\": \"ORGANIZATION\"\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"Google Pixel 8 Pro\",\n",
      "    \"type\": \"PRODUCT\"\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"Jamie\",\n",
      "    \"type\": \"PERSON\"\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"$1,200\",\n",
      "    \"type\": \"MONEY\"\n",
      "  }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "system_prompt = \"\"\"You are an expert named entity recognition model. Identify and classify the entities in the following text. \n",
    "Provide the entities and their types in a JSON format.\"\"\"\n",
    "user_prompt = f\"Text: {text}\\nEntities:\"\n",
    "\n",
    "llm_config = llm_configure(LLM_MODEL)\n",
    "response_ner = llm_generate(llm_config, user_prompt, system_prompt=system_prompt)\n",
    "print(response_ner)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLM returned markdown to display the json result.  To make it easier to display we'll strip the markdown bits and load it as actual json, then convert it to a data frame for display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'Samsung Galaxy S24 Ultra', 'type': 'PRODUCT'}, {'entity': 'Tech\n",
      "Haven', 'type': 'ORGANIZATION'}, {'entity': 'Google Pixel 8 Pro', 'type':\n",
      "'PRODUCT'}, {'entity': 'Jamie', 'type': 'PERSON'}, {'entity': '$1,200', 'type':\n",
      "'MONEY'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "entity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "type",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "1bf45ff1-aa47-4e42-9100-27487c5f6fec",
       "rows": [
        [
         "0",
         "Samsung Galaxy S24 Ultra",
         "PRODUCT"
        ],
        [
         "1",
         "Tech Haven",
         "ORGANIZATION"
        ],
        [
         "2",
         "Google Pixel 8 Pro",
         "PRODUCT"
        ],
        [
         "3",
         "Jamie",
         "PERSON"
        ],
        [
         "4",
         "$1,200",
         "MONEY"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Samsung Galaxy S24 Ultra</td>\n",
       "      <td>PRODUCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tech Haven</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Google Pixel 8 Pro</td>\n",
       "      <td>PRODUCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jamie</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$1,200</td>\n",
       "      <td>MONEY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     entity          type\n",
       "0  Samsung Galaxy S24 Ultra       PRODUCT\n",
       "1                Tech Haven  ORGANIZATION\n",
       "2        Google Pixel 8 Pro       PRODUCT\n",
       "3                     Jamie        PERSON\n",
       "4                    $1,200         MONEY"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Clean the response_ner string\n",
    "cleaned_response_ner = response_ner.strip('```json\\n').strip('\\n```')\n",
    "\n",
    "# Convert the cleaned response to a DataFrame for display\n",
    "ner_result = json.loads(cleaned_response_ner)\n",
    "print(ner_result)\n",
    "\n",
    "df = pd.DataFrame(ner_result)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the LLM is similar to that of the specialized model from HuggingFace.  If we want different output from the LLM we could include instructions for that in our system prompt.\n",
    "\n",
    "In Lesson 10 we'll learn more about Named Entity Recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Task - Question Answering\n",
    "\n",
    "Question Answering (QA) is a subtask of information retrieval and natural language understanding that involves automatically answering questions posed by humans in a natural language. QA systems can be designed to answer questions based on a given context or a large corpus of documents. The goal is to provide accurate and relevant answers to user queries.\n",
    "\n",
    "Practical examples of QA include:\n",
    "- **Customer Support**: Providing instant answers to customer queries based on a knowledge base or FAQ, improving response times and customer satisfaction.\n",
    "- **Education**: Assisting students by answering questions related to their coursework or providing explanations for complex topics.\n",
    "- **Healthcare**: Offering medical professionals quick access to information from medical literature or patient records to support clinical decision-making.\n",
    "- **Search Engines**: Enhancing search results by directly providing answers to user queries, rather than just a list of relevant documents.\n",
    "\n",
    "Here we will let the HuggingFace transformers library provide its default model for QA and apply it to our customer feedback.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Question Answering**\n",
      "\n",
      "Model: distilbert/distilbert-base-cased-distilled-squad, Size: 65,192,450 parameters\n",
      "\n",
      "{'score': 0.39593052864074707, 'start': 220, 'end': 233, 'answer': 'a stock\n",
      "issue'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Question Answering\n",
    "print(\"\\n**Question Answering**\\n\")\n",
    "qa_pipeline = pipeline(\"question-answering\", device=device)\n",
    "print_pipeline_info(qa_pipeline)\n",
    "print(\"\")\n",
    "question = \"What is the main issue?\"\n",
    "qa_result = qa_pipeline(question=question, context=text)\n",
    "print(qa_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline cleared.  Freed 260.77 MB of CUDA memory\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(qa_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA with an LLM and a Zero-Shot Prompt\n",
    "\n",
    "If we don't have much training data or just want something quick and easy we can also use an LLM to for QA.  Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main issue is that Jamie ordered a Samsung Galaxy S24 Ultra from Tech Haven\n",
      "but received a Google Pixel 8 Pro a week late, after experiencing delays, poor\n",
      "communication, and indifferent customer service.\n"
     ]
    }
   ],
   "source": [
    "system_prompt_qa = \"\"\"You are an expert question answering model. Answer the question based on the context provided. Be succinct.\"\"\"\n",
    "user_prompt_qa = f\"Context: {text}\\nQuestion: What is the main issue?\\nAnswer:\"\n",
    "\n",
    "response_qa = llm_generate(llm_config, user_prompt_qa, system_prompt=system_prompt_qa)\n",
    "print(response_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong phone delivered.\n"
     ]
    }
   ],
   "source": [
    "### MAKE THIS AN EXERCISE TO GET A VERY CONCISE ANSWER\n",
    "\n",
    "system_prompt_qa = \"\"\"You are an expert question answering model. Answer the question based on the context provided. \n",
    "Your answer should be a few words at most.\"\"\"\n",
    "user_prompt_qa = f\"Context: {text}\\nQuestion: What is the main issue?\\nAnswer:\"\n",
    "\n",
    "response_qa = llm_generate(llm_config, user_prompt_qa, system_prompt=system_prompt_qa)\n",
    "print(response_qa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have a lesson dedicated to question answering, but it's discussed in our NLP textbook in Chapter 7.  You could investigate this topic further in a project if you're interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.2.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Using cached sentencepiece-0.2.0-cp311-cp311-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[33mWARNING: Error parsing dependencies of pytorch-lightning: .* suffix can only be used with `==` or `!=` operators\n",
      "    torch (>=1.9.*)\n",
      "           ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Translation**\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Translation (English to Spanish)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m**Translation**\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m translation_pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtranslation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHelsinki-NLP/opus-mt-en-es\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m print_pipeline_info(translation_pipeline)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/DS776v2/lib/python3.11/site-packages/transformers/pipelines/__init__.py:999\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    996\u001b[0m             tokenizer_kwargs \u001b[38;5;241m=\u001b[39m model_kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    997\u001b[0m             tokenizer_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 999\u001b[0m         tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer_identifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_fast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_image_processor:\n\u001b[1;32m   1004\u001b[0m     \u001b[38;5;66;03m# Try to infer image processor from model or config name (if provided as str)\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/DS776v2/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:931\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_py\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    930\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    932\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    933\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min order to use this tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    934\u001b[0m             )\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to build an AutoTokenizer.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    938\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mTOKENIZER_MAPPING\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    939\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer."
     ]
    }
   ],
   "source": [
    "\n",
    "# Translation (English to Spanish)\n",
    "print(\"\\n**Translation**\\n\")\n",
    "translation_pipeline = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\", device=device)\n",
    "print_pipeline_info(translation_pipeline)\n",
    "print(\"\")\n",
    "translation_result = translation_pipeline(text, max_length=200)\n",
    "print(translation_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'translation_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Extract the Spanish translation from the previous result\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m spanish_translation \u001b[38;5;241m=\u001b[39m \u001b[43mtranslation_result\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpanish Translation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspanish_translation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Translate the Spanish text back to English\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'translation_result' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract the Spanish translation from the previous result\n",
    "spanish_translation = translation_result[0]['translation_text']\n",
    "print(f\"Spanish Translation: {spanish_translation}\")\n",
    "\n",
    "# Translate the Spanish text back to English\n",
    "back_translation_pipeline = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\", device=device)\n",
    "print_pipeline_info(back_translation_pipeline)\n",
    "back_translation_result = back_translation_pipeline(spanish_translation, max_length=200)\n",
    "print(f\"Back Translation to English: {back_translation_result[0]['translation_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline cleared.  Freed 312.03 MB of CUDA memory\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(translation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Text Generation**\n",
      "Model: openai-community/gpt2, Size: 124,439,808 parameters\n",
      "I ordered the Samsung Galaxy S24 Ultra from Tech Haven, expecting next-day\n",
      "delivery,\n",
      "but after three days, I hadn‚Äôt even received a shipping update. After waiting 45\n",
      "minutes on hold,\n",
      "customer service told me there was a stock issue‚Äîyet no one had informed me!\n",
      "\n",
      "When the package finally arrived a week late, it contained a Google Pixel 8 Pro\n",
      "instead.\n",
      "The support rep was apologetic but said an exchange would take another two\n",
      "weeks.\n",
      "\n",
      "I paid $1,200 for the wrong phone, dealt with delays and poor communication, and\n",
      "now have to wait even longer.\n",
      "Tech Haven, you need to do better! Sincerely, Jamie.\n",
      "\n",
      "Customer service response:\n",
      "Dear Jamie, I am sorry to hear that your order was mixed up.\n",
      "\n",
      "However, due to the delay, the issue was resolved within 2 weeks, and the order\n",
      "has been delivered to its original address.\n",
      "\n",
      "Customer service response:\n",
      "Dear Jamie, I am sorry to hear that your order was mixed up.\n",
      "\n",
      "However, due to the delay, the issue was resolved within 2 weeks, and the order\n",
      "has been delivered to its original address.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n**Text Generation**\")\n",
    "generator_pipeline = pipeline(\"text-generation\", device=device)\n",
    "print_pipeline_info(generator_pipeline)\n",
    "response = \"Dear Jamie, I am sorry to hear that your order was mixed up.\"\n",
    "prompt = text + \"\\n\\nCustomer service response:\\n\" + response\n",
    "outputs = generator_pipeline(prompt, max_length=200)\n",
    "generated_text = outputs[0]['generated_text']\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline cleared.  Freed 511.15 MB of CUDA memory\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(generator_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Summarization**\n",
      "Model: sshleifer/distilbart-cnn-12-6, Size: 305,510,400 parameters\n",
      "[{'summary_text': ' Tech Haven sent a Samsung Galaxy S24 Ultra to Tech Haven,\n",
      "expecting next-day delivery . The package arrived a week late, it contained a\n",
      "Google Pixel 8 Pro instead . An exchange would take another two weeks .'}]\n"
     ]
    }
   ],
   "source": [
    "# Summarization\n",
    "print(\"\\n**Summarization**\")\n",
    "summarization_pipeline = pipeline(\"summarization\", device=device)\n",
    "print_pipeline_info(summarization_pipeline)\n",
    "summarization_result = summarization_pipeline(text, max_length=50, min_length=25, do_sample=False)\n",
    "print(summarization_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline cleared.  Freed 1222.24 MB of CUDA memory\n"
     ]
    }
   ],
   "source": [
    "clear_pipeline(summarization_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõë Unloading model: openai-community/gpt2 from GPU...\n",
      "‚úÖ Model openai-community/gpt2 has been fully unloaded.\n",
      "üöÄ Loading model: unsloth/mistral-7b-instruct-v0.3-bnb-4bit (this may take a while)...\n",
      "üü¢ Model unsloth/mistral-7b-instruct-v0.3-bnb-4bit loaded successfully.\n",
      "\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from introdl.nlp import llm_configure, llm_generate\n",
    "from introdl.utils import wrap_print_text, get_device\n",
    "\n",
    "print = wrap_print_text(print)\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "mistral_config = llm_configure(\"mistral\")\n",
    "response = llm_generate(mistral_config, \"What is the capital of France?\")\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "# Sample Text\n",
    "text = \"\"\"I ordered the Samsung Galaxy S24 Ultra from Tech Haven, expecting next-day delivery, \n",
    "but after three days, I hadn‚Äôt even received a shipping update. After waiting 45 minutes on hold, \n",
    "customer service told me there was a stock issue‚Äîyet no one had informed me! \n",
    "\n",
    "When the package finally arrived a week late, it contained a Google Pixel 8 Pro instead. \n",
    "The support rep was apologetic but said an exchange would take another two weeks.  \n",
    "\n",
    "I paid $1,200 for the wrong phone, dealt with delays and poor communication, and now have to wait even longer. \n",
    "Tech Haven, you need to do better! Sincerely, Jamie.\"\"\"\n",
    "\n",
    "\n",
    "system_prompt_sentiment = \"\"\"You are an expert sentiment analysis model. Analyze the sentiment of the following text. \n",
    "Give only a one word response: positive, negative, or neutral.\"\"\"\n",
    "user_prompt = f\"Text: {text}\\nSentiment:\"\n",
    "\n",
    "response = llm_generate(mistral_config, user_prompt, system_prompt=system_prompt_sentiment)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"name\": \"Samsung Galaxy S24 Ultra\",\n",
      "      \"type\": \"Product\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Tech Haven\",\n",
      "      \"type\": \"Organization\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"$1,200\",\n",
      "      \"type\": \"CurrencyAmount\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Google Pixel 8 Pro\",\n",
      "      \"type\": \"Product\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "system_prompt_ner = \"\"\"You are an expert named entity recognition model. Identify and classify the entities in the following text. \n",
    "Provide the entities and their types in a JSON format.\"\"\"\n",
    "user_prompt_ner = f\"Text: {text}\\nEntities:\"\n",
    "\n",
    "response_ner = llm_generate(mistral_config, user_prompt_ner, system_prompt=system_prompt_ner)\n",
    "print(response_ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The issue involves incorrect order delivery (Google Pixel 8 Pro instead of\n",
      "Samsung Galaxy S24 Ultra), delay in shipment, lack of timely communication about\n",
      "the stock issue, and further prolonged time for an exchange.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"What is the issue?\"\n",
    "system_prompt_qa = \"\"\"You are an expert question answering model. Answer the question based on the context provided. Be succinct\"\"\"\n",
    "user_prompt_qa = f\"Context: {text}\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "response_qa = llm_generate(mistral_config, user_prompt_qa, system_prompt=system_prompt_qa)\n",
    "print(response_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimado Tech Haven, orden√© el Samsung Galaxy S24 Ultra esperando entrega en un\n",
      "d√≠a,\n",
      "sin embargo, despu√©s de tres d√≠as, a√∫n no hab√≠a recibido una actualizaci√≥n de\n",
      "env√≠o. Despu√©s de pasar cuarenta y cinco minutos en llamada por l√≠nea\n",
      "telef√≥nica,\n",
      "el servicio al cliente me inform√≥ que exist√≠a un problema de stock ‚Äî sin\n",
      "embargo, nadie me lo hab√≠a comunicado previamente!\n",
      "Cuando finalmente lleg√≥ la paqueter√≠a retrasada por una semana, conten√≠a un\n",
      "Google Pixel 8 Pro en su lugar. El representante de soporte se disculp√≥ pero\n",
      "dijo que para realizar una intercambio necesitar√≠an otras dos semanas.\n",
      "Pague $1,200 por el tel√©fono equivocado, tuve que lidiar con retras\n"
     ]
    }
   ],
   "source": [
    "system_prompt_translation = \"\"\"You are an expert translation model. Translate the following text from English to Spanish.\"\"\"\n",
    "user_prompt_translation = f\"Text: {text}\\nTranslation:\"\n",
    "\n",
    "response_translation = llm_generate(mistral_config, user_prompt_translation, system_prompt=system_prompt_translation)\n",
    "print(response_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer Jamie experienced delayed delivery of the Samsung Galaxy S24 Ulta from\n",
      "Tech Haven, receiving a Google Pixel 8 Pro instead due to a stock issue that\n",
      "wasn't communicated. The customer service representative offered an exchange,\n",
      "but this process will also take additional time. Overall, Jamie is dissatisfied\n",
      "with the service provided by Tech Haven, citing issues with communication,\n",
      "delays, and incorrect product shipment.\n"
     ]
    }
   ],
   "source": [
    "system_prompt_summarization = \"\"\"You are an expert summarization model. Summarize the following text in a concise manner.\"\"\"\n",
    "user_prompt_summarization = f\"Text: {text}\\nSummary:\"\n",
    "\n",
    "response_summarization = llm_generate(mistral_config, user_prompt_summarization, system_prompt=system_prompt_summarization)\n",
    "print(response_summarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Jamie,\n",
      "Thank you for taking the time to share your experience with us at Tech Haven. We\n",
      "deeply regret the inconvenience you've encountered during your recent purchase\n",
      "of the Samsung Galaxy S24 Ultra.\n",
      "We understand that prompt delivery and clear communication are crucial aspects\n",
      "in maintaining trust with our customers, and we sincerely apologize for falling\n",
      "short of these expectations. The mix-up with the order is unacceptable, and we\n",
      "should have kept you updated about the stock issues earlier.\n",
      "To address this matter, we will immediately process a full refund for the\n",
      "incorrectly shipped device as well as cover any additional costs related to\n",
      "returning the Google Pixel 8 Pro back to us. Furthermore, we will prioritize\n",
      "sending out the correct product (Samsung Galaxy S24 Ultra) without delay, along\n",
      "with expedited shipping to ensure timely arrival.\n",
      "As a token of appreciation for your patience throughout this ordeal, we will\n"
     ]
    }
   ],
   "source": [
    "system_prompt_generation = \"\"\"You are an expert text generation model. Generate a customer service response based on the provided context.\"\"\"\n",
    "user_prompt_generation = f\"Context: {text}\\n\\nCustomer service response:\\n{response}\"\n",
    "\n",
    "response_generation = llm_generate(mistral_config, user_prompt_generation, system_prompt=system_prompt_generation)\n",
    "print(response_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\bagge\\my drive\\python_projects\\ds776_develop_project\\ds776\\lessons\\course_tools\\introdl\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: IPython in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (8.28.0)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (8.1.5)\n",
      "Requirement already satisfied: ipycanvas in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (0.13.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (3.9.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (2.2.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (1.14.1)\n",
      "Requirement already satisfied: seaborn in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (0.13.2)\n",
      "Requirement already satisfied: torch in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (2.4.1)\n",
      "Requirement already satisfied: torchinfo in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (1.8.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (0.19.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from introdl==1.0) (4.66.5)\n",
      "Requirement already satisfied: pillow>=6.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from ipycanvas->introdl==1.0) (10.4.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from ipywidgets->introdl==1.0) (0.2.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from ipywidgets->introdl==1.0) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from ipywidgets->introdl==1.0) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from ipywidgets->introdl==1.0) (3.0.13)\n",
      "Requirement already satisfied: decorator in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (2.18.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from IPython->introdl==1.0) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from matplotlib->introdl==1.0) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from matplotlib->introdl==1.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from matplotlib->introdl==1.0) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from matplotlib->introdl==1.0) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from matplotlib->introdl==1.0) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from matplotlib->introdl==1.0) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from matplotlib->introdl==1.0) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from pandas->introdl==1.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from pandas->introdl==1.0) (2024.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from torch->introdl==1.0) (3.16.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from torch->introdl==1.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from torch->introdl==1.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from torch->introdl==1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from torch->introdl==1.0) (2024.6.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from jedi>=0.16->IPython->introdl==1.0) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->IPython->introdl==1.0) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->introdl==1.0) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from jinja2->torch->introdl==1.0) (3.0.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from stack-data->IPython->introdl==1.0) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from stack-data->IPython->introdl==1.0) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from stack-data->IPython->introdl==1.0) (0.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776_env\\lib\\site-packages (from sympy->torch->introdl==1.0) (1.3.0)\n",
      "Building wheels for collected packages: introdl\n",
      "  Building wheel for introdl (pyproject.toml): started\n",
      "  Building wheel for introdl (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for introdl: filename=introdl-1.0-py3-none-any.whl size=41669 sha256=ef0876a17990433a5c0cf7239eec100df7f67d9a9e3cbccbe83cea9d2b439e2b\n",
      "  Stored in directory: C:\\Users\\bagge\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-2_41wy4l\\wheels\\f5\\d5\\0f\\11f1d5af64d00defb23fa33cf51b2946a0899888d73571e687\n",
      "Successfully built introdl\n",
      "Installing collected packages: introdl\n",
      "  Attempting uninstall: introdl\n",
      "    Found existing installation: introdl 1.0\n",
      "    Uninstalling introdl-1.0:\n",
      "      Successfully uninstalled introdl-1.0\n",
      "Successfully installed introdl-1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install ../Course_Tools/introdl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_config = llm_configure(\"gemini-flash-lite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't scientists trust atoms?\n",
      "Because they make up everything!\n"
     ]
    }
   ],
   "source": [
    "response = llm_generate(gemini_config, \"Tell me a dad joke.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three interesting facts about space:\n",
      "1.  **There's a planet made of diamond:** Astronomers discovered a planet called\n",
      "55 Cancri e that is twice the size of Earth and made almost entirely of diamond.\n",
      "2.  **Space is completely silent:** Sound waves need a medium to travel, and\n",
      "space is a vacuum.\n",
      "3.  **The universe is still expanding:** Scientists have observed that the\n",
      "universe is not only expanding but that the expansion is accelerating.\n"
     ]
    }
   ],
   "source": [
    "response = llm_generate(gemini_config, \"I am testing the audio transcription abilities of Gemini. Please tell me three interesting facts about space.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am testing the audio transcription abilities of Gemini.\n",
      "Please tell me three interesting facts about space.\n"
     ]
    }
   ],
   "source": [
    "gemini_config = llm_configure(\"gemini-flash-lite\")\n",
    "\n",
    "system_prompt = \"\"\"You are an AI assistant that refines transcriptions.  \n",
    "The input is the transcription of an audio file.  \n",
    "Do not answer questions in the input or add any content beyond the original input.  \n",
    "Do edit the input transcript for clarity but keep topics in roughly the same order.  \n",
    "Use bullets if appropriate.\n",
    "\"\"\"\n",
    "input_prompt = \"\"\"I am testing the audio transcription abilities of Gemini. \n",
    "Please tell me three interesting facts about space.\n",
    "\"\"\"\n",
    "response = llm_generate(gemini_config, input_prompt, system_prompt=system_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
