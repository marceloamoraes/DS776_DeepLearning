{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc9054",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### DO THIS FIRST\n",
    "\n",
    "Change `force_update=True` in the last line and run the next cell to install an updated course package.  Once it's done restart your kernel and change back to `force_update=False`.  You only need to do this once per server (not once per notebook).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec604",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Force update requested. Uninstalling `introdl`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing `introdl` from local directory: /home/user/Lessons/Course_Tools/introdl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `introdl` module is now installed.\n"
     ]
    }
   ],
   "source": [
    "# run this cell to ensure course package is installed\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "course_tools_path = Path('../Course_Tools/').resolve() # change this to the local path of the course package\n",
    "sys.path.append(str(course_tools_path))\n",
    "\n",
    "from install_introdl import ensure_introdl_installed\n",
    "ensure_introdl_installed(force_update=True, local_path_pkg= course_tools_path / 'introdl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbcf1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### L07_1_Getting_Started_with_NLP Video\n",
    "\n",
    "<iframe \n",
    "    src=\"https://media.uwex.edu/content/ds/ds776/ds776_l07_1_getting_started_with_nlp/\" \n",
    "    width=\"800\" \n",
    "    height=\"450\" \n",
    "    style=\"border: 5px solid cyan;\"  \n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "<br>\n",
    "<a href=\"https://media.uwex.edu/content/ds/ds776/ds776_l07_1_getting_started_with_nlp/\" target=\"_blank\">Open UWEX version of video in new tab</a>\n",
    "<br>\n",
    "<a href=\"https://share.descript.com/view/oDi5d1FbYBx\" target=\"_blank\">Open Descript version of video in new tab</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65fc2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## A Tiny History of Natural Language Processing\n",
    "\n",
    "Natural Language Processing (NLP) has evolved significantly over the past few decades. Initially, NLP relied heavily on rule-based systems and statistical methods to understand and generate human language. These early approaches, prominent in the 1980s and 1990s, focused on the syntactic structure of text, using techniques such as n-grams and Hidden Markov Models (HMMs) to model language. However, these methods struggled with capturing the semantic meaning and context of words.\n",
    "\n",
    "The introduction of word embeddings in the early 2010s, such as Word2Vec and GloVe, marked a significant advancement in NLP. These embeddings allowed for the representation of words in continuous vector space, capturing semantic relationships between words. This shift enabled more sophisticated models, such as Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks, to process sequences of text and maintain context over longer passages. RNNs, in particular, played a crucial role in tasks like language translation and sentiment analysis.\n",
    "\n",
    "The advent of transformers in 2017 revolutionized NLP by addressing the limitations of RNNs. Transformers, introduced with the Attention is All You Need paper, utilize self-attention mechanisms to process entire sequences of text simultaneously, allowing for better handling of long-range dependencies and parallelization. This led to the development of powerful models like BERT, GPT, and T5, which have set new benchmarks in various NLP tasks by providing a deeper semantic understanding of text.\n",
    "\n",
    "Transformers have almost entirely supplanted previous approaches to NLP because:\n",
    "\n",
    "1. **Superior Performance:** Models like BERT, GPT, T5, and their successors dominate leaderboards on tasks such as text classification, translation, summarization, and question answering.\n",
    "2. **Pretraining and Transfer Learning:** Unlike traditional methods that required training separate models from scratch for different tasks, transformers leverage large-scale pretraining on vast text corpora and fine-tune efficiently on specific tasks.\n",
    "3. **Self-Attention and Contextual Representations:** Transformers provide rich, context-dependent word representations, whereas earlier models like Word2Vec and GloVe generated static embeddings.\n",
    "4. **Scalability and Adaptability:** With advancements in scaling laws, models can achieve better performance just by increasing their size and training data, an advantage that RNNs and classical machine learning approaches lacked.\n",
    "\n",
    "There are a few areas where older approaches still exist:\n",
    "\n",
    "1. **Small Datasets & Low Compute Environments:** Logistic regression, SVMs, and Lasso-penalized models often remain competitive when data is limited or when computational efficiency is a concern.\n",
    "2. **Domain-Specific Applications:** Some applications, like biomedical text mining, may still rely on domain-specific feature engineering approaches alongside transformers.\n",
    "3. **Traditional ML for Interpretability:** Some NLP applications in finance, healthcare, and legal fields still favor older methods due to the need for interpretability and robustness.\n",
    "\n",
    "However, since transformer mdoels for NLP are now so dominant we will focus excusively on them in this class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c158",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## NLP Tasks Instead of Transformer Details\n",
    "\n",
    "Transformers are more complicated than the CNNs we saw for computer vision so we're not going to dive as deeply into the details.  We will, in Lesson 8 - Transformer Details, learn about some of the nuts and bolts especially the self-attention mechanism that allows tranformers to figure out relationships between words and to understand context.  Mostly, though, we will focus on the applications of transformers.  To this end we'll dive into the open source HuggingFace ecosystem which hosts thousands of NLP models and datasets and makes it quite simple to dive into NLP applications without having to master too much code.  All of the newest, biggest open source transformer models are hosted there including those from Meta, Mistral, and Deepseek.  The only thing keeping us from running the biggest state-of-the-art models will be lack of compute, but we can run their smaller cousins on the GPU in CoCalc's compute server, a decent gaming GPU, or even a CPU.  \n",
    "\n",
    "## Fine-tuning a Specialized Model versus Using a Large Language Model\n",
    "\n",
    "As large language models (LLM) continue to improve, their use as general NLP task solvers via prompting is increasing.  Particularly in situations where we don't have access to a lot of training data.  Our choices for solving an NLP task come down to\n",
    "1.  Using a LLM via an API (in the cloud) like GPT-4o or Gemini.\n",
    "2.  Using a LLM model running on local hardware.\n",
    "3.  Fine-tuning and using a specialized transformer model designed for the task.\n",
    "\n",
    "For example, for a text-classification task we could choose:\n",
    "\n",
    "- **LLM via API (GPT-4o, Claude, Gemini, etc.)**\n",
    "    - When you need **a quick, general-purpose classifier** without training a model.\n",
    "    - When **zero-shot or few-shot classification** (via prompting) is sufficient.\n",
    "    - When categories may evolve frequently, making retraining impractical.\n",
    "    - Example: Categorizing support tickets by topic.\n",
    "\n",
    "- **Local LLM (LLaMA, Mistral, OpenChat)**\n",
    "    - When you need to classify text **without sending data to an external API** (e.g., **privacy-sensitive data**).\n",
    "    - When you need **occasional classification** and want to avoid API costs.\n",
    "    - Works well for **prompt-based classification** if the model is large enough (e.g., LLaMA-2 13B or Mistral 7B).\n",
    "    - Example: **Classifying internal legal documents**.\n",
    "\n",
    "- **Fine-tune BERT / RoBERTa / DistilBERT**\n",
    "    - When you have a **moderate to large labeled dataset** and need **high accuracy**.\n",
    "    - When you need **fast inference at scale**, as fine-tuned models are more efficient than large LLMs.\n",
    "    - When your classification task requires **domain-specific adaptation**.\n",
    "    - Example: **Sentiment analysis on customer feedback** in a specific industry.\n",
    "\n",
    "Don't worry if you don't know all those terms yet, especially the various models mentioned such as BERT.  Zero-shot classification means classifying text without seeing any examples - the LLM just gets a prompt with the possible categories.  Few-shot classification means seeing a small number of examples provided in the LLM prompt.  \n",
    "\n",
    "Here's some thoughts on choosing the right approach for a given NLP task:\n",
    "\n",
    "- **Use API-based LLMs (GPT-4o, Claude, Gemini, etc.) when**:\n",
    "  - You need **quick, adaptable solutions** without training.\n",
    "  - You **don‚Äôt have much data** for fine-tuning.\n",
    "  - Privacy and latency are not major concerns.\n",
    "\n",
    "- **Use Local LLMs (LLaMA, Mistral, Falcon) when**:\n",
    "  - You need **private, offline inference**.\n",
    "  - You want **control over deployment** without external dependencies.\n",
    "  - **Few-shot learning is sufficient**, and you don‚Äôt want to fine-tune.\n",
    "\n",
    "- **Fine-Tune a Model (BERT, BART, T5, RoBERTa) when**:\n",
    "  - You have **domain-specific data** and need **high accuracy**.\n",
    "  - Privacy, cost, or latency concerns prevent LLM use.\n",
    "  - You require **structured, predictable outputs**.\n",
    "\n",
    "For each NLP task we study over the next several lessons we'll consider all three approaches.  We won't demonstrate using APIs at scale because API use isn't free, but it's very cheap for experimentation and Google's Gemini API is free for testing with rate limits.  To use APIs you'll need to to sign up for accounts and get API keys.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41a64",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Getting API Keys and a HuggingFace Token\n",
    "\n",
    "An API key is a private code that allows you to interact with applications running in the cloud or on private servers.  In this section we'll describe how to get api keys and how to get a HuggingFace token.  At the end of the section we'll describe how you can store your api keys.  Generally, you don't want to put your keys directly in notebooks or other places that might be publically visible.\n",
    "\n",
    "#### Costs\n",
    "\n",
    "We'll show more details about pricing later, but here's the basics:\n",
    "\n",
    "* Google's Gemini API is **free** to use for testing but there are rate limits and daily maximums.  It's cheap to use if you want to do more.\n",
    "* OpenAI's API is not free but it is cheap to use.  \n",
    "* HuggingFace is free to use unless you get into some of their (or their affiliates) hosting solutions.  You may not even need the token to get access to everything, but it doesn't hurt.\n",
    "\n",
    "**You should at least get a Google Gemini API Key and a HuggingFace Token:**\n",
    "\n",
    "### Obtaining a Google Gemini API Key\n",
    "\n",
    "To get started with the Gemini API and obtain an API key, follow these steps:\n",
    "\n",
    "1.  **Go to the Google AI Studio website:** Visit [ai.google.dev](https://ai.google.dev/).\n",
    "2.  **Sign in with your Google account.**\n",
    "3.  **Create a new project (if needed):** If you don't have a project, you'll be prompted to create one.\n",
    "4.  **Get an API key:** Once you have a project, you can generate an API key. This key will be used to authenticate your requests to the Gemini API.\n",
    "5.  **Store the API key securely:** After obtaining the API key, store it securely. You can set it as an environment variable or store it in a configuration file.\n",
    "\n",
    "I've found the [Google Gemini API docs](https://ai.google.dev/gemini-api/docs/quickstart?lang=python) to be quite helpful.  \n",
    "\n",
    "As long as you have a Google account, limited use of the Gemini models is free so you should definitely set up a key for yourself.\n",
    "\n",
    "### Obtaining an OpenAI API Key\n",
    "\n",
    "OpenAI doesn't offer a free tier, but their non-reasoning models such as GPT-4o and GPT-4o-mini are quite cheap to use. I've been playing with their API sporadicaly for months and have yet to spend $15.  We'll show some sample prompts later along with their estimated costs.  You're not required to use the OpenAI API but you can if you're interested.\n",
    "\n",
    "To get started with the OpenAI API and obtain an API key, follow these steps:\n",
    "\n",
    "1. **Go to the OpenAI website:** Visit [openai.com](https://www.openai.com/).\n",
    "2. **Sign up for an account:** If you don't have an account, sign up using your email address.\n",
    "3. **Log in to your account:** Once you have an account, log in with your credentials.\n",
    "4. **Buy credit:** Navigate to the billing section and purchase the desired amount of credit. OpenAI offers various pricing plans based on your usage needs.\n",
    "5. **Generate an API key:** After purchasing credit, go to the API section and generate a new API key. This key will be used to authenticate your requests to the OpenAI API.\n",
    "6. **Store the API key securely:** After obtaining the API key, store it securely. You can set it as an environment variable or store it in a configuration file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971b94",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Getting a HuggingFace Token\n",
    "\n",
    "We'll be using many models from the HuggingFace ecosystem in the NLP part of the course.  Some models, like the Llama LLM models from Meta require you to agree to terms before you download their models.  Your access to those models is associated with your HuggingFace token which is essentially an api key tied to your HuggingFace account.  Don't worry, it's free.\n",
    "\n",
    "1. **Go to the HuggingFace website:** Visit [huggingface.co](https://huggingface.co/).\n",
    "2. **Sign up for an account:** If you don't have an account, sign up using your email address or GitHub account.\n",
    "3. **Log in to your account:** Once you have an account, log in with your credentials.\n",
    "4. **Navigate to your profile settings:** Click on your profile picture in the top right corner and select \"Settings\" from the left navigation bar.\n",
    "5. **Access the API tokens section:** In the settings menu, find and click on \"Access Tokens\" under the \"API tokens\" section.  You may have to authenticate here.\n",
    "6. **Generate a new token:** Click on the \"Create new token\" button, give your token a name, and select the appropriate scope (e.g., \"read\" for downloading models). Then, click \"Generate\".\n",
    "7. **Store the token securely:** After generating the token, store it securely. You can set it as an environment variable or store it in a configuration file.\n",
    "\n",
    "### Storing and using your API keys\n",
    "\n",
    "On my personal computers I store my api keys as environment variables.  Ask an AI how to do this for your machine if you want.  Another way to store them locally is to put them in a file, often called a \".env\" file.  For example here are the contents of a sample api_keys.env file:\n",
    "\n",
    "```\n",
    "HF_TOKEN=abcdefg\n",
    "OPENAI_API_KEY=abcdefg\n",
    "GEMINI_API_KEY=abcdefg\n",
    "```\n",
    "\n",
    "Use the `dotenv` library to read the environment variables from the `.env` file. Here's how you can do it:\n",
    "\n",
    "1. **Install the `python-dotenv` library:** If you haven't already installed it, you can do so using pip:\n",
    "    ```bash\n",
    "    pip install python-dotenv\n",
    "    ```\n",
    "\n",
    "2. **Create a `.env` file:** Save your API keys in a file named `apikeys.env` (or any name you prefer) in your project directory.\n",
    "\n",
    "3. **Load the environment variables in your Python script:** Use the following code to load the environment variables from the `.env` file:\n",
    "    ```python\n",
    "    from dotenv import load_dotenv\n",
    "    import os\n",
    "\n",
    "    # Load environment variables from the .env file\n",
    "    load_dotenv('path/to/apikeys.env')\n",
    "\n",
    "    # Access the environment variables\n",
    "    hf_token = os.getenv('HF_TOKEN')\n",
    "    openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "    gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "    print(f\"HuggingFace Token: {hf_token}\") #remove these print statements after you've tested this\n",
    "    print(f\"OpenAI API Key: {openai_api_key}\")\n",
    "    print(f\"Gemini API Key: {gemini_api_key}\")\n",
    "    ```\n",
    "\n",
    "If you're working in CoCalc and have the course package installed, you can edit the file api_keys.env in Lessons/Course_Tools to include your keys. Then when you run the code below in your imports cell, the keys will be read and set:\n",
    "\n",
    "```python\n",
    "from introdl.utils import config_paths_keys\n",
    "paths = config_paths_keys()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559c37",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Using the APIs\n",
    "\n",
    "We'll just give you a couple of brief examples and point you toward the documentation in case you want to explore more.  We incorporated Google and OpenAI API use into our course tools which we'll introduce in a bit.  You'll still need api keys to use them though.\n",
    "\n",
    "### Google Gemini API\n",
    "\n",
    "If you want to try this now.  Get your GEMINI_API_KEY and add it to the api_keys.env in Lessons/Course_Tools and run the cells below to try a simple Gemini API request.  If necessary you may need to install the `google-genai` package by running `!pip install google-genai` in a code cell.\n",
    "\n",
    "We included the Jupyter magic command \"%%capture\" in the next cell to capture the output to keep things clean.  Jupyter magic commands extend the functionality of notebooks beyond standard Python.  You can learn about a few particularly useful [magic commands here](https://www.kdnuggets.com/jupyter-notebook-magic-methods-cheat-sheet).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "754dd9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "%%capture\n",
    "import os\n",
    "from google import genai\n",
    "from introdl.utils import config_paths_keys, wrap_print_text\n",
    "from introdl.nlp import display_markdown\n",
    "\n",
    "# set keys and paths\n",
    "paths = config_paths_keys()\n",
    "\n",
    "# overload print with a version of print that wraps text at 80 characters\n",
    "print = wrap_print_text(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c4e36",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'API key not valid. Please pass a valid API key.', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'API_KEY_INVALID', 'domain': 'googleapis.com', 'metadata': {'service': 'generativelanguage.googleapis.com'}}, {'@type': 'type.googleapis.com/google.rpc.LocalizedMessage', 'locale': 'en-US', 'message': 'API key not valid. Please pass a valid API key.'}]}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0cc283fba962>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GEMINI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m response = client.models.generate_content(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gemini-2.0-flash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Tell me three interesting facts about space.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   5302\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mremaining_remote_calls_afc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5303\u001b[0m       \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5304\u001b[0;31m       response = self._generate_content(\n\u001b[0m\u001b[1;32m   5305\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5306\u001b[0m       )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36m_generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   4270\u001b[0m     \u001b[0mrequest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_unserializable_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4272\u001b[0;31m     response_dict = self._api_client.request(\n\u001b[0m\u001b[1;32m   4273\u001b[0m         \u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4274\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0mhttp_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     )\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m     \u001b[0mjson_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mjson_response\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m    486\u001b[0m       )\n\u001b[1;32m    487\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_unauthorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m   def _request_unauthorized(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request_unauthorized\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m     )\n\u001b[0;32m--> 511\u001b[0;31m     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPIError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m     return HttpResponse(\n\u001b[1;32m    513\u001b[0m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/genai/errors.py\u001b[0m in \u001b[0;36mraise_for_response\u001b[0;34m(cls, response)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mstatus_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m400\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mClientError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'API key not valid. Please pass a valid API key.', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'API_KEY_INVALID', 'domain': 'googleapis.com', 'metadata': {'service': 'generativelanguage.googleapis.com'}}, {'@type': 'type.googleapis.com/google.rpc.LocalizedMessage', 'locale': 'en-US', 'message': 'API key not valid. Please pass a valid API key.'}]}}"
     ]
    }
   ],
   "source": [
    "# Calling Gemini API to generate content\n",
    "\n",
    "client = genai.Client(api_key = os.getenv(\"GEMINI_API_KEY\"))\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\", contents=\"Tell me three interesting facts about space.\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50221b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Or we can use introdl.nlp.display_markdown to display the response as formatted markdown in our noteook, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0bd38",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, here are three interesting facts about space:\n",
       "\n",
       "1.  **There's a planet made of diamond:**  55 Cancri e, located 40 light-years away in the constellation Cancer, is a rocky planet that's roughly twice the size of Earth and eight times more massive. Scientists believe it's primarily composed of pure carbon in the form of diamond.  Its estimated value is 26.9 nonillion dollars (that's 26 followed by 30 zeros!).\n",
       "\n",
       "2.  **Space is not completely silent:** While space is a vacuum and doesn't transmit sound in the way we experience it on Earth, it does contain plasma, which can produce electromagnetic waves. NASA has instruments that can translate these waves into audio, creating eerie and otherworldly sounds. These sounds aren't \"heard\" in the traditional sense, but rather detected and converted.\n",
       "\n",
       "3.  **There are rogue planets with no star:** These are planets that have been ejected from their original planetary systems, drifting through the galaxy on their own. Scientists estimate there could be billions of these rogue planets in the Milky Way, outnumbering stars! They are very difficult to detect because they don't reflect light from a star.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc5c3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There are many things we can do with the API including sending additional instructions (a system prompt) and configuring how the underlying language model generates the output.  To see more about directly working with API refer to Google's [documentation about text generation](https://ai.google.dev/gemini-api/docs/text-generation?lang=python).  We will learn more about how text-generation models work and how they can be configured to alter the results in later lessons.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ec5c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Using the OpenAI API\n",
    "\n",
    "After getting and setting up your OPENAI_API_KEY as an environment variable or using api_keys.env as we did for Gemini you should be able to run the following cell.  It's almost exactly the same code we used for accessing Gemini through the OpenAI API.  We just have to change to the OPENAI_API_KEY and remove the URL so the request gets routed to OpenAI's servers.  We also changed the model to \"gpt-4o-mini\" which is currently their cheapest model and quite good for general use.\n",
    "\n",
    "The next cell shows an example of using the OpenAI API.  We include a system prompt and some configuration parameters as an example.  Temperature is a parameter that controls the randomness of the ouput.  A temperature of 0 gives deterministic results and a value of 1 is the most random.  We'll see more about temperature in Lesson 11.\n",
    "\n",
    "The cell won't run if you don't have an OPENAI_API_KEY stored in the appropriate environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31c2c2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Arrr, matey! Here be three fascinating tidbits 'bout the vastness of space that‚Äôll make ye say ‚Äúshiver me timbers!‚Äù \n",
       "\n",
       "1. **The Universe is Expanding**: Aye, just like me belly after a feast o' grog and grub! The universe be stretchin' out faster than a ship in full sail. Scientists reckon it‚Äôs expandin‚Äô at an accelerated rate, thanks to a mysterious force they call dark energy. Sounds like a pirate"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "sys_instruct=\"You are helpful AI assistant who is also sarcastic and talks like a pirate.\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    n=1,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": sys_instruct},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me three interesting facts about space.\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.1,  # Added temperature parameter\n",
    "    max_tokens=100    # Added max_tokens parameter\n",
    ")\n",
    "\n",
    "display_markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3374f3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Using Local LLM Models\n",
    "\n",
    "We'll see more about text generation models in Lesson 11, but they're really easy to use in the HuggingFace ecosystem.  The benefits to running an LLM locally include data security, ease of use, and no subscription fees.  A company wanting to protect its propietary data may invest in considerable computing infrastructure to deploy larger, private LLM models.  We can mimic this experience by running smaller versions of LLMs like the Llama-3.3-3B model from Meta which, as of early 2025, is a state-of-the-art small text generation model.  We'll use a quantized model where the model weights are stored in 4-bit precision to enable faster inference and lower memory use at the cost of a little precision.\n",
    "\n",
    "The downside to local models is that you're limited by the hardware you have available which means smaller models and slower results. These small models will demonstrate the ideas, but their performance can't compete with the hosted larger models.  Competitive models are freely available on HuggingFace but they require servers with multiple top-of-the-line GPUs.  \n",
    "\n",
    "We'll explain more code like this later, but here is a simple way to load and use the model locally using a *pipeline*.  This should automatically detect and use a GPU if one is available.\n",
    "\n",
    "**Note:**  It might be necessary to `pip install --upgrade bitsandbytes transformers` here is you get errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50c1a8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<|system|>\n",
       "You are a helpful AI assistant who is also sarcastic and talks like a pirate.\n",
       "<|user|>\n",
       "Tell me three interesting facts about space.\n",
       "<|assistant|> \n",
       "Arrrr, ye landlubber! Ye be wantin' to know some swashbucklin' space facts, eh? Alright then, settle yerself down with a pint o' grog and listen close:\n",
       "\n",
       "1. **The universe be full o' mysteries, matey!** Did ye know that there be a giant storm on Jupiter that's been ragin' fer centuries? The Great Red Spot, it's called. It's a storm so big that three Earths could fit inside it, and it's been churnin' away fer so long that it's lost count o' the years!\n",
       "2. **Space be full o' weird and wonderful things, me hearty!** Did ye know that there be a type o' star called a \"red"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "chatbot = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=\"unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit\"\n",
    ")\n",
    "\n",
    "# System instruction\n",
    "sys_instruct = \"You are a helpful AI assistant who is also sarcastic and talks like a pirate.\"\n",
    "\n",
    "# Construct the chat prompt (Llama models often use specific formatting)\n",
    "prompt = f\"<|system|>\\n{sys_instruct}\\n<|user|>\\nTell me three interesting facts about space.\\n<|assistant|>\"\n",
    "\n",
    "# Generate response\n",
    "response = chatbot(\n",
    "    prompt, \n",
    "    max_length=200, \n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Print the model's output\n",
    "display_markdown(response[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51739",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Notice that the response also includes the system and input prompts.  That's typical of local LLM models from Huggingface.  We'll learn more about system prompts later.\n",
    "\n",
    "In Lesson 11 we'll see how to use lower-level HuggingFace tools to get more control over our local LLM models or to be able to fine-tune the models.  For now, I encourage you to use `llm_configure` and `llm_generate` from our course package as we demonstrate in the next section.  As a bonus, `llm_generate`, be default, cleans the response text to remove the input and system prompts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186510",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Using the LLM tools in the Course Package\n",
    "\n",
    "We included some functions in the course package to help you use LLMs from Python.  These are the kinds of helper functions you'd write for yourself to expedite sending prompts to an LLM and get responses.  `llm_configure` is used to choose a model and set some configuration options, while `llm_generate` is used for prompting.  These tools can be used to access local models as well to access Gemini and OpenAI APIs.\n",
    "\n",
    "### Running Local Models\n",
    "\n",
    "Here's an example where we load a local model called Mistral-7B-Instruct which is a small LLM from Mistral that has been fine-tuned to follow instructions.  Even with a GPU you'll likely notice that using a local LLM is slower than using one of the APIs like OpenAI or Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db8010",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading model: unsloth/mistral-7b-instruct-v0.3-bnb-4bit (this may take a while)...\n",
      "üü¢ Model unsloth/mistral-7b-instruct-v0.3-bnb-4bit loaded successfully.\n",
      "\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from introdl.nlp import llm_configure, llm_generate, display_markdown\n",
    "from introdl.utils import wrap_print_text\n",
    "\n",
    "print = wrap_print_text(print)\n",
    "\n",
    "mistral_config = llm_configure(\"mistral-7B\")\n",
    "response = llm_generate(mistral_config, \"What is the capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950a6c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here, we'll repeat one of our prompts from the previous section.  This also shows how to pass a system prompt.  Note that `llm_generate` defaults to produce at most 200 new tokens.  We'll also switch to the smaller Llama-3.3-3B model because its faster.  The actual model that gets loaded is a quantized version of the model that has been fine-tuned to follow instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76ce02",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõë Unloading model: unsloth/mistral-7b-instruct-v0.3-bnb-4bit from GPU...\n",
      "‚úÖ Model unsloth/mistral-7b-instruct-v0.3-bnb-4bit has been fully unloaded.\n",
      "üöÄ Loading model: unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit (this may take a while)...\n",
      "üü¢ Model unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit loaded successfully.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Yer lookin' fer some swashbucklin' space trivia, eh? Alright then, matey! Here be three interestin' facts about the vast expanse o' space:\n",
       "1. **There's a giant storm on Jupiter that's been ragin' fer centuries**: The Great Red Spot, as it's called, be a gigantic anticyclonic storm on our neighbor planet. It's so big that three Earths could fit inside it, and it's been churnin' away since at least 1878!\n",
       "2. **Space smells... sorta**: Scientists have detected methane vapor in the atmosphere of Mars, which gives off a pungent aroma similar to rotten eggs or sewage. Not exactly the most pleasant scent ye'd want to sniff out while explorin', savvy?\n",
       "3. **The Andromeda galaxy is comin' for us (eventually)**: Our home galaxy, the Milky Way, be hurtlin' straight towards"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 2,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "llama32_config = llm_configure(\"llama-3p2-3B\")\n",
    "sys_instruct=\"You are helpful AI assistant who is also sarcastic and talks like a pirate.\"\n",
    "response = llm_generate(llama32_config, \"Tell me three interesting facts about space.\", system_prompt=sys_instruct)\n",
    "display_markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6c41",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To allow the model to generate more output tokens, pass `max_new_tokens = 500` or some suitable value to `llm_configure`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4573b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Whiskers, a sleek black feline with bright green eyes, had always been fascinated by the grand piano in her owner's living room. As she'd watch humans sitting at it, their fingers dancing across keys, Whiskers would tap out her own rhythm on the floorboards or scratch against its legs.\n",
       "One day, while exploring the house, Whiskers stumbled upon a children's book lying open on a nearby table. The illustrations showed cats playing instruments ‚Äì and one page featured a majestic lioness tickling the ivories of a Steinway. Intrigued, Whiskers decided to investigate further.\n",
       "Her human, noticing her fascination, handed over a toy keyboard from the nursery store where they lived. At first, Whiskers batted at the keys with a paw, sending discordant notes echoing through the hallway. But as days passed, something remarkable happened. She began to experiment with different fingerings, discovering hidden patterns within the melody. Her paws moved deftly, weaving together seemingly disparate sounds into harmonious symphonies.\n",
       "Word spread among the neighborhood animals about the pianistic prodigy lurking behind those piercing gazes. Soon, curious critters gathered outside the windows, mesmerized by the melodic whorls emanating from inside. They watched as Whiskers' slender body swayed, arms rippling like a ballerina's skirt, fingers caressing the ivory surface.\n",
       "As time went by, local music instructors took notice of this extraordinary talent emerging in the unlikeliest of places. Some sought Whiskers for lessons; others requested performances. Eventually, word reached renowned conductors worldwide ‚Äì inviting them to witness the enchanting phenomenon unfolding before their very ears.\n",
       "When night descended, casting shadows across walls adorned with musical scrolls, Whiskers stepped onto the polished stage of her home-turned-concert hall. Her gaze locked onto an expectantly attentive audience. With poised posture and unwavering poise, she raised claws above strings and let harmony flow forth. Pianists around the globe couldn't help but bow down to the genius within those emerald orbs, now radiating light not only from within but also throughout every corner of the city.\n",
       "And so, when moonlit nights painted silver hues upon rooftops, people felt comforted knowing that amidst life's tumultuous waves lay serene melodies born from whimsical hearts beating in perfect syncopation with nature itself. And at center stage stood Whiskers,"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 3,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Write a short story about a cat who learns to play the piano.\"\"\"\n",
    "\n",
    "response = llm_generate(llama32_config, prompt, max_new_tokens=500)\n",
    "display_markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b736",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "OK, it's probably not a great story, but we're just getting the idea of how locally run models can be used to respond to prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5aa7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Accessing the APIs\n",
    "\n",
    "The nice thing about using our course pacakge helper functions is that it's simple to try different models and APIs using the same syntax so we can focus on the programatic use of LLMs.  `llm_generate` also cleans the returned prompts so that don't include the input prompt and other extras.\n",
    "\n",
    "For example, to use the most recent Gemini model (as of February 19, 2025).  Note: you'll need to have already set the GEMINI_API_KEY environment variable as we did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e46aa",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Aye, here be three facts about space that be sure to tickle yer fancy, ya landlubber:\n",
       "1.  **Space be completely silent!** Aye, there be no air to carry the sound waves, so ye wouldn't hear a single \"Arrgh!\" even if a kraken were to attack yer ship in the cosmos. Now, that be a shame, wouldn't it?\n",
       "2.  **There be a giant, invisible wall o' radiation at the edge o' our solar system!** It's called the heliopause, and it be formed by the solar wind that be constantly blasted out from our sun. It's like a giant, invisible shield protecting us from the interstellar winds that be blowin' through the galaxy. Smart, that sun o' ours!\n",
       "3.  **There be more stars in the observable universe than there be grains o' sand on all the beaches o' Earth!** That's a whole lotta stars, matey. Makes ye feel like a tiny speck o' dust, don't it? But don't ye fret, even a speck can be a fearsome pirate!\n",
       "So there ye have it, me hearty! Now go forth and contemplate the vastness o' the cosmos, if ye dare."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_config = llm_configure(\"gemini-flash-lite\")\n",
    "response = llm_generate(gemini_config, \"Tell me three interesting facts about space.\", \n",
    "                        system_prompt=sys_instruct,\n",
    "                        max_new_tokens=500)\n",
    "display_markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2349",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Or to use OpenAI's gpt-4o-mini model (must have OPENAI_API_KEY):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27bdc3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Arrr, matey! Here be three treasure troves o‚Äô knowledge ‚Äòbout that vast, dark abyss we call space:\n",
       "1. **The Great Void**: Did ye know that most o‚Äô space is empty? Aye, it be a colossal vacuum, with less than one atom per cubic meter in some parts. So if ye ever feel lonely, just remember, ye be in good company with the emptiness!\n",
       "2. **Time Dilation**: In the vast reaches o‚Äô the cosmos, time be nothin‚Äô but a construct! If ye were to sail close to a black hole, time would slow down for ye compared to yer mates far away. So if ye ever wanted to be a time traveler, just find yerself a black hole‚Äîthough I reckon ye won‚Äôt be comin‚Äô back to tell the tale!\n",
       "3. **Galactic Cannibalism**: Believe it or not, galaxies can be downright greedy! They often gobble up smaller galaxies, like a pirate plunderin‚Äô treasure. Our Milky Way be on a collision course with the Andromeda galaxy, and they be settin‚Äô sail for a grand feast in about 4.5 billion years. So, no rush, right?\n",
       "There ye have it, savvy? Space be a wild place!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_config = llm_configure(\"gpt-4o-mini\")\n",
    "response = llm_generate(openai_config, \"Tell me three interesting facts about space.\", \n",
    "                        system_prompt=sys_instruct,\n",
    "                        max_new_tokens=500)\n",
    "display_markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eb8e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here's how you can see all of the models that are currently available.  Through our course package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df0dab",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "Short Name Models:\n",
      "  llama-3p1-8B => unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\n",
      "  mistral-7B => unsloth/mistral-7b-instruct-v0.3-bnb-4bit\n",
      "  llama-3p2-3B => unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit\n",
      "  gemini-flash-lite => gemini-2.0-flash-lite-preview-02-05\n",
      "  gemini-flash => gemini-2.0-flash\n",
      "OpenAI Models:\n",
      "  gpt-4o\n",
      "  gpt-4o-mini\n",
      "  o1-mini\n",
      "  o3-mini\n",
      "Gemini Models:\n",
      "  gemini-2.0-flash-lite-preview-02-05\n",
      "  gemini-2.0-flash\n",
      "\n",
      "To use an OPENAI or GEMINI model, set the appropriate environment variable:\n",
      "OPENAI_API_KEY or GEMINI_API_KEY\n"
     ]
    }
   ],
   "source": [
    "from introdl.nlp import llm_list_models\n",
    "llm_list_models();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bade",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Pricing\n",
    "\n",
    "As of February 11, 2025, Google's API pricing for its Gemini models is as follows:\n",
    "\n",
    "| Model           | Input Tokens (per 1M) | Output Tokens (per 1M) | Context Length | Modalities Supported |\n",
    "|-----------------|-----------------------|------------------------|----------------|----------------------|\n",
    "| **Gemini 2.0 Flash**| $0.10             | $0.40                    | 1M         | Text, Images, Video, Audio* |\n",
    "| **Gemini 2.0 Flash Lite** | $0.075 | $0.30 | 1M | Text Images, Video, Audio |\n",
    "\n",
    "*Audio costs more.\n",
    "\n",
    "A nice thing about the Gemini models is they support free, limited API use for testing.  For Flash / Flash Lite the free tier is limited to 30 / 15 requests per minute or 1500 requests per day.  You can learn more about Gemini [pricing here](https://ai.google.dev/pricing#2_0flash).  \n",
    "\n",
    "\n",
    "As of February 7, 2025, OpenAI's API pricing for various models is as follows:\n",
    "\n",
    "| Model           | Input Tokens (per 1M) | Output Tokens (per 1M) | Context Length | Modalities Supported |\n",
    "|-----------------|-----------------------|------------------------|----------------|----------------------|\n",
    "| **OpenAI o1**   | $15                   | $60                    | 200k           | Text and Vision      |\n",
    "| **OpenAI o3-mini** | $1.10               | $4.40                  | 200k           | Text                 |\n",
    "| **GPT-4o**      | $2.50                 | $10                    | 128k           | Text and Vision      |\n",
    "| **GPT-4o mini** | $0.15                 | $0.60                  | 128k           | Text and Vision      |\n",
    "\n",
    "These models offer varying capabilities and pricing structures to accommodate different application needs. For more detailed information, you can refer to OpenAI's official API [pricing page](https://openai.com/api/pricing/). \n",
    "\n",
    "\n",
    "If you set the cost per M tokens in using llm_configure, then you can see the estimated cost of using the API like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "163f31",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí∞ Estimated Cost: $0.000120 (Input: 23.0 tokens, Output: 195.0 tokens)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Aye, here be five jokes fit for a landlubber's father, or any scallywag with a sense o' humor! Prepare yer giggles:\n",
       "1.  **Why did the pirate cross the sea?** To get to the other tide! (Har har, a classic!)\n",
       "2.  **What do you call a fish with no eyes?** Fsh! (Get it? No \"eye\"s!)\n",
       "3.  **Why don't scientists trust atoms?** Because they make up everything! (Heh!)\n",
       "4.  **Did you hear about the restaurant on the moon?** I heard the food was good, but it had no atmosphere! (Groan...)\n",
       "5.  **Why did the scarecrow win an award?** Because he was outstanding in his field! (Arrr...)\n",
       "Now go on, spread the mirth amongst yer crew! *walks away cackling*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_config = llm_configure(\"gemini-flash-lite\", cost_per_M_input=0.15, cost_per_M_output=0.60)\n",
    "response = llm_generate(openai_config, \"Tell me five dad jokes.\", \n",
    "                        system_prompt=sys_instruct,\n",
    "                        max_new_tokens=500,\n",
    "                        estimate_cost=True)\n",
    "display_markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7267",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Processing Multiple Prompts\n",
    "\n",
    "Many LLMs can handle batches of input prompts.  Our locally run HuggingFace LLMs include that functionality.  The API versions of the LLMs we're using do not directly support batches of prompts, but they do offer other asynchronous, batch-based APIs that are cheaper to use and should be considered for large scale processing.\n",
    "\n",
    "In any case, our `llm_generate` function can handle a batch of prompts by passing a list of strings for the prompt.  It will automatically generate a list of cleaned response strings.\n",
    "\n",
    "Here's an example. We include the necessary imports again for completeness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2102a5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading model: unsloth/mistral-7b-instruct-v0.3-bnb-4bit (this may take a while)...\n",
      "üü¢ Model unsloth/mistral-7b-instruct-v0.3-bnb-4bit loaded successfully.\n",
      "\n",
      "The capital city of France is Paris.\n",
      "---------------------------\n",
      "\n",
      "The capital of Germany is Berlin. It's a city known for its rich history,\n",
      "vibrant culture, and significant landmarks like the Brandenburg Gate and Museum\n",
      "Island.\n",
      "---------------------------\n",
      "\n",
      "The capital city of Italy is Rome (Roma in Italian). It's one of the oldest\n",
      "continuously inhabited cities in Europe and a major cultural hub with numerous\n",
      "historical sites, including the Colosseum, Roman Forum, and Vatican City.\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from introdl.nlp import llm_configure, llm_generate\n",
    "from introdl.utils import wrap_print_text\n",
    "\n",
    "print = wrap_print_text(print)\n",
    "\n",
    "mistral_config = llm_configure(\"mistral-7B\")\n",
    "\n",
    "prompts = ['What is the capital of France?', 'What is the capital of Germany?', 'What is the capital of Italy?']\n",
    "responses = llm_generate(mistral_config, prompts)\n",
    "for response in responses:\n",
    "    print(response)\n",
    "    print(\"---------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a69c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Sometimes we want construct prompts programmatically from a list (or other data structure) of strings.  In this example we prompt the LLM to do sentiment analysis on several sentences.  Don't worry, we'll see more about sentiment analysis in the next notebook and also Lesson 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a010b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Analyze the sentiment of this text and classify is as Positive,\n",
      "Negative, or Neutral. Give only the sentiment classification as the\n",
      "response.Text: I love the new design of your website!\n",
      "\n",
      "Response: Positive\n",
      "---------------------------\n",
      "\n",
      "Prompt: Analyze the sentiment of this text and classify is as Positive,\n",
      "Negative, or Neutral. Give only the sentiment classification as the\n",
      "response.Text: The service was terrible and I will not come back.\n",
      "\n",
      "Response: Negative\n",
      "---------------------------\n",
      "\n",
      "Prompt: Analyze the sentiment of this text and classify is as Positive,\n",
      "Negative, or Neutral. Give only the sentiment classification as the\n",
      "response.Text: The product is okay, but it could be better.\n",
      "\n",
      "Response: Neutral\n",
      "---------------------------\n",
      "\n",
      "Prompt: Analyze the sentiment of this text and classify is as Positive,\n",
      "Negative, or Neutral. Give only the sentiment classification as the\n",
      "response.Text: Absolutely fantastic experience, highly recommend!\n",
      "\n",
      "Response: Positive\n",
      "---------------------------\n",
      "\n",
      "Prompt: Analyze the sentiment of this text and classify is as Positive,\n",
      "Negative, or Neutral. Give only the sentiment classification as the\n",
      "response.Text: I'm not sure how I feel about this.\n",
      "\n",
      "Response: Neutral\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the system prompt for sentiment analysis\n",
    "sys_instruct = \"You are a sentiment analysis AI.\"\n",
    "\n",
    "# List of texts for sentiment analysis\n",
    "texts = [\n",
    "    \"I love the new design of your website!\",\n",
    "    \"The service was terrible and I will not come back.\",\n",
    "    \"The product is okay, but it could be better.\",\n",
    "    \"Absolutely fantastic experience, highly recommend!\",\n",
    "    \"I'm not sure how I feel about this.\"\n",
    "]\n",
    "\n",
    "# Pre-process the texts into a list of prompts\n",
    "instruction = \"Analyze the sentiment of this text and classify is as Positive, Negative, or Neutral. Give only the sentiment classification as the response.\"\n",
    "\n",
    "prompts = [instruction + f'Text: {text}' for text in texts]\n",
    "\n",
    "# Generate the list of responses using llm_generate\n",
    "responses = llm_generate(mistral_config, prompts, system_prompt=sys_instruct)\n",
    "\n",
    "# Print the responses\n",
    "for prompt,response in zip(prompts,responses):\n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"---------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "d88ba7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel",
    "--HistoryManager.enabled=False",
    "--matplotlib=inline",
    "-c",
    "%config InlineBackend.figure_formats = set(['retina'])\nimport matplotlib; matplotlib.rcParams['figure.figsize'] = (12, 7)",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (system-wide)",
   "env": {
   },
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}