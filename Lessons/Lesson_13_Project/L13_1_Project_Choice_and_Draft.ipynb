{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bdf432b",
   "metadata": {},
   "source": [
    "Your assigment is to produce a tutorial that gives a short introduction to a topic and demonstrates a working example using PyTorch and/or Hugging Face.   \n",
    "* The tutorial should be appropriate for someone who hasn't seen the topic, but is familiar with the material in this course to this point.\n",
    "* If you choose a topic we've already covered then you need to do something to make it different than how it was covered in class.  At the very least you should use different data and a different model.\n",
    "* You can also choose a topic that's in one of our textbooks, but wasn't covered in class. \n",
    "* You can also choose other topics, but I encourage you keep your idea relatively simple because you only have two weeks.\n",
    "\n",
    "The deliverables will be:\n",
    "* (40 points) At end of Week 13: working examples and an outline of the tutorial in a Jupyter notebook and HTML (about 50% done)\n",
    "* (60 points) At end of Week 14: complete tutorial in a Jupyter Notebook and HTML.  To be graded both on correctness and presentation.\n",
    "\n",
    "\n",
    "* Here's a list of 14 ideas I generated after some back and forth with ChatGPT\n",
    "\n",
    "## Vision Projects (7)\n",
    "\n",
    "### 1. Exploring Advanced Data Augmentation  \n",
    "Compare techniques like MixUp, CutMix, RandAugment, and AugMix using CIFAR-10 or another dataset.  \n",
    "Bonus: Visualize augmented vs. original images and analyze robustness to distribution shift.\n",
    "\n",
    "### 2. Visualizing What CNNs Learn  \n",
    "Use activation maps, Grad-CAM, t-SNE, or PCA to explore feature representations inside a CNN.  \n",
    "Apply to correctly and incorrectly classified samples.\n",
    "\n",
    "### 3. Adding Attention to CNNs  \n",
    "Enhance a CNN with channel or spatial attention (e.g., Squeeze-and-Excitation or CBAM).  \n",
    "Compare performance and visualize attention heatmaps.\n",
    "\n",
    "### 4. Handling Imbalanced Classes in Vision Tasks  \n",
    "Simulate label imbalance and try loss weighting, oversampling, or focal loss.  \n",
    "Evaluate per-class performance using confusion matrices.\n",
    "\n",
    "### 5. GANs for Image Generation  \n",
    "Build a basic GAN to generate images from noise (e.g., MNIST).  \n",
    "Track generated samples during training and explore tricks to improve stability.\n",
    "\n",
    "### 6. AutoEncoders for Denoising or Anomaly Detection  \n",
    "Train an autoencoder to reconstruct or denoise images.  \n",
    "Use reconstruction error to detect anomalies.\n",
    "\n",
    "### 7. Compressing CNNs via Quantization or Pruning  \n",
    "Apply quantization or pruning to compress a CNN.  \n",
    "Compare accuracy, memory, and inference speed before and after compression.\n",
    "\n",
    "## NLP Projects (7)\n",
    "\n",
    "### 8. Prompt Tuning vs. Full Fine-Tuning  \n",
    "Compare soft prompt tuning and full fine-tuning on a text classification or NER task.  \n",
    "Analyze performance, parameter count, and training efficiency.\n",
    "\n",
    "### 9. Retrieval-Augmented Summarization  \n",
    "Build a small pipeline that retrieves relevant context before summarizing using a model like BART or T5.  \n",
    "Evaluate fluency and factuality compared to a baseline.\n",
    "\n",
    "### 10. Investigating Hallucination in Generative Models  \n",
    "Generate outputs for factual prompts and evaluate hallucination with human review or metrics like BERTScore.  \n",
    "Try few-shot or chain-of-thought prompting to reduce hallucination.\n",
    "\n",
    "### 11. Extractive vs. Abstractive Question Answering  \n",
    "Compare BERT-based extractive QA with an abstractive model like T5.  \n",
    "Evaluate answer accuracy and quality across question types.\n",
    "\n",
    "### 12. Detecting Toxicity and Bias in Language Models  \n",
    "Analyze toxicity or bias using datasets like Jigsaw Toxic Comments.  \n",
    "Test mitigation methods like prompt design or filtering.\n",
    "\n",
    "### 13. Cross-Task Transfer: NER to Improve QA  \n",
    "Use a NER model to extract entities and assist retrieval or answer selection in a QA task.  \n",
    "Analyze whether combining NER and QA improves accuracy.\n",
    "\n",
    "### 14. Sentence Embeddings for Semantic Search or Clustering  \n",
    "Use Sentence-BERT or MiniLM to embed text for similarity search or clustering.  \n",
    "Compare to TF-IDF baselines and visualize embedding spaces.\n",
    "\n",
    "Some of those ideas may be too advanced depending on your approach.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
