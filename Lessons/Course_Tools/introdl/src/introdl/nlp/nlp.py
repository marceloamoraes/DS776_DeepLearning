import os
import gc
import torch
from openai import OpenAI
from transformers import AutoModelForCausalLM, AutoTokenizer
import re
import ipywidgets as widgets
import pyperclip
from IPython.display import display, clear_output, Markdown
from accelerate import cpu_offload
import contextlib
import time
from tqdm.autonotebook import tqdm
import sys

from transformers.utils import logging
logging.set_verbosity_error()

# Suppress logging warnings from PyTorch and Transformers
import logging
logging.getLogger("transformers.models.mistral.modeling_mistral").setLevel(logging.ERROR)
logging.getLogger("torch").setLevel(logging.ERROR)

# Disable CUDA memory caching warning
os.environ["PYTORCH_NO_CUDA_MEMORY_CACHING"] = "1"

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.filterwarnings("ignore", message=".*not compiled with flash attention.*")

# Global caches for Hugging Face models and tokenizers.
LOADED_MODELS = {}
LOADED_TOKENIZERS = {}

# --- Available Model Definitions ---
SHORT_NAME_MAP = {
    "llama-3p2-3B": "unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit",
    "llama-3p1-8B": "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit",
    "mistral-7B": "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "qwen-2p5-3B": "unsloth/Qwen2.5-3B-Instruct-bnb-4bit",
    "qwen-2p5-7B": 'unsloth/Qwen2.5-7B-Instruct-bnb-4bit',
    "gemini-flash-lite": "gemini-2.0-flash-lite-preview-02-05",
    "gemini-flash": "gemini-2.0-flash",
    "gpt-4o": "gpt-4o",
    "gpt-4o-mini": "gpt-4o-mini",
}

OPENAI_MODELS = ["gpt-4o", "gpt-4o-mini"]
GEMINI_MODELS = ["gemini-2.0-flash-lite-preview-02-05", "gemini-2.0-flash"]

def llm_list_models(verbose=True):
    """
    Lists all available models grouped by category:
    - Short Name Models (Hugging Face & Gemini)
    - OpenAI Models
    - Gemini Models
    """
    if verbose:
        print("Available models:")
        for short_name in SHORT_NAME_MAP.keys():
            if short_name in OPENAI_MODELS:
                print(f" {short_name} => needs OPENAI_API_KEY")
            elif short_name.startswith('gemini'):
                print(f" {short_name} => needs GEMINI_API_KEY")
            else:
                print(f" {short_name} => HuggingFace: {SHORT_NAME_MAP[short_name]}")
    models = zip(range(len(SHORT_NAME_MAP)), SHORT_NAME_MAP.keys())
    return models

def release_model():
    """Unloads the currently loaded Hugging Face model from memory and GPU."""
    global LOADED_MODELS, LOADED_TOKENIZERS
    if LOADED_MODELS:
        model_str = next(iter(LOADED_MODELS))
        print(f"üõë Unloading model: {model_str} from GPU...")
        model = LOADED_MODELS[model_str]
        try:
            if hasattr(model, "hf_device_map"):
                cpu_offload(model)
            else:
                print("‚¨áÔ∏è Moving model to CPU manually...")
                with warnings.catch_warnings():
                    warnings.filterwarnings("ignore", message="You shouldn't move a model that is dispatched using accelerate hooks")
                    model.to("cpu")
        except Exception as e:
            print(f"‚ö†Ô∏è Error moving model to CPU: {e}")
        del LOADED_MODELS[model_str]
        del LOADED_TOKENIZERS[model_str]
        torch.cuda.empty_cache()
        gc.collect()  # Force garbage collection
        print(f"‚úÖ Model {model_str} has been fully unloaded.")
    else:
        print("‚ùå No model to unload.")

class ModelConfig:
    """
    Configuration object to store model, tokenizer, and API client settings.
    
    When using API-based models (e.g. OpenAI, Gemini, Together, or Groq), the model and tokenizer
    may be None.
    """
    def __init__(self, model_str, model=None, tokenizer=None, api_type=None,
                 cost_per_M_input=None, cost_per_M_output=None):
        self.model_str = model_str
        self.model = model
        self.tokenizer = tokenizer
        self.api_type = api_type  # Expected values: "openai", "gemini", "together", "groq" or None
        self.client = None
        self.cost_per_M_input = cost_per_M_input
        self.cost_per_M_output = cost_per_M_output

        # Initialize API client if applicable.
        if api_type in ["openai", "gemini", "together", "groq"]:
            if api_type == "together":
                api_key = os.getenv("TOGETHER_API_KEY")
                base_url = "https://api.together.xyz/v1"
            elif api_type == "groq":
                api_key = os.getenv("GROQ_API_KEY")
                base_url = "https://api.groq.com/openai/v1"
            else:
                api_key = os.getenv("GEMINI_API_KEY" if api_type == "gemini" else "OPENAI_API_KEY")
                base_url = "https://generativelanguage.googleapis.com/v1beta/openai/" if api_type == "gemini" else None
            if not api_key:
                raise ValueError(f"Missing {api_type.upper()} API key. Set the appropriate environment variable.")
            self.client = OpenAI(api_key=api_key, base_url=base_url)

    def unload(self):
        release_model()


def llm_configure(model_str, cost_per_M_input=None, cost_per_M_output=None,
                  llm_provider=None):
    """
    Configures the model based on the provided model_str and optional llm_provider.
    
    If llm_provider is None, then:
      - If the expanded model_str is in OPENAI_MODELS or GEMINI_MODELS, then a configuration 
        using that API is returned.
      - Otherwise, a local Hugging Face model is loaded (with caching).
    
    If llm_provider is provided, it must be one of "openai", "gemini", "together", or "groq",
    and the corresponding API is used regardless of the model name.
    
    Parameters:
      model_str (str): A model name string (can be a short alias or a full model name).
      cost_per_M_input (float, optional): Cost per million input tokens (for API-based models).
      cost_per_M_output (float, optional): Cost per million output tokens (for API-based models).
      llm_provider (str, optional): Either None (to load a local model or default API from short name)
          or one of "openai", "gemini", "together", or "groq".
    
    Returns:
      ModelConfig: The configuration object with model, tokenizer, and API client settings.
    """
    model_str = model_str.strip()
    valid_api_providers = ["openai", "gemini", "together", "groq"]
    
    # Expand short names.
    if model_str in SHORT_NAME_MAP:
        model_str = SHORT_NAME_MAP[model_str]
    
    # If llm_provider is provided, use that API regardless of model_str.
    if llm_provider is not None:
        if llm_provider.lower() not in valid_api_providers:
            raise ValueError("llm_provider must be either None or one of 'openai', 'gemini', 'together', or 'groq'")
        return ModelConfig(model_str, api_type=llm_provider.lower(),
                           cost_per_M_input=cost_per_M_input, cost_per_M_output=cost_per_M_output)
    
    # If llm_provider is None, check if the model_str is one of the API models.
    if model_str in OPENAI_MODELS:
        return ModelConfig(model_str, api_type="openai",
                           cost_per_M_input=cost_per_M_input, cost_per_M_output=cost_per_M_output)
    if model_str in GEMINI_MODELS:
        return ModelConfig(model_str, api_type="gemini",
                           cost_per_M_input=cost_per_M_input, cost_per_M_output=cost_per_M_output)
    
    # Otherwise, assume a local Hugging Face model.
    global LOADED_MODELS, LOADED_TOKENIZERS
    if model_str in LOADED_MODELS:
        return ModelConfig(model_str, LOADED_MODELS[model_str], LOADED_TOKENIZERS[model_str],
                           cost_per_M_input=cost_per_M_input, cost_per_M_output=cost_per_M_output)
    elif LOADED_MODELS:
        release_model()
        torch.cuda.empty_cache()
        gc.collect()
    
    print(f"üöÄ Loading model: {model_str} (this may take a while)...")
    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_str,
            torch_dtype=torch.float16,
            device_map="auto",
        )
        tokenizer = AutoTokenizer.from_pretrained(model_str)
        LOADED_MODELS[model_str] = model
        LOADED_TOKENIZERS[model_str] = tokenizer
        print(f"üü¢ Model {model_str} loaded successfully.\n")
        return ModelConfig(model_str, model, tokenizer,
                           cost_per_M_input=cost_per_M_input, cost_per_M_output=cost_per_M_output)
    except Exception as e:
        print(f"‚ùå Error loading model {model_str}: {str(e)}")
        return None

import re

def clean_response(response, prompt=None, cleaning_mode="generic", split_string=None):
    """
    Cleans the response by removing unwanted text.

    If split_string is provided (not None), the function searches for split_string in the response
    and removes everything up to and including any whitespace or new lines immediately after it,
    returning the remainder.

    If split_string is None, the function applies the original cleaning methods:
      - In "qwen" mode, it uses the last 10 characters of the prompt as a marker and removes all text
        up to and including the line immediately following that marker.
      - In generic mode, it removes the input prompt, an "assistant" label, and any extraneous blank lines.
    """
    if split_string is not None:
        pos = response.find(split_string)
        if pos != -1:
            pos_end = pos + len(split_string)
            # Skip any whitespace or newlines immediately after split_string.
            while pos_end < len(response) and response[pos_end] in " \t\n\r":
                pos_end += 1
            return response[pos_end:].strip()
        else:
            return response.strip()

    if cleaning_mode == "qwen" and prompt:
        # Use only the last 10 characters of the prompt as the marker.
        marker = prompt[-10:]
        pos = response.find(marker)
        if pos != -1:
            # Find the end of the line containing the marker.
            end_line = response.find("\n", pos)
            if end_line != -1:
                # Remove up to and including the next line.
                next_line_end = response.find("\n", end_line + 1)
                if next_line_end != -1:
                    response = response[next_line_end+1:].strip()
                else:
                    response = response[end_line+1:].strip()
            else:
                response = response[pos+len(marker):].strip()
        else:
            # Fallback: if the marker isn't found, remove the first line.
            lines = response.splitlines()
            if len(lines) > 1:
                response = "\n".join(lines[1:]).strip()
            else:
                response = response.strip()
    else:
        if prompt:
            # --- Original prompt removal logic ---
            prompt_marker_pattern = re.escape(prompt) + r"\s*\.\s*assistant"
            match = re.search(prompt_marker_pattern, response)
            if match:
                response = response[match.end():].strip()
            else:
                match = re.search(re.escape(prompt), response)
                if match:
                    response = response[match.end():].strip()

            # --- Additional marker removal ---
            extra_marker = prompt[-10:] + "assistant"
            pattern = ""
            for char in extra_marker:
                if char.isspace():
                    pattern += r"\s+"
                else:
                    pattern += re.escape(char) + r"\s*"
            match = re.search(pattern, response, flags=re.IGNORECASE)
            if match:
                response = response[match.end():].strip()

    # Finally, remove any lines that consist solely of the word "assistant" (ignoring case)
    response = re.sub(r"^\s*assistant\s*", "", response, flags=re.IGNORECASE).strip()
    response = "\n".join([line for line in response.split("\n") if line.strip()])
    return response

'''
def clean_response(response, prompt=None, cleaning_mode="generic"):
    """
    Cleans the response by removing the input prompt, an 'assistant' label,
    and any extraneous blank lines.
    
    In generic mode, the original cleaning behavior is applied.
    In Qwen mode, the function looks for the last 10 characters of the prompt as a marker
    (to account for potential formatting changes) and removes all text up to and including
    the line that immediately follows that marker.
    """
    if cleaning_mode == "qwen" and prompt:
        # Use only the last 10 characters of the prompt as the marker.
        marker = prompt[-10:]
        pos = response.find(marker)
        if pos != -1:
            # Find the end of the line containing the marker.
            end_line = response.find("\n", pos)
            if end_line != -1:
                # Remove up to and including the next line.
                next_line_end = response.find("\n", end_line + 1)
                if next_line_end != -1:
                    response = response[next_line_end+1:].strip()
                else:
                    response = response[end_line+1:].strip()
            else:
                response = response[pos+len(marker):].strip()
        else:
            # Fallback: if the marker isn't found, remove the first line.
            lines = response.splitlines()
            if len(lines) > 1:
                response = "\n".join(lines[1:]).strip()
            else:
                response = response.strip()
    else:
        if prompt:
            # --- Original prompt removal logic ---
            prompt_marker_pattern = re.escape(prompt) + r"\s*\.\s*assistant"
            match = re.search(prompt_marker_pattern, response)
            if match:
                response = response[match.end():].strip()
            else:
                match = re.search(re.escape(prompt), response)
                if match:
                    response = response[match.end():].strip()

            # --- Additional marker removal ---
            extra_marker = prompt[-10:] + "assistant"
            pattern = ""
            for char in extra_marker:
                if char.isspace():
                    pattern += r"\s+"
                else:
                    pattern += re.escape(char) + r"\s*"
            match = re.search(pattern, response, flags=re.IGNORECASE)
            if match:
                response = response[match.end():].strip()
    
    # Finally, remove any lines that consist solely of the word "assistant" (ignoring case).
    response = re.sub(r"^\s*assistant\s*", "", response, flags=re.IGNORECASE).strip()
    response = "\n".join([line for line in response.split("\n") if line.strip()])
    return response
'''

def configure_gen_kwargs(strategy, top_k, top_p, num_beams, temperature, max_new_tokens):
    """
    Returns a dictionary of generation kwargs based on the search strategy.
    """
    gen_kwargs = {"max_new_tokens": max_new_tokens}
    if strategy == "top_k":
        gen_kwargs.update({"do_sample": True, "top_k": top_k, "temperature": temperature})
    elif strategy == "top_p":
        gen_kwargs.update({"do_sample": True, "top_p": top_p, "temperature": temperature})
    elif strategy == "contrastive":
        gen_kwargs.update({"do_sample": True, "penalty_alpha": 0.6, "top_k": 4, "temperature": temperature})
    elif strategy == "beam_search":
        gen_kwargs.update({"do_sample": False, "num_beams": num_beams, "temperature": temperature})
    elif strategy == "deterministic":
        gen_kwargs.update({"do_sample": False, "num_beams": 1, "top_p": 1.0, "temperature": None, "top_k": None})
    else:
        gen_kwargs.update({"do_sample": True, "top_p": top_p, "temperature": temperature})
    return gen_kwargs

def generate_api_text(model_config, prompt_list, system_prompt, assistant_prompt,
                      search_strategy, max_new_tokens, temperature, top_p,
                      rate_limit, estimate_cost, remove_input_prompt, split_string):
    """
    Generates text using an API-based model.
    """
    responses = []
    num_input_tokens = 0.0
    num_output_tokens = 0.0
    messages = [{"role": "system", "content": system_prompt}]
    if assistant_prompt is not None:
        messages.append({"role": "assistant", "content": assistant_prompt})
    
    for prompt in prompt_list:
        user_message = {"role": "user", "content": prompt}
        full_messages = messages + [user_message]
        openai_params = {
            "model": model_config.model_str,
            "messages": full_messages,
            "max_tokens": max_new_tokens
        }
        if search_strategy == "deterministic":
            openai_params["temperature"] = 0.0
            openai_params["top_p"] = 1.0
        else:
            openai_params["temperature"] = temperature
            openai_params["top_p"] = top_p
        
        response = model_config.client.chat.completions.create(**openai_params)
        response_text = response.choices[0].message.content.strip()
        responses.append(clean_response(response_text, prompt, split_string=split_string) if remove_input_prompt else response_text)
        
        if rate_limit is not None:
            time.sleep(60.0 / rate_limit)
        if estimate_cost:
            num_input_tokens += response.usage.prompt_tokens
            num_output_tokens += response.usage.completion_tokens
    
    if estimate_cost:
        if model_config.cost_per_M_input is not None and model_config.cost_per_M_output is not None:
            total_cost = ((num_input_tokens / 1_000_000) * model_config.cost_per_M_input) + \
                         ((num_output_tokens / 1_000_000) * model_config.cost_per_M_output)
            print(f"üí∞ Estimated Cost: ${total_cost:.6f} (Input: {num_input_tokens} tokens, Output: {num_output_tokens} tokens)")
        else:
            print("‚ö†Ô∏è Cost parameters not set. Cannot estimate cost.")
            print(f"üí∞ Token Usage: Input: {num_input_tokens} tokens, Output: {num_output_tokens} tokens")
    
    return responses


def generate_local_text(model_config, prompt_list, system_prompt, assistant_prompt,
                        search_strategy, max_new_tokens, temperature, top_p,
                        num_beams, top_k, remove_input_prompt, batch_size, disable_tqdm, split_string):
    """
    Generates text using a local Hugging Face model.
    Batches the inputs and uses a tqdm progress bar if there is more than one prompt.
    """
    tokenizer = model_config.tokenizer
    model = model_config.model

    if model is None or tokenizer is None:
        return "‚ùå Error: Model or tokenizer is not properly initialized."
    
    if tokenizer.eos_token_id is None:
        tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)
    
    def process_batch(batch_prompts):
        if hasattr(tokenizer, "apply_chat_template") and getattr(tokenizer, "chat_template", None) is not None:
            conversations = []
            for p in batch_prompts:
                conv = [{"role": "system", "content": system_prompt}]
                if assistant_prompt is not None:
                    conv.append({"role": "assistant", "content": assistant_prompt})
                conv.append({"role": "user", "content": p})
                conversations.append(conv)
            input_ids = tokenizer.apply_chat_template(conversations, return_tensors="pt",
                                                      padding=True, truncation=True).to(model.device)
        else:
            new_prompts = []
            for p in batch_prompts:
                prompt_parts = [system_prompt]
                if assistant_prompt is not None:
                    prompt_parts.append("Assistant: " + assistant_prompt)
                prompt_parts.append("User: " + p)
                new_prompts.append("\n".join(prompt_parts))
            input_ids = tokenizer(new_prompts, return_tensors="pt",
                                  padding=True, truncation=True).to(model.device)
        
        terminators = [token for token in [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids("<|eot_id|>")]
                       if token is not None]
        
        gen_kwargs = configure_gen_kwargs(search_strategy, top_k, top_p, num_beams, temperature, max_new_tokens)
        gen_kwargs.update({"repetition_penalty": 1.2, "eos_token_id": terminators})
        
        with torch.no_grad():
            output = model.generate(input_ids, **gen_kwargs)
        batch_responses = tokenizer.batch_decode(output, skip_special_tokens=True)
        if remove_input_prompt:
            cleaning_mode = "qwen" if "qwen" in model_config.model_str.lower() else "generic"
            return [clean_response(resp, prompt, cleaning_mode=cleaning_mode, split_string=split_string)
                    for resp, prompt in zip(batch_responses, batch_prompts)]
        else:
            return batch_responses

    responses = []
    use_tqdm = (len(prompt_list) > 1) and (not disable_tqdm)
    if use_tqdm:
        from tqdm.autonotebook import tqdm
        pbar = tqdm(total=len(prompt_list), desc="Local Generation", leave=True, dynamic_ncols=True, file=sys.stdout)
    else:
        pbar = contextlib.nullcontext()
    
    with pbar:
        for i in range(0, len(prompt_list), batch_size):
            batch_prompts = prompt_list[i:i + batch_size]
            responses.extend(process_batch(batch_prompts))
            if use_tqdm:
                pbar.update(len(batch_prompts))
    
    return responses


def llm_generate(model_config, prompts, 
                 batch_size=1, assistant_prompt=None, estimate_cost=False, max_new_tokens=200, 
                 remove_input_prompt=True, search_strategy="top_p", top_k=50, top_p=0.9, num_beams=1, temperature=0.7,
                 system_prompt="You are an AI assistant that provides brief answers. Do not include the input prompt in your answer.",
                 rate_limit=None,
                 disable_tqdm=False,
                 split_string=None):
    """
    Generates responses from a language model.
    Dispatches API-based or local text generation.
    Disables tqdm if there is only one prompt.
    """
    if model_config is None:
        return "‚ùå Error: Invalid model configuration. Please check the model name."
    
    is_batch = isinstance(prompts, list)
    prompt_list = prompts if is_batch else [prompts]
    if len(prompt_list) == 1:
        disable_tqdm = True
    
    if model_config.api_type in ["openai", "gemini", "together", "groq"]:
        try:
            responses = generate_api_text(model_config, prompt_list, system_prompt, assistant_prompt,
                                          search_strategy, max_new_tokens, temperature, top_p,
                                          rate_limit, estimate_cost, remove_input_prompt, split_string)
        except Exception as e:
            return f"{model_config.api_type.capitalize()} API error: {str(e)}"
    else:
        responses = generate_local_text(model_config, prompt_list, system_prompt, assistant_prompt,
                                        search_strategy, max_new_tokens, temperature, top_p,
                                        num_beams, top_k, remove_input_prompt, batch_size, disable_tqdm, split_string)
    
    return responses if is_batch else responses[0]

def generate_text(prompts, 
                  model_name='llama-3p2-3B', 
                  llm_provider=None, 
                  system_prompt="You are an AI assistant that provides brief answers. Do not include the input prompt in your answer.",
                  assistant_prompt=None,
                  batch_size=1,
                  cost_per_M_input=None, 
                  cost_per_M_output=None,
                  disable_tqdm=False,
                  estimate_cost=False,
                  max_new_tokens=200,
                  num_beams=1,
                  rate_limit=None,
                  remove_input_prompt=True,
                  search_strategy="top_p",
                  split_string=None,
                  temperature=0.7,
                  top_k=50,
                  top_p=0.9):
    """
    Generates responses from a language model given a prompt or list of prompts.
    
    This high-level interface accepts a model name (or short name) along with an optional
    llm_provider. If llm_provider is None, then the model is loaded either locally (if not an API model)
    or using the default API (if the model is one of "gpt-4o", "gpt-4o-mini", "gemini-flash", etc.).
    If llm_provider is specified (e.g. "openai" or "gemini"), the code uses that API.
    
    Parameters:
      prompts (str or list of str): The input prompt(s) for text generation.
      model_name (str, optional): Full or short model name. Defaults to 'llama3p2-3B'.
      llm_provider (str, optional): Provider for the model. If None, a local model is loaded or 
          the default API is used if the model name indicates an API model. Otherwise, must be one of 
          "openai", "gemini", "together", or "groq".
      system_prompt (str, optional): The system-level instruction prompt.
      assistant_prompt (str, optional): An optional previous assistant message.
      batch_size (int, optional): Number of prompts processed per batch. Defaults to 1.
      cost_per_M_input (float, optional): Cost per million input tokens (for API-based models).
      cost_per_M_output (float, optional): Cost per million output tokens (for API-based models).
      disable_tqdm (bool, optional): If True, disables progress bars. Defaults to False.
      estimate_cost (bool, optional): If True, prints estimated token cost when available.
      max_new_tokens (int, optional): Maximum number of new tokens to generate. Defaults to 200.
      num_beams (int, optional): Number of beams for beam search. Defaults to 1.
      rate_limit (float, optional): Maximum API requests per minute (if applicable).
      remove_input_prompt (bool, optional): If True, removes the input prompt from the generated response.
      search_strategy (str, optional): Decoding strategy; options include "top_k", "top_p", "contrastive", 
          "beam_search", and "deterministic". Defaults to "top_p".
      temperature (float, optional): Sampling temperature. Defaults to 0.7.
      top_k (int, optional): Number of top tokens to consider for "top_k" sampling. Defaults to 50.
      top_p (float, optional): Cumulative probability threshold for nucleus sampling. Defaults to 0.9.
    
    Returns:
      str or list of str: The generated response(s) as a string or a list of strings.
    """
    # Expand short names if necessary.
    if model_name in SHORT_NAME_MAP:
        model_str = SHORT_NAME_MAP[model_name]
    else:
        model_str = model_name.strip()
    
    # Use llm_configure to obtain the ModelConfig object.
    config = llm_configure(model_str, cost_per_M_input=cost_per_M_input, 
                           cost_per_M_output=cost_per_M_output, llm_provider=llm_provider)
    if config is None:
        return f"‚ùå Error: Unable to configure model {model_str}."
    
    # Dispatch to legacy llm_generate.
    return llm_generate(config, prompts, batch_size=batch_size, assistant_prompt=assistant_prompt,
                        estimate_cost=estimate_cost, max_new_tokens=max_new_tokens,
                        remove_input_prompt=remove_input_prompt, search_strategy=search_strategy,
                        top_k=top_k, top_p=top_p, num_beams=num_beams, temperature=temperature,
                        system_prompt=system_prompt, rate_limit=rate_limit, disable_tqdm=disable_tqdm,
                        split_string=split_string)



def clear_pipeline(pipe, verbosity=0):
    """Clears a Hugging Face pipeline and frees CUDA memory."""
    if hasattr(pipe, "model") and next(pipe.model.parameters()).is_cuda:
        initial_allocated = torch.cuda.memory_allocated() / 1e6
        initial_reserved = torch.cuda.memory_reserved() / 1e6

        if verbosity > 0:
            print(f"üîç Before unloading: {initial_allocated:.2f} MB allocated, {initial_reserved:.2f} MB reserved.")

        try:
            pipe.model.to("cpu")
            for param in pipe.model.parameters():
                param.data = param.data.cpu()
        except Exception as e:
            if verbosity > 0:
                print(f"‚ö†Ô∏è Error moving model to CPU: {e}")

        del pipe.model
        del pipe
        gc.collect()
        torch.cuda.empty_cache()

        final_allocated = torch.cuda.memory_allocated() / 1e6
        final_reserved = torch.cuda.memory_reserved() / 1e6

        if verbosity > 0:
            print(f"‚úÖ Pipeline cleared. Freed {initial_allocated - final_allocated:.2f} MB allocated, "
                  f"{initial_reserved - final_reserved:.2f} MB reserved.")
    else:
        if verbosity > 0:
            print("‚ÑπÔ∏è Pipeline already on CPU. Performing standard cleanup.")
        del pipe
        gc.collect()

    if verbosity > 0:
        print("üóëÔ∏è Cleanup complete.")
    elif verbosity == 0:
        print("‚úÖ Pipeline cleared.")

def print_pipeline_info(pipe):
    model = pipe.model
    model_size = sum(p.numel() for p in model.parameters())
    print(f"Model: {model.name_or_path}, Size: {model_size:,} parameters")

def display_markdown(response):
    if not response or not isinstance(response, str):
        display(Markdown("‚ö†Ô∏è *No valid response to display.*"))
        return
    display(Markdown(response))

class JupyterChat:
    def __init__(self, model_str, system_prompt="You are a helpful assistant."):
        self.model_config = llm_configure(model_str)
        if not self.model_config:
            raise ValueError(f"Could not load model: {model_str}")
        
        self.system_prompt = system_prompt
        self.chat_history = []
        
        self.text_input = widgets.Text(
            placeholder="Type your message here...",
            description="User:",
            layout=widgets.Layout(width="100%")
        )
        self.send_button = widgets.Button(description="Send", button_style='primary')
        self.clear_button = widgets.Button(description="Clear Chat", button_style='warning')
        self.copy_button = widgets.Button(description="Copy", button_style='success')
        self.output_area = widgets.Output()

        self.send_button.on_click(self.handle_input)
        self.text_input.on_submit(self.handle_input)
        self.clear_button.on_click(self.clear_chat)
        self.copy_button.on_click(self.copy_chat)

        button_box = widgets.HBox([self.send_button, self.clear_button, self.copy_button])
        display(self.text_input, button_box, self.output_area)

    def handle_input(self, event):
        user_input = self.text_input.value.strip()
        if not user_input:
            return
        self.text_input.value = ""
        self.chat_history.append({"role": "user", "content": user_input})
        response = self.generate_response(user_input)
        self.chat_history.append({"role": "assistant", "content": response})
        self.update_display()

    def generate_response(self, user_input):
        try:
            response = llm_generate(self.model_config, prompts=user_input, system_prompt=self.system_prompt)
            return response
        except Exception as e:
            return self.format_api_error(e)

    def update_display(self):
        with self.output_area:
            clear_output()
            for entry in self.chat_history:
                role = "**User:**" if entry["role"] == "user" else "**Assistant:**"
                message = f"{role}\n\n{entry['content']}"
                display_markdown(message)

    def clear_chat(self, event):
        self.chat_history = []
        with self.output_area:
            clear_output()
        print("Chat history cleared.")

    def copy_chat(self, event):
        chat_text = "\n".join([f"{entry['role'].capitalize()}: {entry['content']}" for entry in self.chat_history])
        pyperclip.copy(chat_text)
        print("‚úÖ Chat copied to clipboard!")

    def format_api_error(self, error):
        error_str = str(error)
        if "429" in error_str:
            return "‚ö†Ô∏è *API limit reached. Please check your quota.*"
        elif "401" in error_str:
            return "‚ö†Ô∏è *Invalid API key. Please verify your credentials.*"
        elif "RESOURCE_EXHAUSTED" in error_str:
            return "‚ö†Ô∏è *Quota exceeded. Try again later.*"
        return f"‚ö†Ô∏è *API Error:* {error_str}"
