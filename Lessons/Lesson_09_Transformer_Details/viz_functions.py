import torch
import matplotlib.pyplot as plt
import numpy as np
#from bertviz import head_view
import ipywidgets as widgets
from IPython.display import display, HTML, Javascript
import ipywidgets as widgets
from mpl_toolkits.axes_grid1 import make_axes_locatable

import json
import os
import uuid


def visualize_positional_encodings(pos_encoder, max_len=50, d_model=16, figsize=(8,5)):
    """
    Visualizes the positional encodings generated by the PositionalEncoding module.

    Args:
        pos_encoder (PositionalEncoding): Instance of the PositionalEncoding class.
        max_len (int): Number of positions to visualize.
        d_model (int): Dimensionality of the embedding space to visualize.
    """
    # Generate a dummy input to get positional encodings
    dummy_input = torch.zeros(1, max_len, d_model)
    pos_encodings = pos_encoder(dummy_input)[0].detach().numpy()  # Remove batch dimension
    
    # Create the plot
    plt.figure(figsize=figsize)
    plt.title(f"Positional Encodings (Max Length: {max_len}, Embedding Dim: {d_model})")
    plt.imshow(pos_encodings, aspect='auto', cmap='viridis')
    plt.colorbar(label="Encoding Value")
    plt.xlabel("Embedding Dimension")
    plt.ylabel("Position Index")
    plt.show()

def plot_attention_weights_widget(model, loader, vocab, figsize=(6, 6)):
    def visualize(layer_idx, seq_idx):
        # Get a batch from the loader
        for batch_idx, (inputs, labels) in enumerate(loader):
            break  # Take the first batch

        # Move inputs and model to the correct device
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        inputs = inputs.to(device)
        model_on_device = model.to(device)  # Move model to the same device

        # Forward pass to generate predictions
        outputs = model_on_device(inputs)

        # Extract attention weights
        attention_weights = model_on_device.get_attention_weights()  # List of attention weights from all layers

        # Validate indices
        num_layers = len(attention_weights)
        if layer_idx < 0 or layer_idx >= num_layers:
            print(f"Invalid layer index {layer_idx}. Must be between 0 and {num_layers - 1}.")
            return
        
        if seq_idx < 0 or seq_idx >= inputs.size(0):
            print(f"Invalid sequence index {seq_idx}. Must be between 0 and {inputs.size(0) - 1}.")
            return

        # Select attention weights for the specified sequence and layer
        attention_for_idx = attention_weights[layer_idx][seq_idx].detach().cpu().numpy()  # Shape: (seq_len, seq_len)
        inputs_for_idx = inputs[seq_idx].cpu()
        mask = (inputs_for_idx != 0)
        attention_for_idx = attention_for_idx[:, mask][mask, :]
        inputs_for_idx = inputs_for_idx[mask].numpy()

        # Map token indices to tokens
        idx_to_token = {v: k for k, v in vocab.items()}
        tokens = [idx_to_token.get(i, "[UNK]") for i in inputs_for_idx]

        # Plot the attention weights without gridlines
        fig, ax = plt.subplots(figsize=figsize)
        im = ax.imshow(attention_for_idx, cmap='viridis')

        # Adjust colorbar height to match the plot
        divider = make_axes_locatable(ax)
        cax = divider.append_axes("right", size="5%", pad=0.05)  # Adjust width and padding of colorbar
        cb = fig.colorbar(im, cax=cax)
        cb.ax.tick_params(labelsize=10)  # Adjust tick label size

        # Add titles and labels
        ax.set_title(f"Attention Weights, Layer {layer_idx}", fontsize=12)
        ax.set_xlabel("Keys", fontsize=12)
        ax.set_ylabel("Queries", fontsize=12)
        ax.grid(False)  # Remove gridlines

        # Move xticks to the top
        ax.xaxis.set_ticks_position('top')
        ax.xaxis.set_label_position('top')

        ax.set_xticks(np.arange(attention_for_idx.shape[1]))
        ax.set_xticklabels(tokens, rotation=45, fontsize=12)
        ax.set_yticks(np.arange(attention_for_idx.shape[0]))
        ax.set_yticklabels(tokens, fontsize=12)

        plt.show()

    # Get number of layers and sequences
    num_layers = len(model.get_attention_weights())
    for batch_idx, (inputs, _) in enumerate(loader):
        num_sequences = inputs.size(0)
        break

    # Create widgets
    layer_dropdown = widgets.Dropdown(
        options=list(range(num_layers)),
        value=0,
        description="Layer:",
    )
    seq_slider = widgets.IntSlider(
        value=2,
        min=0,
        max=num_sequences - 1,
        step=1,
        description="Sequence:",
        continuous_update=False,
    )
    seq_input = widgets.BoundedIntText(
        value=2,
        min=0,
        max=num_sequences - 1,
        step=1,
        description="Seq Input:",
    )

    # Link slider and input box
    widgets.jslink((seq_slider, 'value'), (seq_input, 'value'))

    # Interactive display
    interact_ui = widgets.VBox([layer_dropdown, widgets.HBox([seq_slider, seq_input])])
    display(interact_ui)

    # Update the visualization when widget values change
    output = widgets.interactive_output(
        visualize, {'layer_idx': layer_dropdown, 'seq_idx': seq_slider}
    )
    display(output)


def plot_attention_weights(model, loader, vocab, layer_idx, idx, figsize=(6, 6)):
    import matplotlib.pyplot as plt

    # Get a batch from the loader
    for batch_idx, (inputs, labels) in enumerate(loader):
        break  # Take the first batch

    # Move inputs and model to the correct device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    inputs = inputs.to(device)
    model = model.to(device)

    # Forward pass to generate predictions
    outputs = model(inputs)

    # Extract attention weights
    attention_weights = model.get_attention_weights()  # List of attention weights from all layers

    # Select attention weights for the specified sequence and layer
    attention_for_idx = attention_weights[layer_idx][idx].detach().cpu().numpy()  # Shape: (seq_len, seq_len)
    inputs_for_idx = inputs[idx].cpu()
    mask = (inputs_for_idx != 0)
    attention_for_idx = attention_for_idx[:, mask][mask, :]
    inputs_for_idx = inputs_for_idx[mask].numpy()

    # Map token indices to tokens
    idx_to_token = {v: k for k, v in vocab.items()}
    tokens = [idx_to_token.get(i, "[UNK]") for i in inputs_for_idx]

    # Plot the attention weights without gridlines
    plt.figure(figsize=figsize)
    plt.imshow(attention_for_idx, cmap='viridis')
    plt.title(f"Attention Weights, Layer {layer_idx}")
    plt.xlabel("Keys")
    plt.ylabel("Querys")
    plt.colorbar()
    plt.grid(False)  # Remove gridlines

    # Move xticks to the top
    plt.gca().xaxis.set_ticks_position('top')
    plt.gca().xaxis.set_label_position('top')

    plt.xticks(ticks=np.arange(attention_for_idx.shape[1]), labels=tokens, rotation=45)
    plt.yticks(ticks=np.arange(attention_for_idx.shape[0]), labels=tokens)

    plt.show()


def display_attention(idx, model, test_loader, vocab):
    """
    Displays the BertViz head_view for a specific sequence in the batch.

    Args:
        idx (int): Index of the sequence in the batch to visualize.
        model (torch.nn.Module): The trained model.
        test_loader (DataLoader): DataLoader containing the test dataset.
        vocab (dict): Mapping of token indices to tokens.
    """
    # Get a batch from the test loader
    for batch_idx, (inputs, labels) in enumerate(test_loader):
        break  # Take the first batch

    # Move inputs and model to the correct device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    inputs = inputs.to(device)
    model = model.to(device)

    # Forward pass to generate predictions
    outputs = model(inputs)

    # Extract attention weights
    attention_weights = model.get_attention_weights()  # List of attention weights from all layers

    # Select the sequence and reshape attention weights
    inputs_for_idx = inputs[idx].cpu()
    attention_for_idx0 = [
        layer_attention[idx].unsqueeze(0).unsqueeze(0).detach().cpu()
        for layer_attention in attention_weights
    ]

    # Remove padding
    mask = (inputs_for_idx != 0)
    attention_for_idx = [
        layer_attention[:, :, mask][:, :, :, mask]
        for layer_attention in attention_for_idx0
    ]
    inputs_for_idx = inputs_for_idx[mask].numpy()

    # Map token indices to tokens
    idx_to_token = {v: k for k, v in vocab.items()}
    tokens = [idx_to_token.get(i, "[UNK]") for i in inputs_for_idx]

    # Display BertViz head_view
    head_view(
        attention=attention_for_idx,  # Attention weights as a list of tensors
        tokens=tokens,
    )


def head_view(
        attention=None,
        tokens=None,
        sentence_b_start=None,
        prettify_tokens=True,
        layer=None,
        heads=None,
        encoder_attention=None,
        decoder_attention=None,
        cross_attention=None,
        encoder_tokens=None,
        decoder_tokens=None,
        include_layers=None,
        html_action='view'
):
    """Render head view

        Args:
            For self-attention models:
                attention: list of ``torch.FloatTensor``(one for each layer) of shape
                    ``(batch_size(must be 1), num_heads, sequence_length, sequence_length)``
                tokens: list of tokens
                sentence_b_start: index of first wordpiece in sentence B if input text is sentence pair (optional)
            For encoder-decoder models:
                encoder_attention: list of ``torch.FloatTensor``(one for each layer) of shape
                    ``(batch_size(must be 1), num_heads, encoder_sequence_length, encoder_sequence_length)``
                decoder_attention: list of ``torch.FloatTensor``(one for each layer) of shape
                    ``(batch_size(must be 1), num_heads, decoder_sequence_length, decoder_sequence_length)``
                cross_attention: list of ``torch.FloatTensor``(one for each layer) of shape
                    ``(batch_size(must be 1), num_heads, decoder_sequence_length, encoder_sequence_length)``
                encoder_tokens: list of tokens for encoder input
                decoder_tokens: list of tokens for decoder input
            For all models:
                prettify_tokens: indicates whether to remove special characters in wordpieces, e.g. Ġ
                layer: index (zero-based) of initial selected layer in visualization. Defaults to layer 0.
                heads: Indices (zero-based) of initial selected heads in visualization. Defaults to all heads.
                include_layers: Indices (zero-based) of layers to include in visualization. Defaults to all layers.
                    Note: filtering layers may improve responsiveness of the visualization for long inputs.
                html_action: Specifies the action to be performed with the generated HTML object
                    - 'view' (default): Displays the generated HTML representation as a notebook cell output
                    - 'return' : Returns an HTML object containing the generated view for further processing or custom visualization
    """

    attn_data = []
    if attention is not None:
        if tokens is None:
            raise ValueError("'tokens' is required")
        if encoder_attention is not None or decoder_attention is not None or cross_attention is not None \
                or encoder_tokens is not None or decoder_tokens is not None:
            raise ValueError("If you specify 'attention' you may not specify any encoder-decoder arguments. This"
                             " argument is only for self-attention models.")
        if include_layers is None:
            include_layers = list(range(num_layers(attention)))
        attention = format_attention(attention, include_layers)
        if sentence_b_start is None:
            attn_data.append(
                {
                    'name': None,
                    'attn': attention.tolist(),
                    'left_text': tokens,
                    'right_text': tokens
                }
            )
        else:
            slice_a = slice(0, sentence_b_start)  # Positions corresponding to sentence A in input
            slice_b = slice(sentence_b_start, len(tokens))  # Position corresponding to sentence B in input
            attn_data.append(
                {
                    'name': 'All',
                    'attn': attention.tolist(),
                    'left_text': tokens,
                    'right_text': tokens
                }
            )
            attn_data.append(
                {
                    'name': 'Sentence A -> Sentence A',
                    'attn': attention[:, :, slice_a, slice_a].tolist(),
                    'left_text': tokens[slice_a],
                    'right_text': tokens[slice_a]
                }
            )
            attn_data.append(
                {
                    'name': 'Sentence B -> Sentence B',
                    'attn': attention[:, :, slice_b, slice_b].tolist(),
                    'left_text': tokens[slice_b],
                    'right_text': tokens[slice_b]
                }
            )
            attn_data.append(
                {
                    'name': 'Sentence A -> Sentence B',
                    'attn': attention[:, :, slice_a, slice_b].tolist(),
                    'left_text': tokens[slice_a],
                    'right_text': tokens[slice_b]
                }
            )
            attn_data.append(
                {
                    'name': 'Sentence B -> Sentence A',
                    'attn': attention[:, :, slice_b, slice_a].tolist(),
                    'left_text': tokens[slice_b],
                    'right_text': tokens[slice_a]
                }
            )
    elif encoder_attention is not None or decoder_attention is not None or cross_attention is not None:
        if encoder_attention is not None:
            if encoder_tokens is None:
                raise ValueError("'encoder_tokens' required if 'encoder_attention' is not None")
            if include_layers is None:
                include_layers = list(range(num_layers(encoder_attention)))
            encoder_attention = format_attention(encoder_attention, include_layers)
            attn_data.append(
                {
                    'name': 'Encoder',
                    'attn': encoder_attention.tolist(),
                    'left_text': encoder_tokens,
                    'right_text': encoder_tokens
                }
            )
        if decoder_attention is not None:
            if decoder_tokens is None:
                raise ValueError("'decoder_tokens' required if 'decoder_attention' is not None")
            if include_layers is None:
                include_layers = list(range(num_layers(decoder_attention)))
            decoder_attention = format_attention(decoder_attention, include_layers)
            attn_data.append(
                {
                    'name': 'Decoder',
                    'attn': decoder_attention.tolist(),
                    'left_text': decoder_tokens,
                    'right_text': decoder_tokens
                }
            )
        if cross_attention is not None:
            if encoder_tokens is None:
                raise ValueError("'encoder_tokens' required if 'cross_attention' is not None")
            if decoder_tokens is None:
                raise ValueError("'decoder_tokens' required if 'cross_attention' is not None")
            if include_layers is None:
                include_layers = list(range(num_layers(cross_attention)))
            cross_attention = format_attention(cross_attention, include_layers)
            attn_data.append(
                {
                    'name': 'Cross',
                    'attn': cross_attention.tolist(),
                    'left_text': decoder_tokens,
                    'right_text': encoder_tokens
                }
            )
    else:
        raise ValueError("You must specify at least one attention argument.")

    if layer is not None and layer not in include_layers:
        raise ValueError(f"Layer {layer} is not in include_layers: {include_layers}")

    # Generate unique div id to enable multiple visualizations in one notebook
    vis_id = 'bertviz-%s'%(uuid.uuid4().hex)

    # Compose html
    if len(attn_data) > 1:
        options = '\n'.join(
            f'<option value="{i}">{attn_data[i]["name"]}</option>'
            for i, d in enumerate(attn_data)
        )
        select_html = f'Attention: <select id="filter">{options}</select>'
    else:
        select_html = ""
    vis_html = f"""      
        <div id="{vis_id}" style="font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;">
            <span style="user-select:none">
                Layer: <select id="layer"></select>
                {select_html}
            </span>
            <div id='vis'></div>
        </div>
    """

    for d in attn_data:
        attn_seq_len_left = len(d['attn'][0][0])
        if attn_seq_len_left != len(d['left_text']):
            raise ValueError(
                f"Attention has {attn_seq_len_left} positions, while number of tokens is {len(d['left_text'])} "
                f"for tokens: {' '.join(d['left_text'])}"
            )
        attn_seq_len_right = len(d['attn'][0][0][0])
        if attn_seq_len_right != len(d['right_text']):
            raise ValueError(
                f"Attention has {attn_seq_len_right} positions, while number of tokens is {len(d['right_text'])} "
                f"for tokens: {' '.join(d['right_text'])}"
            )
        if prettify_tokens:
            d['left_text'] = format_special_chars(d['left_text'])
            d['right_text'] = format_special_chars(d['right_text'])
    params = {
        'attention': attn_data,
        'default_filter': "0",
        'root_div_id': vis_id,
        'layer': layer,
        'heads': heads,
        'include_layers': include_layers
    }

    # require.js must be imported for Colab or JupyterLab:
    if html_action == 'view':
        display(HTML('<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>'))
        display(HTML(vis_html))
        __location__ = os.path.realpath(
            os.path.join(os.getcwd(), os.path.dirname(__file__)))
        vis_js = open(os.path.join(__location__, 'head_view.js')).read().replace("PYTHON_PARAMS", json.dumps(params))
        display(Javascript(vis_js))

    elif html_action == 'return':
        html1 = HTML('<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>')

        html2 = HTML(vis_html)

        __location__ = os.path.realpath(
            os.path.join(os.getcwd(), os.path.dirname(__file__)))
        vis_js = open(os.path.join(__location__, 'head_view.js')).read().replace("PYTHON_PARAMS", json.dumps(params))
        html3 = Javascript(vis_js)
        script = '\n<script type="text/javascript">\n' + html3.data + '\n</script>\n'

        head_html = HTML(html1.data + html2.data + script)
        return head_html

    else:
        raise ValueError("'html_action' parameter must be 'view' or 'return")
    
def format_attention(attention, layers=None, heads=None):
    if layers:
        attention = [attention[layer_index] for layer_index in layers]
    squeezed = []
    for layer_attention in attention:
        # 1 x num_heads x seq_len x seq_len
        if len(layer_attention.shape) != 4:
            raise ValueError("The attention tensor does not have the correct number of dimensions. Make sure you set "
                            "output_attentions=True when initializing your model.")
        layer_attention = layer_attention.squeeze(0)
        if heads:
            layer_attention = layer_attention[heads]
        squeezed.append(layer_attention)
    # num_layers x num_heads x seq_len x seq_len
    return torch.stack(squeezed)


def num_layers(attention):
    return len(attention)


def num_heads(attention):
    return attention[0][0].size(0)


def format_special_chars(tokens):
    return [t.replace('Ġ', ' ').replace('▁', ' ').replace('</w>', '') for t in tokens]