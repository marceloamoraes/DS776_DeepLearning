import torch
import matplotlib.pyplot as plt
import numpy as np
#from bertviz import head_view
import ipywidgets as widgets
from IPython.display import display, HTML, Javascript
import ipywidgets as widgets
from mpl_toolkits.axes_grid1 import make_axes_locatable

import json
import os
import uuid


def visualize_positional_encodings(pos_encoder, max_len=50, d_model=16, figsize=(8,5)):
    """
    Visualizes the positional encodings generated by the PositionalEncoding module.

    Args:
        pos_encoder (PositionalEncoding): Instance of the PositionalEncoding class.
        max_len (int): Number of positions to visualize.
        d_model (int): Dimensionality of the embedding space to visualize.
    """
    # Generate a dummy input to get positional encodings
    dummy_input = torch.zeros(1, max_len, d_model)
    pos_encodings = pos_encoder(dummy_input)[0].detach().numpy()  # Remove batch dimension
    
    # Create the plot
    plt.figure(figsize=figsize)
    plt.title(f"Positional Encodings (Max Length: {max_len}, Embedding Dim: {d_model})")
    plt.imshow(pos_encodings, aspect='auto', cmap='viridis')
    plt.colorbar(label="Encoding Value")
    plt.xlabel("Embedding Dimension")
    plt.ylabel("Position Index")
    plt.show()

def plot_attention_weights_widget(model, loader, vocab, figsize=(6, 6)):
    def visualize(layer_idx, seq_idx):
        # Get a batch from the loader
        for batch_idx, (inputs, labels) in enumerate(loader):
            break  # Take the first batch

        # Move inputs and model to the correct device
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        inputs = inputs.to(device)
        model_on_device = model.to(device)  # Move model to the same device

        # Forward pass to generate predictions
        outputs = model_on_device(inputs)

        # Extract attention weights
        attention_weights = model_on_device.get_attention_weights()  # List of attention weights from all layers

        # Validate indices
        num_layers = len(attention_weights)
        if layer_idx < 0 or layer_idx >= num_layers:
            print(f"Invalid layer index {layer_idx}. Must be between 0 and {num_layers - 1}.")
            return
        
        if seq_idx < 0 or seq_idx >= inputs.size(0):
            print(f"Invalid sequence index {seq_idx}. Must be between 0 and {inputs.size(0) - 1}.")
            return

        # Select attention weights for the specified sequence and layer
        attention_for_idx = attention_weights[layer_idx][seq_idx].detach().cpu().numpy()  # Shape: (seq_len, seq_len)
        inputs_for_idx = inputs[seq_idx].cpu()
        mask = (inputs_for_idx != 0)
        attention_for_idx = attention_for_idx[:, mask][mask, :]
        inputs_for_idx = inputs_for_idx[mask].numpy()

        # Map token indices to tokens
        idx_to_token = {v: k for k, v in vocab.items()}
        tokens = [idx_to_token.get(i, "[UNK]") for i in inputs_for_idx]

        # Plot the attention weights without gridlines
        fig, ax = plt.subplots(figsize=figsize)
        im = ax.imshow(attention_for_idx, cmap='viridis')

        # Adjust colorbar height to match the plot
        divider = make_axes_locatable(ax)
        cax = divider.append_axes("right", size="5%", pad=0.05)  # Adjust width and padding of colorbar
        cb = fig.colorbar(im, cax=cax)
        cb.ax.tick_params(labelsize=10)  # Adjust tick label size

        # Add titles and labels
        ax.set_title(f"Attention Weights, Layer {layer_idx}", fontsize=12)
        ax.set_xlabel("Keys", fontsize=12)
        ax.set_ylabel("Queries", fontsize=12)
        ax.grid(False)  # Remove gridlines

        # Move xticks to the top
        ax.xaxis.set_ticks_position('top')
        ax.xaxis.set_label_position('top')

        ax.set_xticks(np.arange(attention_for_idx.shape[1]))
        ax.set_xticklabels(tokens, rotation=45, fontsize=12)
        ax.set_yticks(np.arange(attention_for_idx.shape[0]))
        ax.set_yticklabels(tokens, fontsize=12)

        plt.show()

    # Get number of layers and sequences
    num_layers = len(model.get_attention_weights())
    for batch_idx, (inputs, _) in enumerate(loader):
        num_sequences = inputs.size(0)
        break

    # Create widgets
    layer_dropdown = widgets.Dropdown(
        options=list(range(num_layers)),
        value=0,
        description="Layer:",
    )
    seq_slider = widgets.IntSlider(
        value=2,
        min=0,
        max=num_sequences - 1,
        step=1,
        description="Sequence:",
        continuous_update=False,
    )
    seq_input = widgets.BoundedIntText(
        value=2,
        min=0,
        max=num_sequences - 1,
        step=1,
        description="Seq Input:",
    )

    # Link slider and input box
    widgets.jslink((seq_slider, 'value'), (seq_input, 'value'))

    # Interactive display
    interact_ui = widgets.VBox([layer_dropdown, widgets.HBox([seq_slider, seq_input])])
    display(interact_ui)

    # Update the visualization when widget values change
    output = widgets.interactive_output(
        visualize, {'layer_idx': layer_dropdown, 'seq_idx': seq_slider}
    )
    display(output)


def plot_attention_weights(model, loader, vocab, layer_idx, idx, figsize=(6, 6)):
    import matplotlib.pyplot as plt

    # Get a batch from the loader
    for batch_idx, (inputs, labels) in enumerate(loader):
        break  # Take the first batch

    # Move inputs and model to the correct device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    inputs = inputs.to(device)
    model = model.to(device)

    # Forward pass to generate predictions
    outputs = model(inputs)

    # Extract attention weights
    attention_weights = model.get_attention_weights()  # List of attention weights from all layers

    # Select attention weights for the specified sequence and layer
    attention_for_idx = attention_weights[layer_idx][idx].detach().cpu().numpy()  # Shape: (seq_len, seq_len)
    inputs_for_idx = inputs[idx].cpu()
    mask = (inputs_for_idx != 0)
    attention_for_idx = attention_for_idx[:, mask][mask, :]
    inputs_for_idx = inputs_for_idx[mask].numpy()

    # Map token indices to tokens
    idx_to_token = {v: k for k, v in vocab.items()}
    tokens = [idx_to_token.get(i, "[UNK]") for i in inputs_for_idx]

    # Plot the attention weights without gridlines
    plt.figure(figsize=figsize)
    plt.imshow(attention_for_idx, cmap='viridis')
    plt.title(f"Attention Weights, Layer {layer_idx}")
    plt.xlabel("Keys")
    plt.ylabel("Querys")
    plt.colorbar()
    plt.grid(False)  # Remove gridlines

    # Move xticks to the top
    plt.gca().xaxis.set_ticks_position('top')
    plt.gca().xaxis.set_label_position('top')

    plt.xticks(ticks=np.arange(attention_for_idx.shape[1]), labels=tokens, rotation=45)
    plt.yticks(ticks=np.arange(attention_for_idx.shape[0]), labels=tokens)

    plt.show()


def display_attention(idx, model, test_loader, vocab):
    """
    Displays the BertViz head_view for a specific sequence in the batch.

    Args:
        idx (int): Index of the sequence in the batch to visualize.
        model (torch.nn.Module): The trained model.
        test_loader (DataLoader): DataLoader containing the test dataset.
        vocab (dict): Mapping of token indices to tokens.
    """
    # Get a batch from the test loader
    for batch_idx, (inputs, labels) in enumerate(test_loader):
        break  # Take the first batch

    # Move inputs and model to the correct device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    inputs = inputs.to(device)
    model = model.to(device)

    # Forward pass to generate predictions
    outputs = model(inputs)

    # Extract attention weights
    attention_weights = model.get_attention_weights()  # List of attention weights from all layers

    # Select the sequence and reshape attention weights
    inputs_for_idx = inputs[idx].cpu()
    attention_for_idx0 = [
        layer_attention[idx].unsqueeze(0).unsqueeze(0).detach().cpu()
        for layer_attention in attention_weights
    ]

    # Remove padding
    mask = (inputs_for_idx != 0)
    attention_for_idx = [
        layer_attention[:, :, mask][:, :, :, mask]
        for layer_attention in attention_for_idx0
    ]
    inputs_for_idx = inputs_for_idx[mask].numpy()

    # Map token indices to tokens
    idx_to_token = {v: k for k, v in vocab.items()}
    tokens = [idx_to_token.get(i, "[UNK]") for i in inputs_for_idx]

    # Display BertViz head_view
    head_view(
        attention=attention_for_idx,  # Attention weights as a list of tensors
        tokens=tokens,
    )


###########################################################################################################
# the following code is from bertviz - https://github.com/jessevig/bertviz
# but has been tweaked to work current versions of Ipython
# and to use white backgrounds to make visualizations more readable in dark mode
###########################################################################################################

# def head_view(
#         attention=None,
#         tokens=None,
#         sentence_b_start=None,
#         prettify_tokens=True,
#         layer=None,
#         heads=None,
#         encoder_attention=None,
#         decoder_attention=None,
#         cross_attention=None,
#         encoder_tokens=None,
#         decoder_tokens=None,
#         include_layers=None,
#         html_action='view'
# ):
#     """Render head view

#         Args:
#             For self-attention models:
#                 attention: list of ``torch.FloatTensor``(one for each layer) of shape
#                     ``(batch_size(must be 1), num_heads, sequence_length, sequence_length)``
#                 tokens: list of tokens
#                 sentence_b_start: index of first wordpiece in sentence B if input text is sentence pair (optional)
#             For encoder-decoder models:
#                 encoder_attention: list of ``torch.FloatTensor``(one for each layer) of shape
#                     ``(batch_size(must be 1), num_heads, encoder_sequence_length, encoder_sequence_length)``
#                 decoder_attention: list of ``torch.FloatTensor``(one for each layer) of shape
#                     ``(batch_size(must be 1), num_heads, decoder_sequence_length, decoder_sequence_length)``
#                 cross_attention: list of ``torch.FloatTensor``(one for each layer) of shape
#                     ``(batch_size(must be 1), num_heads, decoder_sequence_length, encoder_sequence_length)``
#                 encoder_tokens: list of tokens for encoder input
#                 decoder_tokens: list of tokens for decoder input
#             For all models:
#                 prettify_tokens: indicates whether to remove special characters in wordpieces, e.g. Ġ
#                 layer: index (zero-based) of initial selected layer in visualization. Defaults to layer 0.
#                 heads: Indices (zero-based) of initial selected heads in visualization. Defaults to all heads.
#                 include_layers: Indices (zero-based) of layers to include in visualization. Defaults to all layers.
#                     Note: filtering layers may improve responsiveness of the visualization for long inputs.
#                 html_action: Specifies the action to be performed with the generated HTML object
#                     - 'view' (default): Displays the generated HTML representation as a notebook cell output
#                     - 'return' : Returns an HTML object containing the generated view for further processing or custom visualization
#     """

#     attn_data = []
#     if attention is not None:
#         if tokens is None:
#             raise ValueError("'tokens' is required")
#         if encoder_attention is not None or decoder_attention is not None or cross_attention is not None \
#                 or encoder_tokens is not None or decoder_tokens is not None:
#             raise ValueError("If you specify 'attention' you may not specify any encoder-decoder arguments. This"
#                              " argument is only for self-attention models.")
#         if include_layers is None:
#             include_layers = list(range(num_layers(attention)))
#         attention = format_attention(attention, include_layers)
#         if sentence_b_start is None:
#             attn_data.append(
#                 {
#                     'name': None,
#                     'attn': attention.tolist(),
#                     'left_text': tokens,
#                     'right_text': tokens
#                 }
#             )
#         else:
#             slice_a = slice(0, sentence_b_start)  # Positions corresponding to sentence A in input
#             slice_b = slice(sentence_b_start, len(tokens))  # Position corresponding to sentence B in input
#             attn_data.append(
#                 {
#                     'name': 'All',
#                     'attn': attention.tolist(),
#                     'left_text': tokens,
#                     'right_text': tokens
#                 }
#             )
#             attn_data.append(
#                 {
#                     'name': 'Sentence A -> Sentence A',
#                     'attn': attention[:, :, slice_a, slice_a].tolist(),
#                     'left_text': tokens[slice_a],
#                     'right_text': tokens[slice_a]
#                 }
#             )
#             attn_data.append(
#                 {
#                     'name': 'Sentence B -> Sentence B',
#                     'attn': attention[:, :, slice_b, slice_b].tolist(),
#                     'left_text': tokens[slice_b],
#                     'right_text': tokens[slice_b]
#                 }
#             )
#             attn_data.append(
#                 {
#                     'name': 'Sentence A -> Sentence B',
#                     'attn': attention[:, :, slice_a, slice_b].tolist(),
#                     'left_text': tokens[slice_a],
#                     'right_text': tokens[slice_b]
#                 }
#             )
#             attn_data.append(
#                 {
#                     'name': 'Sentence B -> Sentence A',
#                     'attn': attention[:, :, slice_b, slice_a].tolist(),
#                     'left_text': tokens[slice_b],
#                     'right_text': tokens[slice_a]
#                 }
#             )
#     elif encoder_attention is not None or decoder_attention is not None or cross_attention is not None:
#         if encoder_attention is not None:
#             if encoder_tokens is None:
#                 raise ValueError("'encoder_tokens' required if 'encoder_attention' is not None")
#             if include_layers is None:
#                 include_layers = list(range(num_layers(encoder_attention)))
#             encoder_attention = format_attention(encoder_attention, include_layers)
#             attn_data.append(
#                 {
#                     'name': 'Encoder',
#                     'attn': encoder_attention.tolist(),
#                     'left_text': encoder_tokens,
#                     'right_text': encoder_tokens
#                 }
#             )
#         if decoder_attention is not None:
#             if decoder_tokens is None:
#                 raise ValueError("'decoder_tokens' required if 'decoder_attention' is not None")
#             if include_layers is None:
#                 include_layers = list(range(num_layers(decoder_attention)))
#             decoder_attention = format_attention(decoder_attention, include_layers)
#             attn_data.append(
#                 {
#                     'name': 'Decoder',
#                     'attn': decoder_attention.tolist(),
#                     'left_text': decoder_tokens,
#                     'right_text': decoder_tokens
#                 }
#             )
#         if cross_attention is not None:
#             if encoder_tokens is None:
#                 raise ValueError("'encoder_tokens' required if 'cross_attention' is not None")
#             if decoder_tokens is None:
#                 raise ValueError("'decoder_tokens' required if 'cross_attention' is not None")
#             if include_layers is None:
#                 include_layers = list(range(num_layers(cross_attention)))
#             cross_attention = format_attention(cross_attention, include_layers)
#             attn_data.append(
#                 {
#                     'name': 'Cross',
#                     'attn': cross_attention.tolist(),
#                     'left_text': decoder_tokens,
#                     'right_text': encoder_tokens
#                 }
#             )
#     else:
#         raise ValueError("You must specify at least one attention argument.")

#     if layer is not None and layer not in include_layers:
#         raise ValueError(f"Layer {layer} is not in include_layers: {include_layers}")

#     # Generate unique div id to enable multiple visualizations in one notebook
#     vis_id = 'bertviz-%s'%(uuid.uuid4().hex)

#     # Compose html
#     if len(attn_data) > 1:
#         options = '\n'.join(
#             f'<option value="{i}">{attn_data[i]["name"]}</option>'
#             for i, d in enumerate(attn_data)
#         )
#         select_html = f'Attention: <select id="filter">{options}</select>'
#     else:
#         select_html = ""
#     vis_html = f"""      
#         <div id="{vis_id}" style="font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;">
#             <span style="user-select:none">
#                 Layer: <select id="layer"></select>
#                 {select_html}
#             </span>
#             <div id='vis'></div>
#         </div>
#     """

#     for d in attn_data:
#         attn_seq_len_left = len(d['attn'][0][0])
#         if attn_seq_len_left != len(d['left_text']):
#             raise ValueError(
#                 f"Attention has {attn_seq_len_left} positions, while number of tokens is {len(d['left_text'])} "
#                 f"for tokens: {' '.join(d['left_text'])}"
#             )
#         attn_seq_len_right = len(d['attn'][0][0][0])
#         if attn_seq_len_right != len(d['right_text']):
#             raise ValueError(
#                 f"Attention has {attn_seq_len_right} positions, while number of tokens is {len(d['right_text'])} "
#                 f"for tokens: {' '.join(d['right_text'])}"
#             )
#         if prettify_tokens:
#             d['left_text'] = format_special_chars(d['left_text'])
#             d['right_text'] = format_special_chars(d['right_text'])
#     params = {
#         'attention': attn_data,
#         'default_filter': "0",
#         'root_div_id': vis_id,
#         'layer': layer,
#         'heads': heads,
#         'include_layers': include_layers
#     }

#     # require.js must be imported for Colab or JupyterLab:
#     if html_action == 'view':
#         display(HTML('<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>'))
#         display(HTML(vis_html))
#         __location__ = os.path.realpath(
#             os.path.join(os.getcwd(), os.path.dirname(__file__)))
#         vis_js = open(os.path.join(__location__, 'head_view.js')).read().replace("PYTHON_PARAMS", json.dumps(params))
#         display(Javascript(vis_js))

#     elif html_action == 'return':
#         html1 = HTML('<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>')

#         html2 = HTML(vis_html)

#         __location__ = os.path.realpath(
#             os.path.join(os.getcwd(), os.path.dirname(__file__)))
#         vis_js = open(os.path.join(__location__, 'head_view.js')).read().replace("PYTHON_PARAMS", json.dumps(params))
#         html3 = Javascript(vis_js)
#         script = '\n<script type="text/javascript">\n' + html3.data + '\n</script>\n'

#         head_html = HTML(html1.data + html2.data + script)
#         return head_html

#     else:
#         raise ValueError("'html_action' parameter must be 'view' or 'return")

def head_view(
        attention=None,
        tokens=None,
        sentence_b_start=None,
        prettify_tokens=True,
        layer=None,
        heads=None,
        encoder_attention=None,
        decoder_attention=None,
        cross_attention=None,
        encoder_tokens=None,
        decoder_tokens=None,
        include_layers=None,
        html_action='view'
):
    """Render head view"""

    attn_data = []
    if attention is not None:
        if tokens is None:
            raise ValueError("'tokens' is required")
        if encoder_attention is not None or decoder_attention is not None or cross_attention is not None \
                or encoder_tokens is not None or decoder_tokens is not None:
            raise ValueError("If you specify 'attention' you may not specify any encoder-decoder arguments.")
        if include_layers is None:
            include_layers = list(range(num_layers(attention)))
        attention = format_attention(attention, include_layers)
        if sentence_b_start is None:
            attn_data.append(
                {
                    'name': None,
                    'attn': attention.tolist(),
                    'left_text': tokens,
                    'right_text': tokens
                }
            )
        else:
            slice_a = slice(0, sentence_b_start)
            slice_b = slice(sentence_b_start, len(tokens))
            attn_data.append(
                {
                    'name': 'All',
                    'attn': attention.tolist(),
                    'left_text': tokens,
                    'right_text': tokens
                }
            )
            attn_data.append(
                {
                    'name': 'Sentence A -> Sentence A',
                    'attn': attention[:, :, slice_a, slice_a].tolist(),
                    'left_text': tokens[slice_a],
                    'right_text': tokens[slice_a]
                }
            )
            attn_data.append(
                {
                    'name': 'Sentence B -> Sentence B',
                    'attn': attention[:, :, slice_b, slice_b].tolist(),
                    'left_text': tokens[slice_b],
                    'right_text': tokens[slice_b]
                }
            )
            attn_data.append(
                {
                    'name': 'Sentence A -> Sentence B',
                    'attn': attention[:, :, slice_a, slice_b].tolist(),
                    'left_text': tokens[slice_a],
                    'right_text': tokens[slice_b]
                }
            )
            attn_data.append(
                {
                    'name': 'Sentence B -> Sentence A',
                    'attn': attention[:, :, slice_b, slice_a].tolist(),
                    'left_text': tokens[slice_b],
                    'right_text': tokens[slice_a]
                }
            )
    else:
        raise ValueError("You must specify at least one attention argument.")

    vis_id = 'bertviz-%s'%(uuid.uuid4().hex)
    vis_html = f"""
        <div id="{vis_id}" style="font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;">
            <span style="user-select:none">
                Layer: <select id="layer"></select>
            </span>
            <div id='vis'></div>
        </div>
    """

    params = {
        'attention': attn_data,
        'default_filter': "0",
        'root_div_id': vis_id,
        'layer': layer,
        'heads': heads,
        'include_layers': include_layers
    }

    # Injecting JavaScript directly to avoid CoCalc's restrictions on require.js
    injected_js = f"""
    <script>
        if (typeof require === "undefined") {{
            window.require = {{config: function(){{}}}};
        }}
        console.log("Injected dummy require to avoid require.config error in CoCalc.");
    </script>
    """

    if html_action == 'view':
        display(HTML(injected_js))
        display(HTML(vis_html))

        __location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))
        with open(os.path.join(__location__, 'head_view.js')) as f:
            vis_js = f.read().replace("PYTHON_PARAMS", json.dumps(params))

        display(Javascript(vis_js))

    elif html_action == 'return':
        html1 = HTML(injected_js)
        html2 = HTML(vis_html)

        __location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))
        with open(os.path.join(__location__, 'head_view.js')) as f:
            vis_js = f.read().replace("PYTHON_PARAMS", json.dumps(params))
        html3 = Javascript(vis_js)
        script = '\n<script type="text/javascript">\n' + html3.data + '\n</script>\n'

        head_html = HTML(html1.data + html2.data + script)
        return head_html
    else:
        raise ValueError("'html_action' parameter must be 'view' or 'return'")


def format_attention(attention, layers=None, heads=None):
    if layers:
        attention = [attention[layer_index] for layer_index in layers]
    squeezed = []
    for layer_attention in attention:
        # 1 x num_heads x seq_len x seq_len
        if len(layer_attention.shape) != 4:
            raise ValueError("The attention tensor does not have the correct number of dimensions. Make sure you set "
                            "output_attentions=True when initializing your model.")
        layer_attention = layer_attention.squeeze(0)
        if heads:
            layer_attention = layer_attention[heads]
        squeezed.append(layer_attention)
    # num_layers x num_heads x seq_len x seq_len
    return torch.stack(squeezed)


def num_layers(attention):
    return len(attention)


def num_heads(attention):
    return attention[0][0].size(0)


def format_special_chars(tokens):
    return [t.replace('Ġ', ' ').replace('▁', ' ').replace('</w>', '') for t in tokens]