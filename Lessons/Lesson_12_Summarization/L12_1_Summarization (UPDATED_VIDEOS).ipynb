{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe8c867",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# There were some tweaks this week, so update your course package\n",
    "# uncomment the line below to install the latest version\n",
    "#!pip install ../Course_Tools/introdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may wish to manage your diskspace before starting the lesson.  You'll be downloading several new models and datasets and you don't want to run out of diskspace.\n",
    "\n",
    "Deleting both the the `cs_workspace` and the `~/.cache/huggingface` directories before running the lesson code or working on the homework will remove all your previous models and datasets.  It won't affect any of your Lesson or Homework notebooks.\n",
    "\n",
    "You can uncomment and run the following cell on your compute server (it won't hurt to run it on the home server, but cs_workspace doesn't exist there)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# be careful with rm -rf, it will delete everything in the path you give it\n",
    "# !rm -rf ~/cs_workspace\n",
    "# !rm -rf ~/.cache/huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2860545",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODELS_PATH=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\models\n",
      "DATA_PATH=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\data\n",
      "CACHE_PATH=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\n",
      "TORCH_HOME=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\n",
      "HF_HOME=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\n",
      "HF_DATASETS_CACHE=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in to Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "from introdl.utils import config_paths_keys, wrap_print_text\n",
    "\n",
    "# call cnonfig_paths_keys() before importing hugging face packages\n",
    "paths = config_paths_keys()\n",
    "MODELS_PATH = paths['MODELS_PATH'] # where to store your trained models\n",
    "DATA_PATH = paths['DATA_PATH'] # where to store downloaded data\n",
    "CACHE_PATH = paths['CACHE_PATH'] # where to store pretrained models\n",
    "\n",
    "print = wrap_print_text(print, width = 100)\n",
    "\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "from nltk import sent_tokenize, download\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    Trainer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM, \n",
    "    AutoTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    ")\n",
    "import warnings\n",
    "\n",
    "# Download Punkt tokenizer for sentence splitting (used by ROUGE-Lsum)\n",
    "download(\"punkt\", quiet=True)\n",
    "download(\"punkt_tab\", quiet=True)\n",
    "\n",
    "# Load evaluation metrics\n",
    "rouge = load(\"rouge\")\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "# Suppress warnings from the transformers library\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "from helpers import compute_all_metrics, print_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d122fba3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## **Section 1 - Introduction to Text Summarization**\n",
    "\n",
    "Text summarization is the task of generating a concise and coherent summary that captures the key information from a longer piece of text. As the volume of textual data continues to grow‚Äîfrom news articles and research papers to customer reviews and meeting transcripts‚Äîsummarization plays an increasingly important role in helping people process and understand information efficiently.\n",
    "\n",
    "There are two main approaches to summarization:\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Extractive Summarization\n",
    "\n",
    "Extractive summarization works by identifying and selecting the most important sentences or phrases from the original text. The summary is formed by piecing together these extracted parts without modifying the original wording.\n",
    "\n",
    "**Example**:  \n",
    "**Original text**:  \n",
    "> The mayor held a press conference today announcing new environmental initiatives to reduce air pollution in the city. The new policies include increased funding for public transportation and stricter emissions regulations for factories.  \n",
    "\n",
    "**Extractive summary**:  \n",
    "> The mayor announced new environmental initiatives. The policies include funding for public transportation and stricter emissions regulations.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Abstractive Summarization\n",
    "\n",
    "Abstractive summarization goes a step further by generating new sentences that may not appear in the original text. It interprets and paraphrases the key ideas using natural language generation, similar to how a human might summarize.\n",
    "\n",
    "**Abstractive summary**:  \n",
    "> The mayor introduced new plans to cut air pollution through transit funding and factory regulations.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Why This Lesson Focuses on Abstractive Summarization\n",
    "\n",
    "While extractive summarization is easier to implement and often performs well on factual documents, it has limitations:\n",
    "- It may copy irrelevant or redundant text.\n",
    "- It struggles to rephrase or synthesize ideas.\n",
    "\n",
    "Abstractive summarization, powered by deep learning models like BART, T5, PEGASUS and LLMs, enables:\n",
    "- More fluent and human-like summaries.\n",
    "- Better generalization across different domains.\n",
    "- Control over the tone and length of the summary.\n",
    "\n",
    "Because abstractive models are more aligned with how people summarize information‚Äîand because they demonstrate the strengths of modern language generation‚Äîwe‚Äôll focus on **abstractive summarization** in this lesson.\n",
    "\n",
    "In the next section we discuss the major models, inlcuding LLMs, used for summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9cc224",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## **Section 2 - Overview of Popular Models for Abstractive Summarization**\n",
    "\n",
    "Summarization tasks can be tackled using **specialized encoder-decoder models** like **BART, T5, and PEGASUS**, or **general-purpose decoder-only models (LLMs)** like **GPT-4o and LLaMA**.  Our textbook already introduced the models, so we won't go into detail here, rather we'll discuss the strengths and weaknesses of each approach.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Strengths of Specialized Summarization Models (BART, T5, PEGASUS)**\n",
    "1. **Architectural Efficiency**\n",
    "   - Encoder-decoder models process the entire input once with the encoder before generating the summary, making them *computationally efficient* for summarization.\n",
    "   - In contrast, decoder-only models must repeatedly attend to the entire input during generation, which is particularly costly for long inputs.\n",
    "\n",
    "2. **Tailored Training Objectives**\n",
    "   - These models are pre-trained specifically for text-to-text tasks.\n",
    "     - **BART:** Trained as a denoising autoencoder, making it robust to noisy or incomplete input.\n",
    "     - **T5:** Uses a ‚Äútext-to-text‚Äù framework, making it versatile across various NLP tasks, including summarization.\n",
    "     - **PEGASUS:** Pre-trained to generate summaries by masking entire sentences during training, directly optimizing for abstractive summarization.\n",
    "\n",
    "3. **Alignment with Summarization Tasks**\n",
    "   - Fine-tuning on summarization datasets (e.g., CNN/Daily Mail, XSum) leads to **high-quality summaries** that are concise and relevant.\n",
    "   - Performance on benchmarks often surpasses general-purpose LLMs.\n",
    "\n",
    "4. **Better Control over Output**\n",
    "   - Easier to enforce structure, conciseness, or adherence to specific formatting requirements.\n",
    "   - Less prone to **hallucinations** or verbose outputs compared to general-purpose LLMs.\n",
    "\n",
    "5. **Domain-Specific Optimization**\n",
    "   - Fine-tuning encoder-decoder models on specialized datasets (e.g., medical or legal texts) produces highly accurate summaries with relevant terminology and structure.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå **Weaknesses of Specialized Summarization Models**\n",
    "1. **Limited Generalization**\n",
    "   - Models like BART, T5, and PEGASUS require fine-tuning for specific summarization tasks.\n",
    "   - Struggle with novel domains or tasks without retraining.\n",
    "\n",
    "2. **Less Effective at Zero-Shot Summarization**\n",
    "   - General-purpose LLMs can perform reasonably well on summarization tasks without fine-tuning, which is challenging for encoder-decoder models.\n",
    "\n",
    "3. **Inflexibility**\n",
    "   - Encoder-decoder models are often designed for fixed inputs and outputs, making them less adaptable to creative or open-ended summarization tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Strengths of LLMs for Summarization**\n",
    "1. **Generalization Across Tasks**\n",
    "   - Capable of summarization **without fine-tuning** through prompt engineering (e.g., ‚ÄúSummarize the following text...‚Äù).\n",
    "   - Strong performance across various domains with minimal adjustments.\n",
    "\n",
    "2. **Few-Shot & Zero-Shot Learning**\n",
    "   - Easily adaptable to new domains or styles through *in-context learning* (providing examples within the prompt).\n",
    "\n",
    "3. **Versatility**\n",
    "   - Handles a wide range of tasks beyond summarization, making them highly flexible for mixed-use applications.\n",
    "   - Can switch between extractive, abstractive, or creative summarization depending on the prompt.\n",
    "\n",
    "4. **Ease of Use**\n",
    "   - No need for specialized training or fine-tuning, making them immediately usable for various summarization tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå **Weaknesses of LLMs for Summarization**\n",
    "1. **Inefficiency for Long Texts**\n",
    "   - Decoder-only models process the entire input text during every generation step, resulting in high computational costs for long documents.\n",
    "\n",
    "2. **Prone to Hallucination**\n",
    "   - Without fine-tuning or careful prompting, LLMs can generate irrelevant or incorrect information, particularly for factual summarization tasks.\n",
    "\n",
    "3. **Less Structured Output**\n",
    "   - Outputs may be verbose or off-topic unless the prompt is carefully designed to enforce structure and conciseness.\n",
    "\n",
    "4. **Lack of Task-Specific Optimization**\n",
    "   - General-purpose LLMs may underperform compared to fine-tuned encoder-decoder models on specific summarization datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### **State-of-the-art Models**\n",
    "\n",
    "As of April 2025, BART, T5, and PEGASUS remain state-of-the-art for many summarization tasks, especially when compute and data are limited, you need efficient fine-tuned models for specific domains, or you're doing sequence-to-sequence tasks where controllability and reproducibility matter.\n",
    "\n",
    "### **What‚Äôs new for summarization tasks?**\n",
    "\n",
    "More recent models like:\n",
    "\n",
    "- **FLAN-T5** (instruction-tuned T5)  \n",
    "- **UL2** (Universal Language Learning)  \n",
    "- **PaLM**, **Gemma**, **Mixtral**, and **LLaMA** family models (especially when fine-tuned)  \n",
    "- **Longformer Encoder-Decoder (LED)** or **LongT5** for long documents  \n",
    "- **LLMs** (ChatGPT, Claude, Llama etc.) for few-shot or zero-shot summarization  \n",
    "- **HERA** (Hallucination Evaluation and Rewriting Architecture) for post-editing summaries to reduce factual errors  \n",
    "\n",
    "...can outperform **BART**, **PEGASUS**, and **T5** in terms of quality when used with strong prompting or fine-tuning. However, these models:\n",
    "\n",
    "- Are much larger  \n",
    "- Often require API access or substantial compute  \n",
    "- May not be easily reproducible or tunable for every use case  \n",
    "- (In the case of **HERA**) introduce additional stages to the pipeline, increasing complexity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf896ea",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Section 3 - Metrics for Evaluating Generated Text\n",
    "\n",
    "The book discusses two metrics, ROUGE and BLEU.  We introduced BERTScore last week in the text-generation lesson.  We'll use all three of these to evaluate our summarization results.  These metrics also apply to any text-generation task in which a reference text is available.  A fourth, and recently developed metric, is BARTScore and is specific to summarization tasks.  We'll introduce it here, but won't demonstrate it because it's a bit difficult to set up and not widely used yet.\n",
    "\n",
    "Evaluating text generation is challenging because there are **many valid summaries** for a single input, and traditional metrics like ROUGE or BLEU only compare surface-level word overlaps. They often **miss meaning**, **penalize paraphrasing**, and **fail to detect factual errors or hallucinations**. Newer metrics like BERTScore and BARTScore help, but no single metric fully captures quality, faithfulness, and readability.\n",
    "\n",
    "\n",
    "Use an AI here to get more details as needed about n-gram, skip-bigram, etc.\n",
    "\n",
    "### **Brief Introductions to the Metrics**\n",
    "\n",
    "There are many others, but we'll focus on these:\n",
    "\n",
    "1. **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\n",
    "   - Measures **n-gram, subsequence, or skip-bigram overlap** between a candidate and reference text.\n",
    "   - Commonly used for **extractive summarization** but also applied to **abstractive summarization**.\n",
    "   - Variants: **ROUGE-N (e.g., ROUGE-1, ROUGE-2), ROUGE-L (Longest Common Subsequence), ROUGE-S (Skip-bigrams)**.\n",
    "\n",
    "2. **BLEU (Bilingual Evaluation Understudy)**\n",
    "   - Measures **n-gram overlap** between a candidate text and one or more reference texts.\n",
    "   - Originally designed for **machine translation**, but adapted for **summarization**.\n",
    "   - Uses a **brevity penalty** to avoid favoring overly short outputs.\n",
    "   - Often reported with **1-gram to 4-gram precision scores**.\n",
    "\n",
    "3. **BERTScore**\n",
    "   - Measures **semantic similarity** between candidate and reference texts using **contextual embeddings** from models like BERT.\n",
    "   - Matches tokens based on their **cosine similarity in embedding space**.\n",
    "   - Effective for **abstractive summarization**, especially when paraphrasing is present.\n",
    "\n",
    "4. **BARTScore**\n",
    "   - Uses **pretrained language models (e.g., BART)** to estimate the **likelihood of a summary given the source text** and vice versa.\n",
    "   - Evaluates summaries using **bidirectional scoring**: Coverage (`P(summary | source)`) and Faithfulness (`P(source | summary)`).\n",
    "   - Particularly useful for evaluating **fluency, coherence, and factual consistency**.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä **Comparison Table**\n",
    "\n",
    "| **Metric**    | **Use-Cases**                        | **Strengths**                                                                                          | **Weaknesses**                                                                                    |\n",
    "|---------------|--------------------------------------|--------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|\n",
    "| **ROUGE**     | Extractive summarization, Some Abstractive Summarization | - Easy to compute and interpret. <br> - Works well for extractive tasks. <br> - Multiple variants for different needs. | - Ignores paraphrasing. <br> - Surface-level comparison. <br> - Sensitive to minor wording changes. |\n",
    "| **BLEU**      | Machine Translation, Summarization   | - Simple and fast to compute. <br> - Precision-oriented. <br> - Useful for extractive and some abstractive tasks. | - Penalizes paraphrasing. <br> - Limited to local n-gram matching. <br> - Ignores semantic similarity. |\n",
    "| **BERTScore** | Abstractive Summarization, Paraphrasing | - Captures semantic similarity well. <br> - Robust to paraphrasing and rephrasing. <br> - Works well for abstractive summaries. | - Ignores coherence and sentence structure. <br> - Dependent on quality of pretrained embeddings. |\n",
    "| **BARTScore** | Abstractive Summarization, Coherence Evaluation, Faithfulness Check | - Measures fluency, coherence, and factual consistency. <br> - Can evaluate coverage and faithfulness. <br> - Useful for abstractive summarization. | - Sensitive to the training domain. <br> - Can prioritize fluency over factual accuracy. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7013ab61",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### **Demonstrating the Metrics with Examples**\n",
    "\n",
    "#### L12_1_Metrics_Examples Video\n",
    "\n",
    "<iframe \n",
    "    src=\"https://media.uwex.edu/content/ds/ds776/ds776_l12_1_metrics_example/\" \n",
    "    width=\"800\" \n",
    "    height=\"450\" \n",
    "    style=\"border: 5px solid cyan;\"  \n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "<br>\n",
    "<a href=\"https://media.uwex.edu/content/ds/ds776/ds776_l12_1_metrics_example/\" target=\"_blank\">Open UWEX version of video in new tab</a>\n",
    "<br>\n",
    "<a href=\"https://share.descript.com/view/M4axFkxWp6s\" target=\"_blank\">Open Descript version of video in new tab</a>\n",
    "\n",
    "\n",
    "To get a better feel for what these metrics do, let's compute the metrics for a series of sentence pairs. In each case we have `prediction` and `reference`.  You can think of the prediction as a predicted summary and reference as the ground-truth summary or you can simply think of them as two texts for which we want to determine similarity. We'll use `compute_all_metrics` and `print_metrics` from `helpers.py`.  In the video for this section we talk a bit about that code.\n",
    "\n",
    "#### ‚ú≥Ô∏è Example 1. *Exact lexical match ‚Äî all metrics should score well*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "776d1f59",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu: 100.00\n",
      "rouge1: 100.00\n",
      "rouge2: 100.00\n",
      "rougeL: 100.00\n",
      "rougeLsum: 100.00\n",
      "bertscore_f1: 100.00\n"
     ]
    }
   ],
   "source": [
    "reference = \"The cat sat on the mat.\"\n",
    "prediction = \"The cat sat on the mat.\"\n",
    "metrics = compute_all_metrics(prediction, reference)\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6c9fc1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "All the metrics are perfect since there's perfect token overlap and order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df2ca7a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "\n",
    "#### ‚ú≥Ô∏è 2. *Minor synonym substitution ‚Äî shows BERTScore strength*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3820331a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu: 0.00\n",
      "rouge1: 50.00\n",
      "rouge2: 20.00\n",
      "rougeL: 50.00\n",
      "rougeLsum: 50.00\n",
      "bertscore_f1: 95.97\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reference = \"The cat sat on the mat.\"\n",
    "prediction = \"The feline rested on the rug.\"\n",
    "metrics = compute_all_metrics(prediction, reference)\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5748db",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " BLEU and ROUGE struggle due to lack of exact word overlap.  BERTScore understands semantic similarity because the synonyms have produce embeddings that are close toether.\n",
    "\n",
    "#### ‚ú≥Ô∏è 3. *Paraphrase with reordering ‚Äî ROUGE handles this better than BLEU*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9238277",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu: 0.00\n",
      "rouge1: 84.21\n",
      "rouge2: 58.82\n",
      "rougeL: 52.63\n",
      "rougeLsum: 52.63\n",
      "bertscore_f1: 95.91\n"
     ]
    }
   ],
   "source": [
    "reference = \"The cat sat on the mat in the afternoon.\"\n",
    "prediction = \"In the afternoon, the cat was sitting on the mat.\"\n",
    "metrics = compute_all_metrics(prediction, reference)\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bde15f8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "BLEU scores low due to strict n-gram ordering.  ROUGE scores higher due to flexible matching via longest common subsequences.  BERTScore is very good and captures paraphrased meaning.\n",
    "\n",
    "#### ‚ú≥Ô∏è 4. *Prediction is shorter but contains key ideas ‚Äî BLEU drops, ROUGE & BERTScore still decent*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63dce121",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu: 0.00\n",
      "rouge1: 44.44\n",
      "rouge2: 25.00\n",
      "rougeL: 33.33\n",
      "rougeLsum: 33.33\n",
      "bertscore_f1: 92.60\n"
     ]
    }
   ],
   "source": [
    "reference = \"The government announced a stimulus package to support the economy during the recession.\"\n",
    "prediction = \"A stimulus package was announced.\"\n",
    "metrics = compute_all_metrics(prediction, reference)\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9418c30",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The BLEU metric has a harsh penalty for short output.  The ROGUE scores show that key content words are captured.  The BERTScore remains high because it recognizes core meaning.\n",
    "\n",
    "#### ‚ú≥Ô∏è 5. *Prediction uses entirely different vocabulary ‚Äî only BERTScore succeeds*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ffc3412",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu: 0.00\n",
      "rouge1: 13.33\n",
      "rouge2: 0.00\n",
      "rougeL: 13.33\n",
      "rougeLsum: 13.33\n",
      "bertscore_f1: 93.44\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reference = \"The plane crashed due to engine failure.\"\n",
    "prediction = \"The aircraft accident was caused by mechanical problems.\"\n",
    "metrics = compute_all_metrics(prediction, reference)\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e807ca",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "BLEU and ROUGE are both low because there's no surface overlap in the texts while BERTScore is high because it understands semantic similarity.  This example highlights why modern metrics are needed.\n",
    "\n",
    "#### ‚ú≥Ô∏è 6. *Copying style but not content ‚Äî ROUGE and BERTScore may be fooled*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd08e98f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu: 0.00\n",
      "rouge1: 45.45\n",
      "rouge2: 20.00\n",
      "rougeL: 45.45\n",
      "rougeLsum: 45.45\n",
      "bertscore_f1: 90.87\n"
     ]
    }
   ],
   "source": [
    "reference = \"The court ruled in favor of the defendant.\"\n",
    "prediction = \"The judge made a ruling in the case of the cat and the fiddle.\"\n",
    "metrics = compute_all_metrics(prediction, reference)\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e25174",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "BLEU scores low because there is almost no n-gram overlap.  ROUGE scored moderately due to some word/form overlap.  BERTScore was high because many of the tokens (words) have similar embeddings.  This is a **failure case** showing the limitations of automatic metrics.  The texts have similar form and related words, but their factual meanings are quite different.  \n",
    "\n",
    "\n",
    "#### ‚ú≥Ô∏è 7. *Word salad with shared vocabulary ‚Äî BERTScore gives falsely high score*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c377178a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu: 0.00\n",
      "rouge1: 94.12\n",
      "rouge2: 0.00\n",
      "rougeL: 47.06\n",
      "rougeLsum: 47.06\n",
      "bertscore_f1: 89.47\n"
     ]
    }
   ],
   "source": [
    "reference = \"The stock market crashed due to unexpected inflation news.\"\n",
    "prediction = \"Inflation stock news market due crashed the unexpected.\"\n",
    "metrics = compute_all_metrics(prediction, reference)\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fb6530",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "BLEU is low because there's little n-gram overlap.  However, ROUGE and BERTScore both get fooled.  This is another **failure case.** Let's look at the ROUGE scores in detail because it will help us better understand how they work:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309e5cdc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "| Metric     | Score | Why? |\n",
    "|------------|-------|------|\n",
    "| **ROUGE-1** | 94.12 | This measures **unigram (single word)** overlap. The prediction contains nearly all the same words as the reference, just in a jumbled order. So the unigram match is very high. |\n",
    "| **ROUGE-2** | 0.00  | This measures **bigram (2-word sequence)** overlap. Since the word order is completely scrambled, there are **no matching bigrams** ‚Äî hence a zero score. |\n",
    "| **ROUGE-L** | 47.06 | ROUGE-L is based on the **Longest Common Subsequence (LCS)**. Some words appear in the same order (e.g., `\"stock martket crashed the\"`), but the rest are rearranged. So you get a partial score. |\n",
    "| **ROUGE-Lsum** | 47.06 | Same as ROUGE-L here, but adjusted for sentence-level evaluation with potential sentence boundaries. In your case, there's only one sentence, so it's equivalent. |\n",
    "\n",
    "ROUGE-1 alone can be misleading ‚Äî it gives high scores even if the summary is nonsense, as long as the words are right.\n",
    "\n",
    "ROUGE-2 and ROUGE-L help mitigate this by adding sensitivity to word order and structure ‚Äî but they still don't fully capture meaning.\n",
    "\n",
    "BERTScore is high because the token embeddings match even though the means do not.  BERTScore is better at capturing meaning but it isn't perfect.\n",
    "\n",
    "The bottom line is that **no metric is perfect**, and **human review is always necessary**, especially for evaluating meaning and fluency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203d35ae",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## **Section 4 - Fine-Tuning and Evaluating a Summarization Model**\n",
    "\n",
    "#### L12_Fine_Tuning_Summarization Video\n",
    "\n",
    "<iframe \n",
    "    src=\"https://media.uwex.edu/content/ds/ds776/ds776_l12_fine_tuning_summarization/\" \n",
    "    width=\"800\" \n",
    "    height=\"450\" \n",
    "    style=\"border: 5px solid cyan;\"  \n",
    "    allowfullscreen>\n",
    "</iframe>\n",
    "<br>\n",
    "<a href=\"https://media.uwex.edu/content/ds/ds776/ds776_l12_fine_tuning_summarization/\" target=\"_blank\">Open UWEX version of video in new tab</a>\n",
    "<br>\n",
    "<a href=\"https://share.descript.com/view/MYc2Y04Y2LH\" target=\"_blank\">Open Descript version of video in new tab</a>\n",
    "\n",
    "\n",
    "In this section, we'll start with a BART model that has already been fine-tuned for a summarization task and demonstrate how to fine-tune it for a different task. Specifically, we'll begin with `facebook/bart-large-cnn`, a model fine-tuned to summarize news articles using the **CNN/DailyMail dataset**, and adapt it to the **XSum dataset** for generating highly abstractive summaries. \n",
    "\n",
    "The **CNN/DailyMail dataset**, used to fine-tune `facebook/bart-large-cnn`, consists of news articles paired with multi-sentence summaries that are often extractive in nature. In contrast, the **XSum dataset** is a collection of BBC articles, each paired with a single-sentence summary that captures the essence of the article. The **XSum dataset** is widely used for training and evaluating abstractive summarization models due to its focus on generating concise and highly abstractive summaries.\n",
    "\n",
    "This process will demonstrate transfer learning for text summarization.  \n",
    "\n",
    "### ‚ú≥Ô∏è **Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9354ee39",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here we'll instantiate the Hugging Face classes we'll be using.  They are:\n",
    "\n",
    "- **`AutoModelForSeq2SeqLM`**:\n",
    "  - A class for loading pre-trained sequence-to-sequence models (e.g., BART, T5).\n",
    "  - Specifically designed for tasks like summarization, translation, and text generation.\n",
    "  - Includes the `generate()` method for text generation.\n",
    "\n",
    "- **`AutoTokenizer`**:\n",
    "  - A class for loading the appropriate tokenizer for a given model.\n",
    "  - Handles tokenization (converting text to token IDs) and detokenization (converting token IDs back to text).\n",
    "  - Ensures compatibility with the pre-trained model being used.\n",
    "\n",
    "- **`DataCollatorForSeq2Seq`**:\n",
    "  - A class for preparing batches of data for sequence-to-sequence models.\n",
    "  - Handles padding and formatting of input and output sequences to ensure they are compatible with the model.\n",
    "  - Useful for training and evaluation pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd70e2ed",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4e914e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "\n",
    "### ‚ú≥Ô∏è **Load and Preview Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb51153",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We're going to choose small subsets for training and validation to demonsrate how fine-tuning works and performs, but in a production setting we'd use all of the training data that we can or at least as much as we can afford to use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5660032",
   "metadata": {
    "cocalc": {
     "outputs": {
      "2": {
       "name": "input",
       "opts": {
        "password": false,
        "prompt": "The repository for xsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/xsum.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N] "
       },
       "output_type": "stream",
       "value": "y"
      }
     }
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Article 1:\n",
      "The ex-Reading defender denied fraudulent trading charges relating to the Sodje Sports Foundation -\n",
      "a charity to raise money for Nigerian sport.\n",
      "Mr Sodje, 37, is jointly charged with elder brothers Efe, 44, Bright, 50 and Stephen, 42.\n",
      "Appearing at the Old Bailey earlier, all four denied the offence.\n",
      "The charge relates to offences which allegedly took place between 2008 and 2014.\n",
      "Sam, from Kent, Efe and Bright, of Greater Manchester, and Stephen, from Bexley, are due to stand\n",
      "trial in July.\n",
      "They were all released on bail.\n",
      "üìù Summary:\n",
      "Former Premier League footballer Sam Sodje has appeared in court alongside three brothers accused of\n",
      "charity fraud.\n",
      "\n",
      "üìÑ Article 2:\n",
      "Voges was forced to retire hurt on 86 after suffering the injury while batting during the County\n",
      "Championship draw with Somerset on 4 June.\n",
      "Middlesex hope to have the Australian back for their T20 Blast game against Hampshire at Lord's on 3\n",
      "August.\n",
      "The 37-year-old has scored 230 runs in four first-class games this season at an average of 57.50.\n",
      "\"Losing Adam is naturally a blow as he contributes significantly to everything we do,\" director of\n",
      "cricket Angus Fraser said.\n",
      "\"His absence, however, does give opportunities to other players who are desperate to play in the\n",
      "first XI.\n",
      "\"In the past we have coped well without an overseas player and I expect us to do so now.\"\n",
      "Defending county champions Middlesex are sixth in the Division One table, having drawn all four of\n",
      "their matches this season.\n",
      "Voges retired from international cricket in February with a Test batting average of 61.87 from 31\n",
      "innings, second only to Australian great Sir Donald Bradman's career average of 99.94 from 52 Tests.\n",
      "üìù Summary:\n",
      "Middlesex batsman Adam Voges will be out until August after suffering a torn calf muscle in his\n",
      "right leg.\n"
     ]
    }
   ],
   "source": [
    "# Load XSum dataset\n",
    "dataset = load_dataset(\"xsum\", trust_remote_code=True)\n",
    "num_train = min(2000, len(dataset[\"train\"]))\n",
    "num_val = min(200, len(dataset[\"validation\"]))\n",
    "train_data = dataset[\"train\"].select(range(num_train))\n",
    "val_data = dataset[\"validation\"].select(range(num_val))\n",
    "\n",
    "# Preview 2 validation samples\n",
    "for i in range(2):\n",
    "    print(f\"\\nüìÑ Article {i+1}:\\n{val_data[i]['document']}\")\n",
    "    print(f\"üìù Summary:\\n{val_data[i]['summary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6600536b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ‚ú≥Ô∏è **Tokenize the Datasets**\n",
    "\n",
    "We start with a function that takes one item from the dataset, e.g. `train_data[0]`.  Let's look at the structure of that item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e977784c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document': 'The full cost of damage in Newton Stewart, one of the areas worst affected, is still\n",
      "being assessed.\\nRepair work is ongoing in Hawick and many roads in Peeblesshire remain badly\n",
      "affected by standing water.\\nTrains on the west coast mainline face disruption due to damage at the\n",
      "Lamington Viaduct.\\nMany businesses and householders were affected by flooding in Newton Stewart\n",
      "after the River Cree overflowed into the town.\\nFirst Minister Nicola Sturgeon visited the area to\n",
      "inspect the damage.\\nThe waters breached a retaining wall, flooding many commercial properties on\n",
      "Victoria Street - the main shopping thoroughfare.\\nJeanette Tate, who owns the Cinnamon Cafe which\n",
      "was badly affected, said she could not fault the multi-agency response once the flood hit.\\nHowever,\n",
      "she said more preventative work could have been carried out to ensure the retaining wall did not\n",
      "fail.\\n\"It is difficult but I do think there is so much publicity for Dumfries and the Nith - and I\n",
      "totally appreciate that - but it is almost like we\\'re neglected or forgotten,\" she said.\\n\"That may\n",
      "not be true but it is perhaps my perspective over the last few days.\\n\"Why were you not ready to\n",
      "help us a bit more when the warning and the alarm alerts had gone out?\"\\nMeanwhile, a flood alert\n",
      "remains in place across the Borders because of the constant rain.\\nPeebles was badly hit by\n",
      "problems, sparking calls to introduce more defences in the area.\\nScottish Borders Council has put a\n",
      "list on its website of the roads worst affected and drivers have been urged not to ignore closure\n",
      "signs.\\nThe Labour Party\\'s deputy Scottish leader Alex Rowley was in Hawick on Monday to see the\n",
      "situation first hand.\\nHe said it was important to get the flood protection plan right but backed\n",
      "calls to speed up the process.\\n\"I was quite taken aback by the amount of damage that has been\n",
      "done,\" he said.\\n\"Obviously it is heart-breaking for people who have been forced out of their homes\n",
      "and the impact on businesses.\"\\nHe said it was important that \"immediate steps\" were taken to\n",
      "protect the areas most vulnerable and a clear timetable put in place for flood prevention\n",
      "plans.\\nHave you been affected by flooding in Dumfries and Galloway or the Borders? Tell us about\n",
      "your experience of the situation and how it was handled. Email us on selkirk.news@bbc.co.uk or\n",
      "dumfries@bbc.co.uk.', 'summary': 'Clean-up operations are continuing across the Scottish Borders and\n",
      "Dumfries and Galloway after flooding caused by Storm Frank.', 'id': '35232142'}\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227b034a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We need to produce a new dictionary that has 'input_ids' which will be the tokenized text of 'document'.  It will have an 'attention_mask' of 1's and 0's where 0's indicate padding tokens that should be ignored in the 'input_ids'.  Finally we'll add a 'labels' key in the dictionary and the corresponding value is the tokenized summary.\n",
    "\n",
    "The best way to understand this is to examine the code below and study the input dictionary above and the output dictionary below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44be4509",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def dataset_tokenizer(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples['document'], max_length=512, truncation=True\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        text_target=examples['summary'], max_length=64, truncation=True\n",
    "    )\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a17c1bb9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 133, 455, 701, 9, 1880, 11, 10793, 6192, 6, 65, 9, 5, 911, 2373, 2132, 6, 16, 202,\n",
      "145, 11852, 4, 50118, 22026, 2456, 173, 16, 2256, 11, 10034, 1758, 8, 171, 3197, 11, 221, 1942, 428,\n",
      "1672, 6867, 1091, 7340, 2132, 30, 2934, 514, 4, 50118, 12667, 5069, 15, 5, 3072, 3673, 42656, 652,\n",
      "10044, 528, 7, 1880, 23, 5, 226, 9708, 1054, 16376, 625, 21491, 4, 50118, 10787, 1252, 8, 6028, 268,\n",
      "58, 2132, 30, 5681, 11, 10793, 6192, 71, 5, 1995, 30084, 41031, 9725, 88, 5, 1139, 4, 50118, 10993,\n",
      "692, 14371, 21801, 3790, 5, 443, 7, 18973, 5, 1880, 4, 50118, 133, 5794, 18646, 10, 17784, 2204, 6,\n",
      "5681, 171, 1861, 3611, 15, 4769, 852, 111, 5, 1049, 3482, 10675, 17825, 4, 50118, 35689, 3398,\n",
      "16255, 6, 54, 1831, 5, 43351, 16542, 61, 21, 7340, 2132, 6, 26, 79, 115, 45, 7684, 5, 3228, 12,\n",
      "26904, 1263, 683, 5, 5005, 478, 4, 50118, 10462, 6, 79, 26, 55, 2097, 3693, 173, 115, 33, 57, 2584,\n",
      "66, 7, 1306, 5, 17784, 2204, 222, 45, 5998, 4, 50118, 113, 243, 16, 1202, 53, 38, 109, 206, 89, 16,\n",
      "98, 203, 13698, 13, 16664, 506, 4458, 8, 5, 234, 3432, 111, 8, 38, 4940, 5478, 14, 111, 53, 24, 16,\n",
      "818, 101, 52, 214, 20428, 50, 9885, 60, 79, 26, 4, 50118, 113, 1711, 189, 45, 28, 1528, 53, 24, 16,\n",
      "2532, 127, 4263, 81, 5, 94, 367, 360, 4, 50118, 113, 7608, 58, 47, 45, 1227, 7, 244, 201, 10, 828,\n",
      "55, 77, 5, 2892, 8, 5, 8054, 5431, 56, 1613, 66, 1917, 50118, 38488, 6, 10, 5005, 5439, 1189, 11,\n",
      "317, 420, 5, 23987, 142, 9, 5, 5891, 1895, 4, 50118, 510, 1942, 11060, 21, 7340, 478, 30, 1272, 6,\n",
      "17959, 1519, 7, 6581, 55, 30356, 11, 5, 443, 4, 50118, 22041, 1173, 23987, 1080, 34, 342, 10, 889,\n",
      "15, 63, 998, 9, 5, 3197, 2373, 2132, 8, 2377, 33, 57, 2966, 45, 7, 8861, 6803, 2434, 4, 50118, 133,\n",
      "4165, 1643, 18, 3193, 5411, 884, 2618, 9224, 607, 21, 11, 10034, 1758, 15, 302, 7, 192, 5, 1068, 78,\n",
      "865, 4, 50118, 894, 26, 24, 21, 505, 7, 120, 5, 5005, 2591, 563, 235, 53, 4094, 1519, 7, 2078, 62,\n",
      "5, 609, 4, 50118, 113, 100, 21, 1341, 551, 36347, 30, 5, 1280, 9, 1880, 14, 34, 57, 626, 60, 37, 26,\n",
      "4, 50118, 113, 12455, 24, 16, 1144, 12, 10536, 13, 82, 54, 33, 57, 1654, 66, 9, 49, 1611, 8, 5, 913,\n",
      "15, 1252, 72, 50118, 894, 26, 24, 21, 505, 14, 22, 757, 30771, 2402, 113, 58, 551, 7, 1744, 5, 911,\n",
      "144, 4478, 8, 10, 699, 17777, 342, 11, 317, 13, 5005, 8555, 708, 4, 50118, 17781, 47, 57, 2132, 30,\n",
      "5681, 11, 16664, 506, 4458, 8, 7155, 20574, 50, 5, 23987, 116, 11378, 201, 59, 110, 676, 9, 5, 1068,\n",
      "8, 141, 24, 21, 7521, 4, 3593, 201, 15, 842, 462, 330, 10126, 4, 2926, 1039, 14141, 438, 4, 876, 4,\n",
      "1350, 50, 385, 783, 506, 4458, 1039, 14141, 438, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1,\n",
      "1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 40827, 12, 658, 1414, 32, 3348, 420, 5, 5411, 23987, 8,\n",
      "16664, 506, 4458, 8, 7155, 20574, 71, 5681, 1726, 30, 5809, 3848, 4, 2]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_tokenizer(train_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aedadb1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Finally, we apply the function, using `map` function, to produce the tokenized datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c3334ea",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize data\n",
    "\n",
    "tokenized_train = train_data.map(dataset_tokenizer, batched=True)\n",
    "tokenized_val = val_data.map(dataset_tokenizer, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ef858",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ‚ú≥Ô∏è **Fine-Tune BART-Large-CNN**\n",
    "\n",
    "The code for fine-tuning is similar to what we've seen in previous lessons.  We have to setup the training arguments, then configure the trainer and execute the training.  Some of you have expressed interest in learning more about these configurations.  I've found AI to be really helpful for getting started here.\n",
    "\n",
    "<details>\n",
    "<summary>You can CLICK HERE to get a description of each of the training arguments</summary>\n",
    "\n",
    "- **`output_dir`**: Specifies the directory where model checkpoints and outputs will be saved.\n",
    "- **`eval_strategy`**: Determines when to evaluate the model during training. Options include \"no\", \"steps\", or \"epoch\". Here, it evaluates at the end of each epoch.\n",
    "- **`save_strategy`**: Specifies when to save model checkpoints. Options include \"no\", \"steps\", or \"epoch\". Here, it saves at the end of each epoch.\n",
    "- **`learning_rate`**: Sets the initial learning rate for the optimizer. Here, it is set to `3e-5`.\n",
    "- **`per_device_train_batch_size`**: Defines the batch size for training on each device (e.g., GPU or CPU). Here, it is set to `8`.\n",
    "- **`per_device_eval_batch_size`**: Defines the batch size for evaluation on each device. Here, it is set to `8`.\n",
    "- **`num_train_epochs`**: Specifies the total number of training epochs. Here, it is set to `3`.\n",
    "- **`weight_decay`**: Applies weight decay (L2 regularization) to prevent overfitting. Here, it is set to `0.01`.\n",
    "- **`fp16`**: Enables mixed precision training for faster computation and reduced memory usage. Set to `True` to use FP16 (16-bit floating point).\n",
    "- **`disable_tqdm`**: Disables the progress bar during training. Set to `False` to keep the progress bar visible.\n",
    "- **`predict_with_generate`**: Enables text generation during evaluation to compute metrics like ROUGE. Set to `True` to generate predictions.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80892db2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=str(MODELS_PATH / \"xsum_bart_large\"),\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    disable_tqdm=False,\n",
    "    predict_with_generate=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8094e5b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<details>\n",
    "<summary>CLICK HERE to get details about the arguments we're using in `Seq2SeqTrainer`</summary>\n",
    "\n",
    "- **`model`**: The Hugging Face model to train or evaluate. In this case, it is the sequence-to-sequence model (`model`) loaded earlier (e.g., `facebook/bart-large-cnn`).\n",
    "\n",
    "- **`args`**: The training arguments provided as an instance of `Seq2SeqTrainingArguments`. This includes configurations like learning rate, batch size, number of epochs, and evaluation strategy.\n",
    "\n",
    "- **`train_dataset`**: The dataset used for training. Here, it is the tokenized training dataset (`tokenized_train`).\n",
    "\n",
    "- **`eval_dataset`**: The dataset used for evaluation. Here, it is the tokenized validation dataset (`tokenized_val`).\n",
    "\n",
    "- **`data_collator`**: A function or object that batches and preprocesses data for the model. In this case, it is a ` DataCollatorForSeq2Seq` object we loaded above, which ensures proper padding and formatting for sequence-to-sequence tasks.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ea8a66f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 03:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.845416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.562400</td>\n",
       "      <td>1.904088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.562400</td>\n",
       "      <td>2.082829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bagge\\miniforge-pypy3\\envs\\DS776_env_v2\\lib\\site-packages\\transformers\\modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=1.2987611083984374, metrics={'train_runtime': 181.3485, 'train_samples_per_second': 33.085, 'train_steps_per_second': 4.136, 'total_flos': 6482063822487552.0, 'train_loss': 1.2987611083984374, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7d3c75",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There's some evidence of overfitting there since the validation loss increases.  We're using a very small subset of the data for this demonstration so it's not surprising that we're seeing overfitting. \n",
    "\n",
    "**Note:** If you're seeing a warning it's because the pre-trained model we loaded stores it's configuration parameter differently than the latest transformers library expect.  It's OK to ignore.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54c2f5e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ‚ú≥Ô∏è **Qualitative Comparison of Summaries**\n",
    "\n",
    "Let's see how the summaries compare for the first article in the validation set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c724e61e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The first article in the validation set is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ab0198b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Sample article:\n",
      "The ex-Reading defender denied fraudulent trading charges relating to the Sodje Sports Foundation -\n",
      "a charity to raise money for Nigerian sport.\n",
      "Mr Sodje, 37, is jointly charged with elder brothers Efe, 44, Bright, 50 and Stephen, 42.\n",
      "Appearing at the Old Bailey earlier, all four denied the offence.\n",
      "The charge relates to offences which allegedly took place between 2008 and 2014.\n",
      "Sam, from Kent, Efe and Bright, of Greater Manchester, and Stephen, from Bexley, are due to stand\n",
      "trial in July.\n",
      "They were all released on bail.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_article = val_data[0]['document']\n",
    "print(f\"üìÑ Sample article:\\n{sample_article}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7da34cc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here's a function to help you generate summaries.  We added many comments to help you understand the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bb20203",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_summary(text, model, tokenizer, device, max_length=64, length_penalty=1.0):\n",
    "    # Tokenize the input text and convert it into tensors suitable for the model\n",
    "    # `max_length=512` ensures the input is truncated if it exceeds 512 tokens\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    \n",
    "    # Generate the summary using the model\n",
    "    # `num_beams=4` specifies the beam search size for better quality summaries\n",
    "    # `max_length=64` limits the length of the generated summary\n",
    "    # `early_stopping=True` stops generation when all beams reach the end token\n",
    "    # length_penalty adjusts the length of the generated summary\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, \n",
    "                                 max_length=max_length, early_stopping=True,\n",
    "                                 length_penalty=length_penalty)\n",
    "    \n",
    "    # Decode the generated token IDs back into a human-readable string\n",
    "    # `skip_special_tokens=True` removes special tokens like <s> and </s>\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e6f4c5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "Now let's load the base-model, our fine-tuned model, and a model that has been fine-tuned on the complete xsum dataset.  We'll generate the summaries for each so we can compare them qualitatively.\n",
    "\n",
    "**Note:**  This line of code `fine_tuned_model.config.forced_bos_token_id = None` shouldn't be necessary, but `transformers` is setting `forced_bos_token_id = 0` in the saved model which causes the text generation to work incorrectly.  I'm opening an issue on Github for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "141f5e2e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bagge\\miniforge-pypy3\\envs\\DS776_env_v2\\lib\\site-packages\\transformers\\models\\bart\\configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Reference Summary:\n",
      "Former Premier League footballer Sam Sodje has appeared in court alongside three brothers accused of\n",
      "charity fraud.\n",
      "\n",
      "üìù Base Model Summary:\n",
      "Sam Sodje, 37, is jointly charged with elder brothers Efe, 44, Bright, 50 and Stephen, 42. The\n",
      "charge relates to offences which allegedly took place between 2008 and 2014. Sam, from Kent, Efe and\n",
      "Bright, of Greater Manchester, and Stephen,. from Bexley,\n",
      "\n",
      "üìù Fine-Tuned Model Summary:\n",
      "Former England footballer Sam Sodje has appeared in court accused of defrauding a sports charity he\n",
      "set up in his native Nigeria out of more than ¬£1.5m over a period of four years, the Old Bailey has\n",
      "heard. The charity was set up by Mr Sodje's father,\n",
      "\n",
      "üìù Fully Fine-Tuned Model Summary:\n",
      "Former Premier League footballer Sam Sodje has appeared in court charged with fraud.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# reload the base model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Reload the fine-tuned model\n",
    "checkpoint_path = MODELS_PATH / \"xsum_bart_large\" / \"checkpoint-750\"\n",
    "fine_tuned_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "fine_tuned_model.config.forced_bos_token_id = None # Set to None to squash bug\n",
    "# tokenizer is the same as base model\n",
    "\n",
    "# Fully-fine-tuned model summary\n",
    "full_ft_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-xsum\").to(device)\n",
    "\n",
    "reference_summary = val_data[0]['summary']\n",
    "base_summary = generate_summary(sample_article, model, tokenizer, device)\n",
    "fine_tuned_summary = generate_summary(sample_article, fine_tuned_model, tokenizer, device)\n",
    "full_ft_summary = generate_summary(sample_article, full_ft_model, tokenizer, device)\n",
    "\n",
    "print(f\"üìù Reference Summary:\\n{reference_summary}\\n\")\n",
    "print(f\"üìù Base Model Summary: \\n{base_summary}\\n\" )\n",
    "print(f\"üìù Fine-Tuned Model Summary: \\n{fine_tuned_summary}\\n\" )\n",
    "print(f\"üìù Fully Fine-Tuned Model Summary: \\n{full_ft_summary}\\n\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febb5b66",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Our fine-tuned model clearly knows more about Sam Sodje than what is contained in the article.  It does manage a summary but it's not exactly brief.  We'lve only used a very small subset of the training data for a demonstration so our result isn't that great. However, you can see that the fine-tuned model that was already trained on the whole dataset produces a great, simple abstractive summary that's perhaps better than the reference summary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c3e2e1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ‚ú≥Ô∏è **Define Evaluation Metrics (ROUGE and BERTScore)**\n",
    "\n",
    "The `compute_metrics` function below takes the model prediction and label which are lists or tensors of token IDs, decodes them back to text, and computes the metrics.  We didn't include BLEU since our example above showed that BLEU isn't very good for assessing text similarity.  AI is helpful here to figure out how to include the evaluation metrics you want.  \n",
    "\n",
    "We make use of the Hugging Face `evaluate` library.  [Learn more here.](https://huggingface.co/docs/evaluate/en/index)  You'll need the `rouge` and `bert_score` packages installed - they should be if you've installed the latest course package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36e09d7f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "rouge = load(\"rouge\")\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute ROUGE and BERTScore metrics for evaluating summarization models.\n",
    "    \n",
    "    This function is designed to be used with Hugging Face's Trainer.evaluate() method.\n",
    "    It compares model predictions to reference summaries using:\n",
    "    \n",
    "    - ROUGE-1, ROUGE-2, ROUGE-L, and ROUGE-Lsum (recall-focused measures of overlap)\n",
    "    - BERTScore-F1 (semantic similarity based on contextual embeddings)\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with metric names as keys and scores as values (multiplied by 100).\n",
    "    \"\"\"\n",
    "    # Unpack predictions and labels\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Some models return (logits, ...) as predictions, so we extract the first element\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "\n",
    "    # Convert to numpy arrays for easier handling\n",
    "    predictions = np.asarray(predictions)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    # If predictions are logits (batch_size x seq_len x vocab_size), take argmax\n",
    "    if predictions.ndim == 3:\n",
    "        predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    # Convert to plain Python lists\n",
    "    predictions = predictions.tolist()\n",
    "    labels = labels.tolist()\n",
    "\n",
    "    # Replace -100 (used to ignore padding in labels) with the tokenizer's pad token ID\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    labels = [[(token if token != -100 else pad_token_id) for token in label] for label in labels]\n",
    "\n",
    "    # Decode token IDs to strings\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Add newlines between sentences for ROUGE-Lsum to work properly\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "    # Compute ROUGE scores (with stemming)\n",
    "    rouge_scores = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    rouge_result = {f\"{k}_f1\": v * 100 for k, v in rouge_scores.items()}\n",
    "\n",
    "    # Compute BERTScore (average F1 across examples)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")  # Suppress tokenizer/model loading warnings\n",
    "        bertscore_result = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "    bertscore_f1 = {\"bertscore_f1\": np.mean(bertscore_result[\"f1\"]) * 100}\n",
    "\n",
    "    # Merge all metrics into a single dictionary\n",
    "    return {**rouge_result, **bertscore_f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6af30e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ‚ú≥Ô∏è **Evaluate Models on the Validation Set**\n",
    "\n",
    "Here we'll compute all the metrics on the (reduced) validation set for the base model, our fine-tuned model, and the fully fine-tuned model.\n",
    "\n",
    "You need to run the cell that loads the models in the qualitative comparisons section above before running the code below.  \n",
    "\n",
    "To expedite the evaluation we use a Trainer to take advantage of batch processing.  First we'll build a small helper function that takes the model, dataset, collator, and compute_metrics function as input and returns the dictionary of metrics evaluated on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c24c7a1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_metrics(model, training_args, dataset, data_collator, compute_metrics):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the given dataset and compute metrics.\n",
    "\n",
    "    Args:\n",
    "        model: The model to evaluate.\n",
    "        training_args: Training arguments containing evaluation settings.\n",
    "        dataset: The dataset to evaluate on.\n",
    "        data_collator: Data collator for batching data.\n",
    "        compute_metrics: Function to compute metrics.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with metric names as keys and scores as values.\n",
    "    \"\"\"\n",
    "    # Create a Trainer instance for evaluation\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        eval_dataset=dataset,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a181c09",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we'll fetch the results.  This could take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3dbd9717",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Base BART Results:\n",
      "{'eval_loss': 2.52492618560791, 'eval_model_preparation_time': 0.0056, 'eval_rouge1_f1':\n",
      "44.73847994604354, 'eval_rouge2_f1': 17.584912484216776, 'eval_rougeL_f1': 41.397483143740445,\n",
      "'eval_rougeLsum_f1': 41.94427735000251, 'eval_bertscore_f1': 86.6550963819027, 'eval_runtime':\n",
      "42.1136, 'eval_samples_per_second': 4.749, 'eval_steps_per_second': 0.594}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Fine-Tuned BART Results:\n",
      "{'eval_loss': 2.082829236984253, 'eval_model_preparation_time': 0.0041, 'eval_rouge1_f1':\n",
      "53.11391717145055, 'eval_rouge2_f1': 27.323786668296307, 'eval_rougeL_f1': 50.544033050052086,\n",
      "'eval_rougeLsum_f1': 50.617683562004956, 'eval_bertscore_f1': 88.53831321001053, 'eval_runtime':\n",
      "55.0593, 'eval_samples_per_second': 3.632, 'eval_steps_per_second': 0.454}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Fully Fine-Tuned BART Results:\n",
      "{'eval_loss': 2.3121683597564697, 'eval_model_preparation_time': 0.004, 'eval_rouge1_f1':\n",
      "57.32454164802877, 'eval_rouge2_f1': 32.91543717646353, 'eval_rougeL_f1': 55.161648779914984,\n",
      "'eval_rougeLsum_f1': 55.20277263969467, 'eval_bertscore_f1': 89.46658211946487, 'eval_runtime':\n",
      "57.2807, 'eval_samples_per_second': 3.492, 'eval_steps_per_second': 0.436}\n"
     ]
    }
   ],
   "source": [
    "base_results = evaluate_metrics(model, training_args, tokenized_val, data_collator, compute_metrics)\n",
    "print(\"\\nüìà Base BART Results:\")\n",
    "print(base_results)\n",
    "\n",
    "fine_tuned_results = evaluate_metrics(fine_tuned_model, training_args, tokenized_val, data_collator, compute_metrics)\n",
    "print(\"\\nüìà Fine-Tuned BART Results:\")\n",
    "print(fine_tuned_results)\n",
    "\n",
    "full_ft_results = evaluate_metrics(full_ft_model, training_args, tokenized_val, data_collator, compute_metrics)\n",
    "print(\"\\nüìà Fully Fine-Tuned BART Results:\")\n",
    "print(full_ft_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1e4673",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We didn't print those out nicely, but if you look carefully you can see that the metrics all increased for the fine-tuned model, and even more for the fully fine-tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf752c6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the homework we'll compare using LLMs for summarization to using these specialized models.  We'll also further explore using LLMs for evaluating text similarity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776_env_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
