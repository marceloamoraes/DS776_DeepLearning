{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbe8c867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ../Course_Tools/introdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2860545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODELS_PATH=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\models\n",
      "DATA_PATH=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\data\n",
      "TORCH_HOME=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\n",
      "HF_HOME=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\n",
      "HF_HUB_CACHE=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\n",
      "Successfully logged in to Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    BartForConditionalGeneration, BartTokenizer, \n",
    "    Trainer, TrainingArguments, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    ")\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "from evaluate import load\n",
    "from pathlib import Path\n",
    "\n",
    "from evaluate import load\n",
    "import numpy as np\n",
    "\n",
    "from transformers import EvalPrediction\n",
    "import numpy as np\n",
    "import torch\n",
    "from evaluate import load\n",
    "\n",
    "from introdl.utils import config_paths_keys, wrap_print_text, cleanup_torch\n",
    "\n",
    "print = wrap_print_text(print, width = 100)\n",
    "paths = config_paths_keys()\n",
    "MODELS_PATH = paths['MODELS_PATH']\n",
    "DATA_PATH = paths['DATA_PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf896ea",
   "metadata": {},
   "source": [
    "# Section 2 - Metrics\n",
    "\n",
    "\n",
    "### üìö **Brief Introductions to the Four Metrics**\n",
    "\n",
    "1. **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\n",
    "   - Measures **n-gram, subsequence, or skip-bigram overlap** between a candidate and reference text.\n",
    "   - Commonly used for **extractive summarization** but also applied to **abstractive summarization**.\n",
    "   - Variants: **ROUGE-N (e.g., ROUGE-1, ROUGE-2), ROUGE-L (Longest Common Subsequence), ROUGE-S (Skip-bigrams)**.\n",
    "\n",
    "2. **BLEU (Bilingual Evaluation Understudy)**\n",
    "   - Measures **n-gram overlap** between a candidate text and one or more reference texts.\n",
    "   - Originally designed for **machine translation**, but adapted for **summarization**.\n",
    "   - Uses a **brevity penalty** to avoid favoring overly short outputs.\n",
    "   - Often reported with **1-gram to 4-gram precision scores**.\n",
    "\n",
    "3. **BERTScore**\n",
    "   - Measures **semantic similarity** between candidate and reference texts using **contextual embeddings** from models like BERT.\n",
    "   - Matches tokens based on their **cosine similarity in embedding space**.\n",
    "   - Effective for **abstractive summarization**, especially when paraphrasing is present.\n",
    "\n",
    "4. **BARTScore**\n",
    "   - Uses **pretrained language models (e.g., BART)** to estimate the **likelihood of a summary given the source text** and vice versa.\n",
    "   - Evaluates summaries using **bidirectional scoring**: Coverage (`P(summary | source)`) and Faithfulness (`P(source | summary)`).\n",
    "   - Particularly useful for evaluating **fluency, coherence, and factual consistency**.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä **Comparison Table**\n",
    "\n",
    "| **Metric**    | **Use-Cases**                        | **Strengths**                                                                                          | **Weaknesses**                                                                                    |\n",
    "|---------------|--------------------------------------|--------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|\n",
    "| **ROUGE**     | Extractive summarization, Some Abstractive Summarization | - Easy to compute and interpret. <br> - Works well for extractive tasks. <br> - Multiple variants for different needs. | - Ignores paraphrasing. <br> - Surface-level comparison. <br> - Sensitive to minor wording changes. |\n",
    "| **BLEU**      | Machine Translation, Summarization   | - Simple and fast to compute. <br> - Precision-oriented. <br> - Useful for extractive and some abstractive tasks. | - Penalizes paraphrasing. <br> - Limited to local n-gram matching. <br> - Ignores semantic similarity. |\n",
    "| **BERTScore** | Abstractive Summarization, Paraphrasing | - Captures semantic similarity well. <br> - Robust to paraphrasing and rephrasing. <br> - Works well for abstractive summaries. | - Ignores coherence and sentence structure. <br> - Dependent on quality of pretrained embeddings. |\n",
    "| **BARTScore** | Abstractive Summarization, Coherence Evaluation, Faithfulness Check | - Measures fluency, coherence, and factual consistency. <br> - Can evaluate coverage and faithfulness. <br> - Useful for abstractive summarization. | - Sensitive to the training domain. <br> - Can prioritize fluency over factual accuracy. |\n",
    "\n",
    "---\n",
    "\n",
    "### üîë **Summary**\n",
    "- **ROUGE and BLEU** are foundational metrics that are simple to compute and interpret but have limitations in handling semantic similarity and paraphrasing.\n",
    "- **BERTScore** introduces the idea of using embeddings to capture meaning, making it more robust for abstractive summarization.\n",
    "- **BARTScore** leverages pretrained language models to evaluate coherence and coverage, providing deeper insight into the quality of generated summaries.\n",
    "\n",
    "Would you like me to help you design a **hands-on tutorial** where students apply these metrics to evaluate different summaries and compare their results?\n",
    "\n",
    "\n",
    "For an **introductory NLP class**, it's best to focus on metrics that are:\n",
    "\n",
    "1. **Simple to understand and compute.**\n",
    "2. **Illustrative of important concepts.**\n",
    "3. **Widely used in research and applications.**\n",
    "4. **Relevant to both extractive and abstractive summarization.**\n",
    "\n",
    "### üîë **Recommended Metrics**\n",
    "\n",
    "| **Metric**    | **Why Include It?**                                    | **Focus Areas**                                   |\n",
    "|---------------|-------------------------------------------------------|--------------------------------------------------|\n",
    "| **ROUGE**     | - Most commonly used for summarization evaluation. <br> - Straightforward to compute and interpret. <br> - Shows how n-gram overlap works (ROUGE-N) and how ordering matters (ROUGE-L). | - Introduce n-gram overlap. <br> - Precision, recall, F1-score. <br> - Compare ROUGE-N vs. ROUGE-L. |\n",
    "| **BLEU**      | - Easy to understand as a basic n-gram overlap metric. <br> - Commonly used for MT but applicable to summarization. | - Explain brevity penalty. <br> - Illustrate difference between extractive and abstractive summaries. |\n",
    "| **BERTScore** | - Introduces semantic similarity beyond n-gram overlap. <br> - Demonstrates strengths of contextual embeddings. | - Compare embedding-based metrics with n-gram metrics. <br> - Show limitations with coherence and order. |\n",
    "| **BARTScore** | - Demonstrates how pretrained language models can be used for evaluation. <br> - Illustrates sequence-to-sequence models. | - Discuss how bidirectional scoring works. <br> - Show difference between fluency, coverage, and faithfulness. |\n",
    "\n",
    "### üìå **Rationale for Choice**\n",
    "\n",
    "1. **ROUGE and BLEU** are easy to understand and compute. They provide a clear starting point for introducing how evaluation metrics work, especially for **extractive summarization**.\n",
    "2. **BERTScore** introduces the idea of **semantic similarity** using contextual embeddings. This bridges the gap to **abstractive summarization** and modern deep learning models.\n",
    "3. **BARTScore** is an excellent introduction to using **pretrained language models** for evaluation, which is critical for students to understand **modern evaluation techniques**.\n",
    "\n",
    "### üö´ **Metrics to Defer for Later**\n",
    "- **METEOR, MoverScore, SummaQA, BLEURT** are more advanced and require a deeper understanding of embeddings, paraphrasing, and QA systems.\n",
    "- They're worth mentioning, but they‚Äôre not essential for an introductory lecture.\n",
    "\n",
    "### üìñ **Suggested Flow for Teaching**\n",
    "1. **Introduction to Evaluation Metrics:** Explain precision, recall, and F1-score.\n",
    "2. **ROUGE and BLEU:** Compute these metrics for extractive vs. abstractive summaries.\n",
    "3. **BERTScore:** Show how embeddings can capture meaning beyond surface-level overlap.\n",
    "4. **BARTScore:** Demonstrate how modern models are used for evaluation.\n",
    "5. **Comparison and Discussion:** Compare these metrics using a small summarization task.\n",
    "\n",
    "Would you like me to help you create a **Jupyter Notebook tutorial** for your students with these metrics?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9cc224",
   "metadata": {},
   "source": [
    "# Section 3 \n",
    "\n",
    "### Comparing Specialized Summarization Models (BART, T5, PEGASUS) vs. LLMs (GPT-4, LLaMA)\n",
    "\n",
    "Summarization tasks can be tackled using **specialized encoder-decoder models** like **BART, T5, and PEGASUS**, or **general-purpose decoder-only models** like **GPT-4 and LLaMA**. Each approach has its own strengths and weaknesses.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Strengths of Specialized Summarization Models (BART, T5, PEGASUS)**\n",
    "1. **Architectural Efficiency**\n",
    "   - Encoder-decoder models process the entire input once with the encoder before generating the summary, making them *computationally efficient* for summarization.\n",
    "   - In contrast, decoder-only models must repeatedly attend to the entire input during generation, which is particularly costly for long inputs.\n",
    "\n",
    "2. **Tailored Training Objectives**\n",
    "   - These models are pre-trained specifically for text-to-text tasks.\n",
    "     - **BART:** Trained as a denoising autoencoder, making it robust to noisy or incomplete input.\n",
    "     - **T5:** Uses a ‚Äútext-to-text‚Äù framework, making it versatile across various NLP tasks, including summarization.\n",
    "     - **PEGASUS:** Pre-trained to generate summaries by masking entire sentences during training, directly optimizing for abstractive summarization.\n",
    "\n",
    "3. **Alignment with Summarization Tasks**\n",
    "   - Fine-tuning on summarization datasets (e.g., CNN/Daily Mail, XSum) leads to **high-quality summaries** that are concise and relevant.\n",
    "   - Performance on benchmarks often surpasses general-purpose LLMs.\n",
    "\n",
    "4. **Better Control over Output**\n",
    "   - Easier to enforce structure, conciseness, or adherence to specific formatting requirements.\n",
    "   - Less prone to **hallucinations** or verbose outputs compared to general-purpose LLMs.\n",
    "\n",
    "5. **Domain-Specific Optimization**\n",
    "   - Fine-tuning encoder-decoder models on specialized datasets (e.g., medical or legal texts) produces highly accurate summaries with relevant terminology and structure.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùå **Weaknesses of Specialized Summarization Models**\n",
    "1. **Limited Generalization**\n",
    "   - Models like BART, T5, and PEGASUS require fine-tuning for specific summarization tasks.\n",
    "   - Struggle with novel domains or tasks without retraining.\n",
    "\n",
    "2. **Less Effective at Zero-Shot Summarization**\n",
    "   - General-purpose LLMs can perform reasonably well on summarization tasks without fine-tuning, which is challenging for encoder-decoder models.\n",
    "\n",
    "3. **Inflexibility**\n",
    "   - Encoder-decoder models are often designed for fixed inputs and outputs, making them less adaptable to creative or open-ended summarization tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Strengths of LLMs (GPT-4, LLaMA) for Summarization**\n",
    "1. **Generalization Across Tasks**\n",
    "   - Capable of summarization **without fine-tuning** through prompt engineering (e.g., ‚ÄúSummarize the following text...‚Äù).\n",
    "   - Strong performance across various domains with minimal adjustments.\n",
    "\n",
    "2. **Few-Shot & Zero-Shot Learning**\n",
    "   - Easily adaptable to new domains or styles through *in-context learning* (providing examples within the prompt).\n",
    "\n",
    "3. **Versatility**\n",
    "   - Handles a wide range of tasks beyond summarization, making them highly flexible for mixed-use applications.\n",
    "   - Can switch between extractive, abstractive, or creative summarization depending on the prompt.\n",
    "\n",
    "4. **Ease of Use**\n",
    "   - No need for specialized training or fine-tuning, making them immediately usable for various summarization tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùå **Weaknesses of LLMs for Summarization**\n",
    "1. **Inefficiency for Long Texts**\n",
    "   - Decoder-only models process the entire input text during every generation step, resulting in high computational costs for long documents.\n",
    "\n",
    "2. **Prone to Hallucination**\n",
    "   - Without fine-tuning or careful prompting, LLMs can generate irrelevant or incorrect information, particularly for factual summarization tasks.\n",
    "\n",
    "3. **Less Structured Output**\n",
    "   - Outputs may be verbose or off-topic unless the prompt is carefully designed to enforce structure and conciseness.\n",
    "\n",
    "4. **Lack of Task-Specific Optimization**\n",
    "   - General-purpose LLMs may underperform compared to fine-tuned encoder-decoder models on specific summarization datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä **Summary: When to Use Each Approach**\n",
    "\n",
    "| Aspect                  | Specialized Models (BART, T5, PEGASUS) | LLMs (GPT-4, LLaMA)                     |\n",
    "|-------------------------|-----------------------------------------|----------------------------------------|\n",
    "| Efficiency              | High (Pre-processed input)             | Low (Full input processed repeatedly) |\n",
    "| Domain Adaptation       | Excellent (Fine-tuning)                | Moderate (Prompt engineering)         |\n",
    "| Generalization          | Limited                                | High                                  |\n",
    "| Output Control          | High (Structure, format)               | Moderate (Requires careful prompting)|\n",
    "| Ease of Use             | Requires fine-tuning                   | Prompt-based, no fine-tuning needed   |\n",
    "| Hallucination Risk      | Lower                                   | Higher                                |\n",
    "\n",
    "---\n",
    "\n",
    "## State-of-the-art Models\n",
    "\n",
    "As of April 2025, specialized models for text summarization continue to evolve, building upon foundational architectures like BART, T5, and PEGASUS. Recent advancements have introduced models such as **Longformer Encoder-Decoder (LED)** and **HERA**, which are tailored to address specific challenges in summarizing lengthy documents.ÓàÜ\n",
    "\n",
    "**Longformer Encoder-Decoder (LED):**\n",
    "LED is designed to handle long documents by extending the Transformer architecture's context window. This allows for efficient processing of extended text inputs, making it particularly effective for summarizing lengthy documents without the need to truncate content. ÓàÄciteÓàÇturn0search15ÓàÅÓàÜ\n",
    "\n",
    "**HERA:**\n",
    "HERA focuses on improving long document summarization by segmenting the text based on its semantic structure. It retrieves and reorders segments related to the same event, enhancing the coherence and relevance of the generated summaries. This approach has shown improvements in handling complex narratives within extensive documents. ÓàÄciteÓàÇturn0academia20ÓàÅÓàÜ\n",
    "\n",
    "While these specialized models offer targeted solutions for summarization tasks, large language models (LLMs) like GPT-4 and Google's Gemini have also demonstrated strong summarization capabilities. However, specialized models remain relevant, especially in scenarios requiring domain-specific knowledge or the ability to process longer texts effectively. ÓàÄciteÓàÇturn0search16ÓàÅÓàÜ\n",
    "\n",
    "In summary, the state-of-the-art in specialized summarization models as of April 2025 includes architectures like LED and HERA, which build upon earlier models to address challenges associated with long document summarization. These models offer efficient and coherent summarization solutions, particularly for extensive and complex texts.ÓàÜ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203d35ae",
   "metadata": {},
   "source": [
    "# Fine-Tuning and Evaluating a BART Model for Summarization\n",
    "\n",
    "Here‚Äôs how you can **refactor and organize your notebook** into clean, modular sections to accomplish your goals:\n",
    "\n",
    "---\n",
    "\n",
    "### üìò Section 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd70e2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4e914e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üì∞ Section 2: Load and Preview Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb51153",
   "metadata": {},
   "source": [
    "We're going to choose small subsets for training and validation to demonsrate how fine-tuning works and performs, but in a production setting we'd use all of the training data that we can or at least as much as we can afford to use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5660032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Article 1:\n",
      "The ex-Reading defender denied fraudulent trading charges relating to the Sodje Sports Foundation -\n",
      "a charity to raise money for Nigerian sport.\n",
      "Mr Sodje, 37, is jointly charged with elder brothers Efe, 44, Bright, 50 and Stephen, 42.\n",
      "Appearing at the Old Bailey earlier, all four denied the offence.\n",
      "The charge relates to offences which allegedly took place between 2008 and 2014.\n",
      "Sam, from Kent, Efe and Bright, of Greater Manchester, and Stephen, from Bexley, are due to stand\n",
      "trial in July.\n",
      "They were all released on bail.\n",
      "üìù Summary:\n",
      "Former Premier League footballer Sam Sodje has appeared in court alongside three brothers accused of\n",
      "charity fraud.\n",
      "\n",
      "üìÑ Article 2:\n",
      "Voges was forced to retire hurt on 86 after suffering the injury while batting during the County\n",
      "Championship draw with Somerset on 4 June.\n",
      "Middlesex hope to have the Australian back for their T20 Blast game against Hampshire at Lord's on 3\n",
      "August.\n",
      "The 37-year-old has scored 230 runs in four first-class games this season at an average of 57.50.\n",
      "\"Losing Adam is naturally a blow as he contributes significantly to everything we do,\" director of\n",
      "cricket Angus Fraser said.\n",
      "\"His absence, however, does give opportunities to other players who are desperate to play in the\n",
      "first XI.\n",
      "\"In the past we have coped well without an overseas player and I expect us to do so now.\"\n",
      "Defending county champions Middlesex are sixth in the Division One table, having drawn all four of\n",
      "their matches this season.\n",
      "Voges retired from international cricket in February with a Test batting average of 61.87 from 31\n",
      "innings, second only to Australian great Sir Donald Bradman's career average of 99.94 from 52 Tests.\n",
      "üìù Summary:\n",
      "Middlesex batsman Adam Voges will be out until August after suffering a torn calf muscle in his\n",
      "right leg.\n"
     ]
    }
   ],
   "source": [
    "# Load XSum dataset\n",
    "dataset = load_dataset(\"xsum\", cache_dir=DATA_PATH)\n",
    "num_train = min(6000, len(dataset[\"train\"]))\n",
    "num_val = min(200, len(dataset[\"validation\"]))\n",
    "train_data = dataset[\"train\"].select(range(num_train))\n",
    "val_data = dataset[\"validation\"].select(range(num_val))\n",
    "\n",
    "# Preview 2 validation samples\n",
    "for i in range(2):\n",
    "    print(f\"\\nüìÑ Article {i+1}:\\n{val_data[i]['document']}\")\n",
    "    print(f\"üìù Summary:\\n{val_data[i]['summary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6600536b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üßπ Section 3: Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44be4509",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples['document'], max_length=512, truncation=True\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        text_target=examples['summary'], max_length=64, truncation=True\n",
    "    )\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize data\n",
    "eval_subset_size = 200\n",
    "tokenized_train = train_data.map(preprocess_function, batched=True)\n",
    "tokenized_val = val_data.select(range(eval_subset_size)).map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ef858",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üîß Section 4: Fine-Tune BART-Large-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80892db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=str(MODELS_PATH / \"xsum_bart_large\"),\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    disable_tqdm=False,\n",
    "    predict_with_generate=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ea8a66f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 05:02, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.903600</td>\n",
       "      <td>1.798864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.230600</td>\n",
       "      <td>1.832299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bagge\\miniforge-pypy3\\envs\\DS776_env_v2\\lib\\site-packages\\transformers\\modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1500, training_loss=1.5514997965494792, metrics={'train_runtime': 303.1938, 'train_samples_per_second': 39.579, 'train_steps_per_second': 4.947, 'total_flos': 1.2950887990296576e+16, 'train_loss': 1.5514997965494792, 'epoch': 2.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7d3c75",
   "metadata": {},
   "source": [
    "There's some evidence of overfitting there since the validation loss increases.  We're using a very small subset of the data for this demonstration so it's not surprising that we're seeing overfitting.  Let's look at the predicted summary for the base model and the fine-tuned model for the first article in the validation set.\n",
    "\n",
    "**Note:**  This line of code `fine_tuned_model.config.forced_bos_token_id = None` shouldn't be necessary, but `transformers` is setting `forced_bos_token_id = 0` in the saved model which causes the text generation to work incorrectly.  I'm opening an issue on Github for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bb20203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make a helper function to generate summaries\n",
    "\n",
    "def generate_summary(text, model, tokenizer, device):\n",
    "    # Tokenize the input text and convert it into tensors suitable for the model\n",
    "    # `max_length=512` ensures the input is truncated if it exceeds 512 tokens\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    \n",
    "    # Generate the summary using the model\n",
    "    # `num_beams=4` specifies the beam search size for better quality summaries\n",
    "    # `max_length=64` limits the length of the generated summary\n",
    "    # `early_stopping=True` stops generation when all beams reach the end token\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=64, early_stopping=True)\n",
    "    \n",
    "    # Decode the generated token IDs back into a human-readable string\n",
    "    # `skip_special_tokens=True` removes special tokens like <s> and </s>\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c724e61e",
   "metadata": {},
   "source": [
    "The first article in the validation set is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ab0198b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Sample article:\n",
      "The ex-Reading defender denied fraudulent trading charges relating to the Sodje Sports Foundation -\n",
      "a charity to raise money for Nigerian sport.\n",
      "Mr Sodje, 37, is jointly charged with elder brothers Efe, 44, Bright, 50 and Stephen, 42.\n",
      "Appearing at the Old Bailey earlier, all four denied the offence.\n",
      "The charge relates to offences which allegedly took place between 2008 and 2014.\n",
      "Sam, from Kent, Efe and Bright, of Greater Manchester, and Stephen, from Bexley, are due to stand\n",
      "trial in July.\n",
      "They were all released on bail.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_article = val_data[0]['document']\n",
    "print(f\"üìÑ Sample article:\\n{sample_article}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54c2f5e",
   "metadata": {},
   "source": [
    "Now let's load the base-model, our fine-tuned model, and a model that has been fine-tuned on the complete xsum dataset.  We'll generate the summaries for each so we can compare them qualitatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "141f5e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bagge\\miniforge-pypy3\\envs\\DS776_env_v2\\lib\\site-packages\\transformers\\models\\bart\\configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Reference Summary:\n",
      "Former Premier League footballer Sam Sodje has appeared in court alongside three brothers accused of\n",
      "charity fraud.\n",
      "\n",
      "üìù Base Model Summary:\n",
      "Sam Sodje, 37, is jointly charged with elder brothers Efe, 44, Bright, 50 and Stephen, 42. The\n",
      "charge relates to offences which allegedly took place between 2008 and 2014. Sam, from Kent, Efe and\n",
      "Bright, of Greater Manchester, and Stephen,. from Bexley,\n",
      "\n",
      "üìù Fine-Tuned Model Summary:\n",
      "Former England footballer Sam Sodje has appeared in court charged with embezzling more than ¬£300,000\n",
      "from a sports charity he set up in his home country of Nigeria, the capital city of Lagos, the Old\n",
      "Bailey has heard for the first time.\n",
      "\n",
      "üìù Fully Fine-Tuned Model Summary:\n",
      "Former Premier League footballer Sam Sodje has appeared in court charged with fraud.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# reload the base model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Reload the fine-tuned model\n",
    "checkpoint_path = MODELS_PATH / \"xsum_bart_large\" / \"checkpoint-1500\"\n",
    "fine_tuned_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "fine_tuned_model.config.forced_bos_token_id = None # Set to None to squash bug\n",
    "# tokenizer is the same as base model\n",
    "\n",
    "# Fully-fine-tuned model summary\n",
    "full_ft_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-xsum\").to(device)\n",
    "\n",
    "reference_summary = val_data[0]['summary']\n",
    "base_summary = generate_summary(sample_article, model, tokenizer, device)\n",
    "fine_tuned_summary = generate_summary(sample_article, fine_tuned_model, tokenizer, device)\n",
    "full_ft_summary = generate_summary(sample_article, full_ft_model, tokenizer, device)\n",
    "\n",
    "print(f\"üìù Reference Summary:\\n{reference_summary}\\n\")\n",
    "print(f\"üìù Base Model Summary: \\n{base_summary}\\n\" )\n",
    "print(f\"üìù Fine-Tuned Model Summary: \\n{fine_tuned_summary}\\n\" )\n",
    "print(f\"üìù Fully Fine-Tuned Model Summary: \\n{full_ft_summary}\\n\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c3e2e1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìè Section 5: Define Evaluation Metrics (ROUGE and BERTScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e09d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "rouge = load(\"rouge\")\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute ROUGE and BERTScore metrics for model predictions vs. reference summaries.\n",
    "\n",
    "    Parameters:\n",
    "        eval_pred (EvalPrediction): Contains tokenized model predictions and reference label.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with ROUGE F1 scores (rouge1, rouge2, rougeL, rougeLsum)\n",
    "              and BERTScore precision, recall, and F1.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Some models return a tuple (logits, ...)\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    predictions = np.asarray(predictions)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    # If predictions are logits, take argmax\n",
    "    if predictions.ndim == 3:\n",
    "        predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    # Convert to lists\n",
    "    predictions = predictions.tolist()\n",
    "    labels = labels.tolist()\n",
    "\n",
    "    # Replace -100 with pad_token_id\n",
    "    labels = [[(token if token != -100 else tokenizer.pad_token_id) for token in label] for label in labels]\n",
    "\n",
    "    # Decode\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Strip whitespace\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    # Compute ROUGE\n",
    "    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    rouge_f1 = {f\"{key}_f1\": value * 100 for key, value in rouge_result.items()}\n",
    "\n",
    "    # Compute BERTScore\n",
    "    bert_result = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "    bert_f1 = {\n",
    "        \"bertscore_precision\": np.mean(bert_result[\"precision\"]) * 100,\n",
    "        \"bertscore_recall\": np.mean(bert_result[\"recall\"]) * 100,\n",
    "        \"bertscore_f1\": np.mean(bert_result[\"f1\"]) * 100,\n",
    "    }\n",
    "\n",
    "    # Combine results\n",
    "    return {**rouge_f1, **bert_f1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6af30e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üß™ Section 6: Evaluate Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbd9717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bagge\\miniforge-pypy3\\envs\\DS776_env_v2\\lib\\site-packages\\transformers\\models\\bart\\configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:51]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Fine-Tuned BART Results:\n",
      "{'eval_loss': 1.8322992324829102, 'eval_model_preparation_time': 0.004, 'eval_rouge1_f1':\n",
      "53.466338054514615, 'eval_rouge2_f1': 28.142027701746425, 'eval_rougeL_f1': 50.86069110365715,\n",
      "'eval_rougeLsum_f1': 50.82767043771599, 'eval_runtime': 53.2635, 'eval_samples_per_second': 3.755,\n",
      "'eval_steps_per_second': 0.469}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Reload the fine-tuned model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "checkpoint_path = MODELS_PATH / \"xsum_bart_large\" / \"checkpoint-1500\"\n",
    "fine_tuned_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path).to(device)\n",
    "\n",
    "trainer_with_metrics = Trainer(\n",
    "    model=fine_tuned_model,\n",
    "    args=training_args,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "ft_results = trainer_with_metrics.evaluate()\n",
    "print(\"\\nüìà Fine-Tuned BART Results:\")\n",
    "print(ft_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a31ccad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NASA has announced a new moon mission to be launched next year. The mission will be the first to\n",
      "orbit the moon's surface. The moon mission is expected to launch in 2018. The cost of the mission is\n",
      "not yet known. The project will cost an estimated $1.5 billion.\n"
     ]
    }
   ],
   "source": [
    "sample_input = \"NASA has announced a new moon mission to be launched next year.\"\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(sample_input, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "    output = base_model.generate(**inputs, max_length=64)\n",
    "    print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1ae8040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 1 ---\n",
      "üìÑ Article:\n",
      "The ex-Reading defender denied fraudulent trading charges relating to the Sodje Sports Foundation -\n",
      "a charity to raise money for Nigerian sport.\n",
      "Mr Sodje, 37, is jointly charged with elder brothers Efe, 44, Bright, 50 and Stephen, 42.\n",
      "Appearing at the Old Bailey earlier, all four denied the offence....\n",
      "üìù Reference Summary:\n",
      "Former Premier League footballer Sam Sodje has appeared in court alongside three brothers accused of\n",
      "charity fraud.\n",
      "üß† Generated Summary:\n",
      "\n",
      "\n",
      "--- Example 2 ---\n",
      "üìÑ Article:\n",
      "Voges was forced to retire hurt on 86 after suffering the injury while batting during the County\n",
      "Championship draw with Somerset on 4 June.\n",
      "Middlesex hope to have the Australian back for their T20 Blast game against Hampshire at Lord's on 3\n",
      "August.\n",
      "The 37-year-old has scored 230 runs in four first-c...\n",
      "üìù Reference Summary:\n",
      "Middlesex batsman Adam Voges will be out until August after suffering a torn calf muscle in his\n",
      "right leg.\n",
      "üß† Generated Summary:\n",
      "\n",
      "\n",
      "--- Example 3 ---\n",
      "üìÑ Article:\n",
      "Seven photographs taken in the Norfolk countryside by photographer Josh Olins will appear in the\n",
      "June edition.\n",
      "In her first sitting for a magazine, the duchess is seen looking relaxed and wearing casual clothes.\n",
      "The shoot was in collaboration with the National Portrait Gallery, where two images are ...\n",
      "üìù Reference Summary:\n",
      "The Duchess of Cambridge will feature on the cover of British Vogue to mark the magazine's\n",
      "centenary.\n",
      "üß† Generated Summary:\n",
      "\n",
      "\n",
      "üìä ROUGE Scores (F1):\n",
      "rouge1: 0.00\n",
      "rouge2: 0.00\n",
      "rougeL: 0.00\n",
      "rougeLsum: 0.00\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "import torch\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load fine-tuned model and tokenizer\n",
    "checkpoint_path = MODELS_PATH / \"xsum_bart_large\" / \"checkpoint-1125\"\n",
    "model = BartForConditionalGeneration.from_pretrained(checkpoint_path).to(device)\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "# Load dataset and evaluation subset\n",
    "val_data = load_dataset(\"xsum\", split=\"validation[:20]\")  # test on 20 samples\n",
    "\n",
    "# Load ROUGE\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "# Generate and collect predictions\n",
    "references = []\n",
    "predictions = []\n",
    "\n",
    "for example in val_data:\n",
    "    article = example[\"document\"]\n",
    "    reference = example[\"summary\"]\n",
    "    references.append(reference.strip())\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(article, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "\n",
    "    # Generate\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_length=64,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    generated = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    predictions.append(generated.strip())\n",
    "\n",
    "# Print a few examples\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"üìÑ Article:\\n{val_data[i]['document'][:300]}...\")\n",
    "    print(f\"üìù Reference Summary:\\n{references[i]}\")\n",
    "    print(f\"üß† Generated Summary:\\n{predictions[i]}\")\n",
    "\n",
    "# Compute and print ROUGE scores\n",
    "results = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "print(\"\\nüìä ROUGE Scores (F1):\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v*100:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7992b797",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìä Section 7: Evaluate Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9b2efa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Pretrained BART Results:\n",
      "{'eval_loss': 8.350750923156738, 'eval_model_preparation_time': 0.003, 'eval_rouge1_f1':\n",
      "42.87823639342292, 'eval_rouge2_f1': 16.809071657348294, 'eval_rougeL_f1': 39.68965887481013,\n",
      "'eval_rougeLsum_f1': 39.617765500714405, 'eval_runtime': 10.041, 'eval_samples_per_second': 19.918,\n",
      "'eval_steps_per_second': 2.49}\n"
     ]
    }
   ],
   "source": [
    "base_model = BartForConditionalGeneration.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "base_trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "base_results = base_trainer.evaluate()\n",
    "print(\"\\nüìä Pretrained BART Results:\")\n",
    "print(base_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5782bd2e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üëÅÔ∏è Section 8: Qualitative Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e796894b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Qualitative Comparison:\n",
      "\n",
      "Generated IDs: tensor([[    2,     0, 21169, 33270,  2359,     6,  2908,     6,    16, 13521,\n",
      "          1340,    19, 15172,  5396,   381,  7068,     6,  3550,     6, 15463,\n",
      "             6,   654,     8,  3259,     6,  3330,     4,    20,  1427, 16009,\n",
      "             7,  9971,    61,  2346,   362,   317,   227,  2266,     8,   777,\n",
      "             4,    20,  1931,    12, 43952,  5142,  2296, 15381,  1446,  1103,\n",
      "             4,     2]], device='cuda:0')\n",
      "Generated IDs: tensor([[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]], device='cuda:0')\n",
      "\n",
      "üìÑ Article 1:\n",
      "The ex-Reading defender denied fraudulent trading charges relating to the Sodje Sports Foundation -\n",
      "a charity to raise money for Nigerian sport.\n",
      "Mr Sodje, 37, is jointly charged with elder brothers Efe, 44, Bright, 50 and Stephen, 42.\n",
      "Appearing at the Old Bailey earlier, all four denied the offence.\n",
      "The charge relates to offences which allegedly took place between 2008 and 2014.\n",
      "Sam, from Kent, Efe and Bright, of Greater Manchester, and Stephen, from Bexley, are due to stand\n",
      "trial in July.\n",
      "They were all released on bail.\n",
      "üìù Reference:\n",
      "Former Premier League footballer Sam Sodje has appeared in court alongside three brothers accused of\n",
      "charity fraud.\n",
      "üìö BART Summary:\n",
      "Sam Sodje, 37, is jointly charged with elder brothers Efe, 44, Bright, 50 and Stephen, 42. The\n",
      "charge relates to offences which allegedly took place between 2008 and 2014. The ex-Reading defender\n",
      "denied fraudulent trading charges.\n",
      "üß† Fine-Tuned Summary:\n",
      "\n",
      "Length of fine-tuned summary: 0 words\n",
      "Generated IDs: tensor([[    2,     0,   448, 40741,  3463,  1034,     7,    33,  3086, 21723,\n",
      "           293,   124,    13,    49,   255,   844, 31610,   177,   136, 10372,\n",
      "            23,  5736,    18,    15,   155,   830,     4, 21723,   293,    21,\n",
      "          1654,     7,  7865,  2581,    15,  8162,    71,  3606,     5,  1356,\n",
      "           150,  8032,   148,     5,   413,  3261,  2451,    19, 24011,    15,\n",
      "           204,   502,     4,     2]], device='cuda:0')\n",
      "Generated IDs: tensor([[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]], device='cuda:0')\n",
      "\n",
      "üìÑ Article 2:\n",
      "Voges was forced to retire hurt on 86 after suffering the injury while batting during the County\n",
      "Championship draw with Somerset on 4 June.\n",
      "Middlesex hope to have the Australian back for their T20 Blast game against Hampshire at Lord's on 3\n",
      "August.\n",
      "The 37-year-old has scored 230 runs in four first-class games this season at an average of 57.50.\n",
      "\"Losing Adam is naturally a blow as he contributes significantly to everything we do,\" director of\n",
      "cricket Angus Fraser said.\n",
      "\"His absence, however, does give opportunities to other players who are desperate to play in the\n",
      "first XI.\n",
      "\"In the past we have coped well without an overseas player and I expect us to do so now.\"\n",
      "Defending county champions Middlesex are sixth in the Division One table, having drawn all four of\n",
      "their matches this season.\n",
      "Voges retired from international cricket in February with a Test batting average of 61.87 from 31\n",
      "innings, second only to Australian great Sir Donald Bradman's career average of 99.94 from 52 Tests.\n",
      "üìù Reference:\n",
      "Middlesex batsman Adam Voges will be out until August after suffering a torn calf muscle in his\n",
      "right leg.\n",
      "üìö BART Summary:\n",
      "Middlesex hope to have Adam Voges back for their T20 Blast game against Hampshire at Lord's on 3\n",
      "August. Voges was forced to retire hurt on 86 after suffering the injury while batting during the\n",
      "County Championship draw with Somerset on 4 June.\n",
      "üß† Fine-Tuned Summary:\n",
      "\n",
      "Length of fine-tuned summary: 0 words\n"
     ]
    }
   ],
   "source": [
    "def generate_summary(model, article):\n",
    "    inputs = tokenizer(article, return_tensors=\"pt\", max_length=512, truncation=True).to(model.device)\n",
    "    output_ids = model.generate(**inputs, max_length=64, min_length = 10, num_beams=4, early_stopping=True)\n",
    "    print(\"Generated IDs:\", output_ids) \n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nüîç Qualitative Comparison:\\n\")\n",
    "for i in range(2):\n",
    "    article = val_data[i]['document']\n",
    "    reference = val_data[i]['summary']\n",
    "    bart_sum = generate_summary(base_model, article)\n",
    "    fine_tuned_sum = generate_summary(fine_tuned_model, article)\n",
    "\n",
    "    print(f\"\\nüìÑ Article {i+1}:\\n{article}\")\n",
    "    print(f\"üìù Reference:\\n{reference}\")\n",
    "    print(f\"üìö BART Summary:\\n{bart_sum}\")\n",
    "    print(f\"üß† Fine-Tuned Summary:\\n{fine_tuned_sum}\")\n",
    "    print(f\"Length of fine-tuned summary: {len(fine_tuned_sum.split())} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c02300",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let me know if you'd like this turned into a downloadable `.ipynb`, modular scripts, or paired with a `requirements.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b08e5b",
   "metadata": {},
   "source": [
    "# üìù Exercise: Domain-Specific Summarization Using Fine-Tuned Models vs. General-Purpose LLMs\n",
    "\n",
    "## Overview\n",
    "In this exercise, you will fine-tune a specialized summarization model, `facebook/bart-large`, on the **PubMed** dataset to perform domain-specific summarization. You will then compare its performance to a general-purpose LLM (`gpt-4` or another open LLM like `LLaMA-2`) on the same task.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand the process of fine-tuning a pre-trained transformer model for domain-specific summarization.\n",
    "2. Compare performance of fine-tuned models to general-purpose LLMs.\n",
    "3. Use standard summarization metrics (e.g., ROUGE) to evaluate performance.\n",
    "4. Explore trade-offs between specialized and general-purpose models for summarization.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Fine-Tuning `facebook/bart-large` on PubMed Dataset\n",
    "\n",
    "### Setup\n",
    "Install necessary libraries:\n",
    "```bash\n",
    "pip install transformers datasets accelerate\n",
    "```\n",
    "\n",
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fe1f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"scientific_papers\", \"pubmed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dea9b6",
   "metadata": {},
   "source": [
    "### Fine-Tuning Code\n",
    "Follow the provided code snippet to fine-tune `facebook/bart-large` on a subset of the PubMed dataset.\n",
    "\n",
    "- Train on a subset (2000 samples).\n",
    "- Validate on a smaller subset (500 samples).\n",
    "- Use `fp16=True` for efficient training.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: Evaluation with ROUGE Metrics\n",
    "\n",
    "Add the following to your code after training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a22b1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "rouge = load_metric(\"rouge\")\n",
    "outputs = trainer.predict(tokenized_val)\n",
    "\n",
    "preds = tokenizer.batch_decode(outputs.predictions, skip_special_tokens=True)\n",
    "refs = [example['abstract'] for example in val_data]\n",
    "\n",
    "rouge_output = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
    "print(rouge_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4bc6c9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Comparison with General-Purpose LLM\n",
    "\n",
    "### Instructions\n",
    "1. Use an LLM (e.g., GPT-4 via API or LLaMA-2 locally) to generate summaries for the same validation set.\n",
    "2. Compare outputs using the same ROUGE metrics.\n",
    "\n",
    "### Prompt Example:\n",
    "\"Summarize the following scientific paper: \\n\\n{Article}\"\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4: Analysis & Discussion\n",
    "\n",
    "Answer the following questions:\n",
    "1. How do the ROUGE scores of the fine-tuned BART model compare to those of the general-purpose LLM?\n",
    "2. Are there qualitative differences in the summaries (e.g., coherence, conciseness, relevance)?\n",
    "3. What are the strengths and weaknesses of each approach?\n",
    "4. In what scenarios would you prefer using a fine-tuned model over a general-purpose LLM?\n",
    "\n",
    "---\n",
    "\n",
    "## Extension (Optional)\n",
    "- Fine-tune the model on the full PubMed dataset or another domain-specific dataset.\n",
    "- Compare the performance of BART, PEGASUS, and T5 for this task.\n",
    "- Explore different evaluation metrics (e.g., BERTScore).\n",
    "\n",
    "---\n",
    "\n",
    "## Submission\n",
    "Submit a Jupyter notebook containing:\n",
    "- Your fine-tuning code.\n",
    "- ROUGE metric outputs for both models.\n",
    "- Your analysis and discussion.\n",
    "\n",
    "---\n",
    "\n",
    "Good luck and have fun! üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c663bc99",
   "metadata": {},
   "source": [
    "# Assignment: Comparing General and Fine-Tuned BART Models\n",
    "\n",
    "## Objective\n",
    "In this assignment, you will compare the performance of two pretrained BART models on the task of scientific text summarization:\n",
    "- A general-domain BART model: `facebook/bart-large` (trained on CNN/DailyMail)\n",
    "- A domain-specific BART model: 'ccdv/lsg-bart-base-4096-pubmed' (fine-tuned on PubMed)\n",
    "\n",
    "You will evaluate the models both qualitatively (by examining the generated summaries) and quantitatively (using ROUGE scores).\n",
    "\n",
    "## Instructions\n",
    "\n",
    "### Part 1: Setup\n",
    "1. **Install the required packages**\n",
    "```bash\n",
    "%pip install transformers datasets rouge_score\n",
    "```\n",
    "\n",
    "2. **Import necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e379cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration, pipeline\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54037366",
   "metadata": {},
   "source": [
    "### Part 2: Load Models and Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6338dbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load general BART model\n",
    "general_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "general_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "general_summarizer = pipeline(\"summarization\", model=general_model, tokenizer=general_tokenizer)\n",
    "\n",
    "# Load domain-specific BART model\n",
    "pubmed_tokenizer = BartTokenizer.from_pretrained(\"ccdv/lsg-bart-base-4096-pubmed\")\n",
    "pubmed_model = BartForConditionalGeneration.from_pretrained(\"ccdv/lsg-bart-base-4096-pubmed\")\n",
    "\n",
    "domain_summarizer = pipeline(\"summarization\", model=pubmed_model, tokenizer=pubmed_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1d9052",
   "metadata": {},
   "source": [
    "### Part 3: Provide Scientific Text for Summarization\n",
    "Paste a sample scientific text below. It should be at least a few paragraphs long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61e161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "Your scientific text goes here.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b413f9",
   "metadata": {},
   "source": [
    "### Part 4: Generate Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d275ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries using both models\n",
    "general_summary = general_summarizer(sample_text, max_length=150, min_length=40, do_sample=False)[0]['summary_text']\n",
    "domain_summary = domain_summarizer(sample_text, max_length=150, min_length=40, do_sample=False)[0]['summary_text']\n",
    "\n",
    "print(\"General Model Summary:\\n\", general_summary)\n",
    "print(\"\\nDomain-Finetuned Model Summary:\\n\", domain_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a741c70",
   "metadata": {},
   "source": [
    "### Part 5: Evaluate using ROUGE Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafea6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Initialize the ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Provide a reference summary for comparison\n",
    "reference_summary = \"\"\"\n",
    "Your reference summary goes here.\n",
    "\"\"\"\n",
    "\n",
    "# Compute ROUGE scores for each model\n",
    "scores_general = scorer.score(reference_summary, general_summary)\n",
    "scores_domain = scorer.score(reference_summary, domain_summary)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nGeneral Model ROUGE Scores:\")\n",
    "for metric, score in scores_general.items():\n",
    "    print(f\"{metric}: {score}\")\n",
    "\n",
    "print(\"\\nDomain Model ROUGE Scores:\")\n",
    "for metric, score in scores_domain.items():\n",
    "    print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0458bdd9",
   "metadata": {},
   "source": [
    "### Part 6: Analysis\n",
    "1. **Qualitative Comparison**\n",
    "    - Compare the outputs of the general and domain-specific models. What differences do you notice? Which model produces more coherent, relevant, or precise summaries?\n",
    "\n",
    "2. **Quantitative Comparison**\n",
    "    - Compare the ROUGE scores from both models. Which model achieves higher scores? Does this align with your qualitative observations?\n",
    "\n",
    "3. **Discussion**\n",
    "    - Discuss why the domain-specific model might perform better on PubMed data. What trade-offs might exist between using a general-purpose model vs. a fine-tuned model?\n",
    "\n",
    "### Submission\n",
    "Save your notebook and submit it according to your instructor‚Äôs guidelines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776_env_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
