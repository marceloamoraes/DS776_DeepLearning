{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Comparison: API-Based vs. Local Models\n",
    "\n",
    "### This notebook demonstrates how to interact with both API-based and local models using a simple ConversationManager class.\n",
    "### Students will compare responses from OpenAI GPT-4 and a local model (using HuggingFace Transformers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODELS_PATH=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\models\n",
      "DATA_PATH=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\data\n",
      "TORCH_HOME=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\n",
      "HF_HOME=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\n",
      "HF_HUB_CACHE=C:\\Users\\bagge\\My Drive\\Python_Projects\\DS776_Develop_Project\\downloads\n",
      "Successfully logged in to Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from transformers import pipeline\n",
    "\n",
    "from introdl.utils import config_paths_keys\n",
    "paths = config_paths_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationManager:\n",
    "    def __init__(self, system_prompt: str = \"You are a helpful assistant.\"):\n",
    "        self.history = []\n",
    "        self.system_prompt = system_prompt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the conversation history.\"\"\"\n",
    "        self.history = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "\n",
    "    def add_user_message(self, message: str):\n",
    "        \"\"\"Adds a user message to the conversation history.\"\"\"\n",
    "        self.history.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    def add_assistant_message(self, message: str):\n",
    "        \"\"\"Adds an assistant message to the conversation history.\"\"\"\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": message})\n",
    "\n",
    "    def get_history(self):\n",
    "        \"\"\"Returns the conversation history formatted for an API or LLM call.\"\"\"\n",
    "        return self.history\n",
    "\n",
    "    def get_formatted_history(self):\n",
    "        \"\"\"Returns the conversation history as a formatted string for local models.\"\"\"\n",
    "        return \"\\n\".join([f\"{msg['role'].capitalize()}: {msg['content']}\" for msg in self.history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the conversation manager\n",
    "conversation = ConversationManager(system_prompt=\"You are a helpful tutor for a deep learning class.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API-Based Model Interaction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Gemini API client\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "def interact_with_api(conversation: ConversationManager, **kwargs):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gemini-2.0-flash-lite\",\n",
    "        messages=conversation.get_history(), **kwargs\n",
    "    )\n",
    "    message = response.choices[0].message.content\n",
    "    conversation.add_assistant_message(message)\n",
    "    return message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Model Interaction (HuggingFace GPT-2) with Configurable Decoding Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer separately for better control over generation\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "local_model_name = \"unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(local_model_name, device_map=\"auto\")\n",
    "model.eval();  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_with_local_model(conversation: ConversationManager, split_str='Assistant:', **kwargs):\n",
    "    if hasattr(tokenizer, \"apply_chat_template\") and getattr(tokenizer, \"chat_template\", None) is not None:\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            conversation.get_history(),\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        ).to(model.device)\n",
    "    else:\n",
    "        prompt = conversation.get_formatted_history() + 'Assistant:'\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(inputs, **kwargs)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    message = response.split(split_str)[-1].strip() if split_str in response else response\n",
    "    conversation.add_assistant_message(message)\n",
    "    return message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Interaction (API-based)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "API Response:\n",
      " Okay, let's break down overfitting in the context of deep learning. Imagine you're trying to learn a new skill, like riding a bike.\n",
      "\n",
      "**Overfitting: Learning the Noise, Not the Signal**\n",
      "\n",
      "Overfitting is a common problem in machine learning, including deep learning. It happens when a model learns the training data *too well*, including the noise and random fluctuations present in that data.  Think of it like memorizing specific answers to practice questions instead of understanding the underlying concepts.\n",
      "\n",
      "Here's a more detailed explanation:\n",
      "\n",
      "*   **Training Data:** This is the dataset the model uses to learn.\n",
      "*   **Noise:**  Real-world data often has errors, inconsistencies, and random variations. This is noise. It might be due to measurement errors, irrelevant features, or simply random chance.\n",
      "*   **Signal:** The underlying patterns, relationships, and true trends that the data represents.  This is what we *want* the model to learn.\n",
      "\n",
      "**What Happens During Overfitting?**\n",
      "\n",
      "1.  **Excellent Performance on Training Data:** The overfitted model will achieve very high accuracy or low error on the training data because it has essentially memorized it.\n",
      "2.  **Poor Performance on New, Unseen Data (Testing Data):**  When presented with new data that the model hasn't seen before (the \"testing data\"), its performance will be significantly worse. This is because the model has learned the specific details of the training data, including the noise, which doesn't generalize well to new examples.\n",
      "3.  **Focus on Unimportant Features:** An overfitted model might latch onto irrelevant features in the training data and give them undue importance in its predictions.\n",
      "\n",
      "**Analogy: The Student and the Exam**\n",
      "\n",
      "Imagine a student studying for an exam.\n",
      "\n",
      "*   **Good Learning (Not Overfitting):** The student understands the fundamental concepts, principles, and problem-solving approaches. They can apply this knowledge to a variety of problems, even ones they haven't seen before.  They get a good score on both practice questions and the actual exam.\n",
      "*   **Overfitting:** The student only memorizes the specific questions and answers from a practice exam. They do well on the practice exam but struggles on the actual exam because they don't understand the underlying concepts. The student learned the *specific* details, including the typos, and the particular order of the questions.\n",
      "\n",
      "**Why is Overfitting a Problem?**\n",
      "\n",
      "The goal of machine learning is to create models that can generalize well to *unseen* data. We want a model that can make accurate predictions on new examples, not just on the data it was trained on. Overfitting prevents this generalization. If your model overfits, it's not truly learning the underlying patterns; it's simply memorizing the training data.\n",
      "\n",
      "**How to Detect Overfitting:**\n",
      "\n",
      "*   **Compare Training and Validation/Test Performance:** The most common way to detect overfitting.\n",
      "    *   Train your model on a training set.\n",
      "    *   Evaluate the model's performance on a separate validation set (or test set) that was *not* used during training.\n",
      "    *   If the model's performance is much better on the training set than on the validation/test set, overfitting is likely occurring.\n",
      "    *   Plot the training and validation/test performance (e.g., accuracy, loss) over time (epochs) to visualize how they change during training. You'll often see the training performance continue to improve while the validation performance plateaus or even gets worse, which is a sign of overfitting.\n",
      "\n",
      "**How to Prevent or Mitigate Overfitting:**\n",
      "\n",
      "There are many techniques to combat overfitting:\n",
      "\n",
      "*   **More Data:** The more training data you have, the harder it is for the model to memorize the noise.  More data helps the model learn the true underlying patterns.\n",
      "*   **Data Augmentation:** Artificially increase the size of your training dataset by creating modified versions of your existing data (e.g., rotating images, adding noise).\n",
      "*   **Simpler Model:** Use a model with fewer parameters (e.g., fewer layers, fewer neurons per layer).  A simpler model has less capacity to memorize the noise.\n",
      "*   **Regularization:** Add a penalty term to the model's loss function that discourages complex models. Common types of regularization include:\n",
      "    *   **L1 Regularization (Lasso):** Adds a penalty proportional to the absolute value of the weights.  Can drive some weights to zero, effectively removing irrelevant features.\n",
      "    *   **L2 Regularization (Ridge):** Adds a penalty proportional to the square of the weights.  Shrinks the weights towards zero, making the model less sensitive to individual features.\n",
      "*   **Dropout:** Randomly \"drop out\" (deactivate) some neurons during training. This prevents the model from relying too heavily on any single neuron and forces it to learn more robust features.\n",
      "*   **Early Stopping:** Monitor the model's performance on a validation set during training. Stop training when the performance on the validation set starts to decrease, even if the training performance is still improving.\n",
      "*   **Batch Normalization:** Normalize the activations of each layer. This helps to stabilize the training process and can act as a form of regularization.\n",
      "\n",
      "Let me know if you'd like me to elaborate on any of these aspects or provide examples!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conversation.add_user_message(\"What is overfitting in deep learning?\")\n",
    "api_response = interact_with_api(conversation)\n",
    "print(\"\\nAPI Response:\\n\", api_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Interaction (Local Model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Local Model Response:\n",
      " system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 05 Apr 2025\n",
      "\n",
      "You are a helpful tutor for a deep learning class.user\n",
      "\n",
      "What is overfitting in deep learning?assistant\n",
      "\n",
      "Overfitting is a common problem in deep learning where a model becomes too specialized to the training data and fails to generalize well to new, unseen data. This occurs when the model is too complex and has too many parameters, causing it to fit the noise and patterns in the training data too closely.\n",
      "\n",
      "In other words, overfitting happens when a model is too good at fitting the training data, but poorly at fitting the underlying data distribution. This results in poor performance on new, unseen data, which can be similar to the training data but not identical.\n",
      "\n",
      "There are several causes of overfitting:\n",
      "\n",
      "1. **Model complexity**: Models with too many parameters or layers can fit the training data too closely, leading to overfitting.\n",
      "2. **Insufficient training data**: If the training data is too small, the model may not have enough information to learn the underlying patterns, leading to overfitting.\n",
      "3. **Poor regularization**: Regularization techniques, such as L1 and L2 regularization, can help prevent overfitting by penalizing large weights, but if not used properly, they can also prevent the model from learning the underlying patterns.\n",
      "4. **Overparameterization**: When the number of parameters in the model exceeds the number of training examples, the model can become overconfident and overfit the training data.\n",
      "\n",
      "Symptoms of overfitting include:\n",
      "\n",
      "1. **High training accuracy**: The model performs well on the training data, but poorly on new, unseen data.\n",
      "2. **High training loss**: The model's loss function decreases during training, but the validation loss (on unseen data) increases.\n",
      "3. **Large weights**: The model's weights are large, indicating that the model is trying to fit the training data too closely.\n",
      "\n",
      "To prevent overfitting, you can try:\n",
      "\n",
      "1. **Regularization**: Use regularization techniques, such as L1 and L2 regularization, dropout, or early stopping.\n",
      "2. **Data augmentation**: Increase the size of the training data by applying transformations, such as rotation, scaling, or flipping.\n",
      "3. **Early stopping**: Stop training the model when the validation loss stops improving.\n",
      "4. **Ensemble methods**: Combine the predictions of multiple models to improve overall performance.\n",
      "5. **Model pruning**: Remove unnecessary weights or layers to reduce model complexity.\n",
      "\n",
      "By understanding overfitting and using these techniques, you can develop more robust and generalizable deep learning models.\n"
     ]
    }
   ],
   "source": [
    "conversation.reset()\n",
    "conversation.add_user_message(\"What is overfitting in deep learning?\")\n",
    "local_response = interact_with_local_model(conversation)\n",
    "print(\"\\nLocal Model Response:\\n\", local_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Students will try different prompts and compare the responses from both models.\n",
    "### They will also examine how the conversation history affects the responses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bagge\\miniforge-pypy3\\envs\\DS776env\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bagge\\miniforge-pypy3\\envs\\DS776env\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Generated Text</th>\n",
       "      <th>BLEU Score</th>\n",
       "      <th>ROUGE-1</th>\n",
       "      <th>ROUGE-2</th>\n",
       "      <th>ROUGE-L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Greedy</th>\n",
       "      <td>Explain the difference between supervised and unsupervised learning. In the context of machine learning, supervised learning is used to predict outcomes based on labeled data, while unsupervised learning is used to identify patterns and relationships in unlabeled data.\\nIn the context of machine learning, supervised learning is used to predict outcomes based on labeled data, while unsupervised learning is used to identify patterns and relationships in unlabeled data. The key difference between the two is the type of data used to train the model.\\n\\n**Supervised Learning:**\\n\\nIn supervised learning, the model is trained on labeled data, where each example is associated with a target output. The goal is to learn a mapping between inputs and outputs, so the model can make predictions on new, unseen data. The model is trained to minimize the difference between its predictions and the actual outputs.\\n\\n**Unsupervised Learning:**\\n\\nIn unsupervised learning, the model is trained on unlabeled data, where there is no target output. The</td>\n",
       "      <td>0.0264</td>\n",
       "      <td>0.2054</td>\n",
       "      <td>0.0874</td>\n",
       "      <td>0.1730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top-K</th>\n",
       "      <td>Explain the difference between supervised and unsupervised learning. \\nSupervised learning is a type of machine learning where the data is labeled or classified into predefined categories. In this type of learning, the algorithm is trained on labeled data to learn the relationships between the input and output variables. The goal of supervised learning is to make predictions on new, unseen data.\\n\\nUnsupervised learning, on the other hand, is a type of machine learning where the data is not labeled or classified into predefined categories. In this type of learning, the algorithm is trained on unlabeled data to identify patterns, relationships, or groupings within the data. The goal of unsupervised learning is to discover hidden structures or patterns in the data that may not be immediately apparent.\\n\\nHere is an example to illustrate the difference:\\n\\nSupervised learning:\\n\\n* A company wants to predict whether a customer will buy a product based on their age, income, and other demographic information.\\n* The company has labeled data with the outcome (</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.0957</td>\n",
       "      <td>0.1684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top-P</th>\n",
       "      <td>Explain the difference between supervised and unsupervised learning. In the context of the dataset provided in the problem you'll be working on, the dataset has 3 features: x1, x2, and x3. The target variable y is the number of days it takes to complete a task. The goal is to predict the number of days to complete a task based on the input features x1, x2, and x3.\\n\\n## Step 1: Understand the Basics of Supervised and Unsupervised Learning\\nSupervised learning involves training a model on labeled data, where the correct output is already known. The goal is to learn a mapping between inputs and outputs, so the model can make predictions on new, unseen data. Unsupervised learning, on the other hand, involves training a model on unlabeled data, where the correct output is not known. The goal is to discover patterns or structure in the data.\\n\\n## Step 2: Apply Supervised Learning to the</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.2011</td>\n",
       "      <td>0.0904</td>\n",
       "      <td>0.1788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beam Search</th>\n",
       "      <td>Explain the difference between supervised and unsupervised learning. \\n\\n**Supervised Learning**\\n======================\\n\\nIn supervised learning, the algorithm is trained on labeled data, where each example is accompanied by a target or response variable. The goal is to learn a mapping between input data and output labels, so the algorithm can make predictions on new, unseen data.\\n\\n**Key Characteristics:**\\n\\n*   The algorithm is trained on labeled data.\\n*   The algorithm learns a mapping between input data and output labels.\\n*   The goal is to make predictions on new, unseen data.\\n\\n**Example Use Cases:**\\n\\n*   Image classification (e.g., classifying images as \"dog\" or \"cat\")\\n*   Sentiment analysis (e.g., classifying text as \"positive\" or \"negative\")\\n*   Regression (e.g., predicting continuous values, such as house prices)\\n\\n**Unsupervised Learning**\\n=====================\\n\\nIn unsupervised learning, the algorithm is trained on unlabeled data,</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.2208</td>\n",
       "      <td>0.0921</td>\n",
       "      <td>0.1948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"Explain the difference between supervised and unsupervised learning.\"\n",
    "\n",
    "# Encode the prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "max_length = 200  # Maximum length of the generated text\n",
    "\n",
    "# Generate text using different decoding strategies\n",
    "with torch.no_grad():\n",
    "    greedy_output = model.generate(**inputs, max_length=max_length, do_sample=False)\n",
    "    top_k_output = model.generate(**inputs, max_length=max_length, top_k=50)\n",
    "    top_p_output = model.generate(**inputs, max_length=max_length, top_p=0.9)\n",
    "    beam_output = model.generate(**inputs, max_length=max_length, num_beams=5)\n",
    "\n",
    "# Decode the outputs\n",
    "generated_texts = {\n",
    "    \"Greedy\": tokenizer.decode(greedy_output[0], skip_special_tokens=True),\n",
    "    \"Top-K\": tokenizer.decode(top_k_output[0], skip_special_tokens=True),\n",
    "    \"Top-P\": tokenizer.decode(top_p_output[0], skip_special_tokens=True),\n",
    "    \"Beam Search\": tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
    "}\n",
    "\n",
    "# Reference text (ground truth)\n",
    "reference = [\n",
    "    \"Supervised learning uses labeled data to learn a mapping from inputs to outputs, while unsupervised learning tries to find patterns or groupings within unlabeled data.\"\n",
    "]\n",
    "\n",
    "# Calculate BLEU and ROUGE scores\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "results = {}\n",
    "\n",
    "for strategy, text in generated_texts.items():\n",
    "    bleu_score = sentence_bleu([reference[0].split()], text.split())\n",
    "    rouge_scores = scorer.score(reference[0], text)\n",
    "    results[strategy] = {\n",
    "        \"Generated Text\": text,\n",
    "        \"BLEU Score\": bleu_score,\n",
    "        \"ROUGE-1\": rouge_scores['rouge1'].fmeasure,\n",
    "        \"ROUGE-2\": rouge_scores['rouge2'].fmeasure,\n",
    "        \"ROUGE-L\": rouge_scores['rougeL'].fmeasure,\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(results, orient='index')\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(df.to_html(float_format=\"%.4f\", justify=\"center\", index=True, border=0, classes='dataframe')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Generated Text</th>\n",
       "      <th>BLEU Score</th>\n",
       "      <th>ROUGE-1</th>\n",
       "      <th>ROUGE-2</th>\n",
       "      <th>ROUGE-L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Greedy</th>\n",
       "      <td>In supervised learning, the algorithm is trained on labeled data, where each example is associated with a target output. The goal is to learn a mapping between inputs and outputs, so the algorithm can make predictions on new, unseen data. Here's an example: : A picture of a cat : A label indicating whether the picture is of a cat (yes or no</td>\n",
       "      <td>0.0441</td>\n",
       "      <td>0.2759</td>\n",
       "      <td>0.1176</td>\n",
       "      <td>0.2529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top-K</th>\n",
       "      <td>In machine learning, supervised learning is used to predict the output of a target variable, whereas unsupervised learning is used to identify patterns and relationships in data without a target variable. In supervised learning, the algorithm is trained on labeled data, where each example is associated with a target variable. The goal is to learn a mapping between input features and target variables. The algorithm learns to predict the output of the target variable based on the input</td>\n",
       "      <td>0.0409</td>\n",
       "      <td>0.3301</td>\n",
       "      <td>0.1188</td>\n",
       "      <td>0.2330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top-P</th>\n",
       "      <td>Supervised learning involves training a model on labeled data, where the model is given a set of input examples, each labeled with a target output. The goal is to learn a mapping between inputs and outputs, so the model can make predictions on new, unseen data. Unsupervised learning, on the other hand, involves training a model on unlabeled data, where the goal is to identify patterns or structure in the data.</td>\n",
       "      <td>0.0495</td>\n",
       "      <td>0.3542</td>\n",
       "      <td>0.1702</td>\n",
       "      <td>0.3333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beam Search</th>\n",
       "      <td>In supervised learning, the algorithm is trained on labeled data, where each example is accompanied by a target or response variable. The goal is to learn a mapping between input data and output labels, so the algorithm can make predictions on new, unseen data. In unsupervised learning, the algorithm is trained on unlabeled data, and the goal is</td>\n",
       "      <td>0.0493</td>\n",
       "      <td>0.3614</td>\n",
       "      <td>0.1728</td>\n",
       "      <td>0.3373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Complete Notebook Demonstration: Decoding Strategy Evaluation with Text Cleaning\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "def clean_output(text, prompt=\"\"):\n",
    "    \"\"\"\n",
    "    Cleans the model-generated text by removing the prompt, formatting, and common prefixes/suffixes.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The generated text from the model.\n",
    "        prompt (str): The prompt used for generating the text. If present at the beginning, it will be removed.\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text ready for evaluation.\n",
    "    \"\"\"\n",
    "    # Remove the prompt if it exists at the beginning of the text\n",
    "    if text.startswith(prompt):\n",
    "        text = text[len(prompt):].strip()\n",
    "    \n",
    "    # Remove markdown headers, formatting, and common prefixes\n",
    "    unwanted_prefixes = [\n",
    "        \"The answer is:\", \"Here is the explanation:\", \n",
    "        \"In conclusion,\", \"To summarize,\", \"As follows:\"\n",
    "    ]\n",
    "    for prefix in unwanted_prefixes:\n",
    "        if text.startswith(prefix):\n",
    "            text = text[len(prefix):].strip()\n",
    "    \n",
    "    # Remove markdown-like formatting (titles, headers, etc.)\n",
    "    text = re.sub(r\"\\*\\*.*?\\*\\*|=+\", \"\", text)\n",
    "    \n",
    "    # Remove bullet points, numbers, dashes, and unwanted newlines\n",
    "    text = re.sub(r\"(\\*|-|•|\\d+\\.)\\s\", \"\", text)\n",
    "    text = re.sub(r\"\\n+\", \" \", text)\n",
    "    \n",
    "    # Remove anything that's not alphanumeric, standard punctuation, or whitespace\n",
    "    text = re.sub(r\"[^a-zA-Z0-9.,!?;:\\-()\\'\\\"\\s]\", \"\", text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"Explain the difference between supervised and unsupervised learning.\"\n",
    "\n",
    "# Encode the prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate text using different decoding strategies\n",
    "with torch.no_grad():\n",
    "    greedy_output = model.generate(**inputs, max_length=100)\n",
    "    top_k_output = model.generate(**inputs, max_length=100, top_k=50)\n",
    "    top_p_output = model.generate(**inputs, max_length=100, top_p=0.9)\n",
    "    beam_output = model.generate(**inputs, max_length=100, num_beams=5)\n",
    "\n",
    "# Decode the outputs\n",
    "generated_texts = {\n",
    "    \"Greedy\": tokenizer.decode(greedy_output[0], skip_special_tokens=True),\n",
    "    \"Top-K\": tokenizer.decode(top_k_output[0], skip_special_tokens=True),\n",
    "    \"Top-P\": tokenizer.decode(top_p_output[0], skip_special_tokens=True),\n",
    "    \"Beam Search\": tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
    "}\n",
    "\n",
    "# Clean all generated texts\n",
    "cleaned_texts = {strategy: clean_output(text, prompt) for strategy, text in generated_texts.items()}\n",
    "\n",
    "# Reference text (ground truth)\n",
    "reference = [\n",
    "    \"Supervised learning uses labeled data to learn a mapping from inputs to outputs, while unsupervised learning tries to find patterns or groupings within unlabeled data.\"\n",
    "]\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Calculate BLEU and ROUGE scores\n",
    "results = {}\n",
    "for strategy, text in cleaned_texts.items():\n",
    "    bleu_score = sentence_bleu([reference[0].split()], text.split())\n",
    "    rouge_scores = scorer.score(reference[0], text)\n",
    "    results[strategy] = {\n",
    "        \"Generated Text\": text,\n",
    "        \"BLEU Score\": bleu_score,\n",
    "        \"ROUGE-1\": rouge_scores['rouge1'].fmeasure,\n",
    "        \"ROUGE-2\": rouge_scores['rouge2'].fmeasure,\n",
    "        \"ROUGE-L\": rouge_scores['rougeL'].fmeasure,\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "dataframe=pd.DataFrame.from_dict(results, orient='index')\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(dataframe.to_html(float_format=\"%.4f\", justify=\"center\", index=True, border=0, classes='dataframe')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\bagge\\my drive\\python_projects\\ds776_develop_project\\ds776\\lessons\\course_tools\\introdl\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: accelerate in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (1.5.1)\n",
      "Collecting bert_score (from introdl==1.0)\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: bertviz in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (1.4.0)\n",
      "Requirement already satisfied: bitsandbytes>=0.45.3 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (0.45.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (3.3.2)\n",
      "Requirement already satisfied: evaluate in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (0.4.3)\n",
      "Requirement already satisfied: gliner in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (0.2.17)\n",
      "Requirement already satisfied: google-genai in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (1.5.0)\n",
      "Requirement already satisfied: IPython in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (9.0.2)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (8.1.5)\n",
      "Requirement already satisfied: ipycanvas in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (0.13.3)\n",
      "Requirement already satisfied: kagglehub in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (0.3.10)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (3.10.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (2.1.1)\n",
      "Requirement already satisfied: openai>=1.65.5 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (1.69.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (2.2.3)\n",
      "Requirement already satisfied: protobuf in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (5.29.4)\n",
      "Requirement already satisfied: pyperclip in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (1.9.0)\n",
      "Requirement already satisfied: rapidfuzz in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (3.12.2)\n",
      "Requirement already satisfied: rouge_score in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (0.1.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (1.15.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (0.13.2)\n",
      "Requirement already satisfied: segmentation_models_pytorch in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (0.4.0)\n",
      "Requirement already satisfied: seqeval in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (1.2.2)\n",
      "Requirement already satisfied: spacy in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (3.8.4)\n",
      "Requirement already satisfied: timm in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (1.0.15)\n",
      "Requirement already satisfied: torch in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchinfo in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (1.8.0)\n",
      "Requirement already satisfied: torchmetrics in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (1.6.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (0.21.0+cu124)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (4.67.1)\n",
      "Requirement already satisfied: transformers>=4.40.9 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (4.49.0)\n",
      "Requirement already satisfied: ultralytics in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from introdl==1.0) (8.3.89)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from openai>=1.65.5->introdl==1.0) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from openai>=1.65.5->introdl==1.0) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from openai>=1.65.5->introdl==1.0) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from openai>=1.65.5->introdl==1.0) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from openai>=1.65.5->introdl==1.0) (2.11.0b1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from openai>=1.65.5->introdl==1.0) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from openai>=1.65.5->introdl==1.0) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from torch->introdl==1.0) (3.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from torch->introdl==1.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from torch->introdl==1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from torch->introdl==1.0) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from torch->introdl==1.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from sympy==1.13.1->torch->introdl==1.0) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from tqdm->introdl==1.0) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from transformers>=4.40.9->introdl==1.0) (0.29.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from transformers>=4.40.9->introdl==1.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from transformers>=4.40.9->introdl==1.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from transformers>=4.40.9->introdl==1.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from transformers>=4.40.9->introdl==1.0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from transformers>=4.40.9->introdl==1.0) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from transformers>=4.40.9->introdl==1.0) (0.5.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from accelerate->introdl==1.0) (7.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from pandas->introdl==1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from pandas->introdl==1.0) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from pandas->introdl==1.0) (2025.1)\n",
      "Requirement already satisfied: boto3 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from bertviz->introdl==1.0) (1.37.13)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from bertviz->introdl==1.0) (0.2.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from datasets->introdl==1.0) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from datasets->introdl==1.0) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from datasets->introdl==1.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from datasets->introdl==1.0) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from datasets->introdl==1.0) (3.11.13)\n",
      "Requirement already satisfied: onnxruntime in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from gliner->introdl==1.0) (1.21.0)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from google-genai->introdl==1.0) (2.38.0)\n",
      "Requirement already satisfied: websockets<15.0dev,>=13.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from google-genai->introdl==1.0) (14.2)\n",
      "Requirement already satisfied: pillow>=6.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from ipycanvas->introdl==1.0) (11.0.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from ipywidgets->introdl==1.0) (0.2.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from ipywidgets->introdl==1.0) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from ipywidgets->introdl==1.0) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from ipywidgets->introdl==1.0) (3.0.13)\n",
      "Requirement already satisfied: decorator in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from IPython->introdl==1.0) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from IPython->introdl==1.0) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from IPython->introdl==1.0) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from IPython->introdl==1.0) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from IPython->introdl==1.0) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from IPython->introdl==1.0) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from IPython->introdl==1.0) (0.6.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from matplotlib->introdl==1.0) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from matplotlib->introdl==1.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from matplotlib->introdl==1.0) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from matplotlib->introdl==1.0) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from matplotlib->introdl==1.0) (3.2.1)\n",
      "Requirement already satisfied: click in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from nltk->introdl==1.0) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from nltk->introdl==1.0) (1.4.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from rouge_score->introdl==1.0) (2.2.2)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from rouge_score->introdl==1.0) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from scikit-learn->introdl==1.0) (3.6.0)\n",
      "Requirement already satisfied: efficientnet-pytorch>=0.6.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from segmentation_models_pytorch->introdl==1.0) (0.7.1)\n",
      "Requirement already satisfied: pretrainedmodels>=0.7.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from segmentation_models_pytorch->introdl==1.0) (0.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from spacy->introdl==1.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from spacy->introdl==1.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from spacy->introdl==1.0) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from spacy->introdl==1.0) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from spacy->introdl==1.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from spacy->introdl==1.0) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from spacy->introdl==1.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from spacy->introdl==1.0) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from spacy->introdl==1.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from spacy->introdl==1.0) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from spacy->introdl==1.0) (0.15.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from spacy->introdl==1.0) (78.0.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from spacy->introdl==1.0) (3.5.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from torchmetrics->introdl==1.0) (0.14.0)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from ultralytics->introdl==1.0) (4.11.0.86)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from ultralytics->introdl==1.0) (9.0.0)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from ultralytics->introdl==1.0) (2.0.14)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from anyio<5,>=3.5.0->openai>=1.65.5->introdl==1.0) (3.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from aiohttp->datasets->introdl==1.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from aiohttp->datasets->introdl==1.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from aiohttp->datasets->introdl==1.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from aiohttp->datasets->introdl==1.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from aiohttp->datasets->introdl==1.0) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from aiohttp->datasets->introdl==1.0) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from aiohttp->datasets->introdl==1.0) (1.18.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai->introdl==1.0) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai->introdl==1.0) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai->introdl==1.0) (4.9)\n",
      "Requirement already satisfied: certifi in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.65.5->introdl==1.0) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.65.5->introdl==1.0) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.65.5->introdl==1.0) (0.14.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from jedi>=0.16->IPython->introdl==1.0) (0.8.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy->introdl==1.0) (1.3.0)\n",
      "Requirement already satisfied: munch in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from pretrainedmodels>=0.7.1->segmentation_models_pytorch->introdl==1.0) (4.0.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->IPython->introdl==1.0) (0.2.13)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from pydantic<3,>=1.9.0->openai>=1.65.5->introdl==1.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.31.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from pydantic<3,>=1.9.0->openai>=1.65.5->introdl==1.0) (2.31.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from pydantic<3,>=1.9.0->openai>=1.65.5->introdl==1.0) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from requests->transformers>=4.40.9->introdl==1.0) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from requests->transformers>=4.40.9->introdl==1.0) (2.3.0)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy->introdl==1.0) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy->introdl==1.0) (0.1.5)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy->introdl==1.0) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy->introdl==1.0) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy->introdl==1.0) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy->introdl==1.0) (7.1.0)\n",
      "Requirement already satisfied: botocore<1.38.0,>=1.37.13 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from boto3->bertviz->introdl==1.0) (1.37.13)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from boto3->bertviz->introdl==1.0) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from boto3->bertviz->introdl==1.0) (0.11.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from jinja2->torch->introdl==1.0) (2.1.5)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from onnxruntime->gliner->introdl==1.0) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from onnxruntime->gliner->introdl==1.0) (25.2.10)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from stack_data->IPython->introdl==1.0) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from stack_data->IPython->introdl==1.0) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from stack_data->IPython->introdl==1.0) (0.2.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->introdl==1.0) (1.2.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-genai->introdl==1.0) (0.6.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->introdl==1.0) (3.0.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->introdl==1.0) (1.17.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from coloredlogs->onnxruntime->gliner->introdl==1.0) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime->gliner->introdl==1.0) (3.5.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\bagge\\miniforge-pypy3\\envs\\ds776env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->introdl==1.0) (0.1.2)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Building wheels for collected packages: introdl\n",
      "  Building wheel for introdl (pyproject.toml): started\n",
      "  Building wheel for introdl (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for introdl: filename=introdl-1.0-py3-none-any.whl size=46579 sha256=aff3cbdb4b46b3db49cfeea6faafd5deaaae3a6c954977ba51a9616ed63d0040\n",
      "  Stored in directory: C:\\Users\\bagge\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-d5xz68e3\\wheels\\f5\\d5\\0f\\11f1d5af64d00defb23fa33cf51b2946a0899888d73571e687\n",
      "Successfully built introdl\n",
      "Installing collected packages: bert_score, introdl\n",
      "  Attempting uninstall: introdl\n",
      "    Found existing installation: introdl 1.0\n",
      "    Uninstalling introdl-1.0:\n",
      "      Successfully uninstalled introdl-1.0\n",
      "Successfully installed bert_score-0.3.13 introdl-1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install ../Course_Tools/introdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Generated Text</th>\n",
       "      <th>BERTScore P</th>\n",
       "      <th>BERTScore R</th>\n",
       "      <th>BERTScore F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Greedy</th>\n",
       "      <td>In the context of a classification problem. In the classification problem, the data is split into two classes: 0 and Class 0 represents a normal state, while Class 1 represents a faulty state. The goal is to train a model to predict the class of a new, unseen input. In supervised learning, the model is trained on labeled data, where each example is associated with a target</td>\n",
       "      <td>0.5546</td>\n",
       "      <td>0.6315</td>\n",
       "      <td>0.5906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top-K</th>\n",
       "      <td>Supervised learning involves training a model on labeled data, where the model learns to predict the target variable based on the input features. In contrast, unsupervised learning involves training a model on unlabeled data, where the model learns to identify patterns or relationships in the data without a target variable. In the context of natural language processing, supervised learning is often used for tasks such as text classification, sentiment analysis, and machine translation. These</td>\n",
       "      <td>0.6513</td>\n",
       "      <td>0.8146</td>\n",
       "      <td>0.7238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top-P</th>\n",
       "      <td>In the context of machine learning, supervised learning is used to predict outcomes, while unsupervised learning is used to identify patterns. Step 1: Define Supervised Learning Supervised learning is a type of machine learning where the algorithm is trained on labeled data. This means that the data is already classified or labeled with the correct output, and the goal is to learn a mapping between inputs and outputs. Step 2:</td>\n",
       "      <td>0.6209</td>\n",
       "      <td>0.7607</td>\n",
       "      <td>0.6837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beam Search</th>\n",
       "      <td>In supervised learning, the algorithm is trained on labeled data, where each example is accompanied by a target or response variable. The goal is to learn a mapping between input data and output labels, so the algorithm can make predictions on new, unseen data. In unsupervised learning, the algorithm is trained on unlabeled data, and the goal is</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>0.7898</td>\n",
       "      <td>0.7330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Complete Notebook Demonstration: Decoding Strategy Evaluation with Text Cleaning\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from bert_score import score\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "def clean_output(text, prompt=\"\"):\n",
    "    \"\"\"\n",
    "    Cleans the model-generated text by removing the prompt, formatting, and common prefixes/suffixes.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The generated text from the model.\n",
    "        prompt (str): The prompt used for generating the text. If present at the beginning, it will be removed.\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text ready for evaluation.\n",
    "    \"\"\"\n",
    "    # Remove the prompt if it exists at the beginning of the text\n",
    "    if text.startswith(prompt):\n",
    "        text = text[len(prompt):].strip()\n",
    "    \n",
    "    # Remove markdown headers, formatting, and common prefixes\n",
    "    unwanted_prefixes = [\n",
    "        \"The answer is:\", \"Here is the explanation:\", \n",
    "        \"In conclusion,\", \"To summarize,\", \"As follows:\"\n",
    "    ]\n",
    "    for prefix in unwanted_prefixes:\n",
    "        if text.startswith(prefix):\n",
    "            text = text[len(prefix):].strip()\n",
    "    \n",
    "    # Remove markdown-like formatting (titles, headers, etc.)\n",
    "    text = re.sub(r\"\\*\\*.*?\\*\\*|=+\", \"\", text)\n",
    "    \n",
    "    # Remove bullet points, numbers, dashes, and unwanted newlines\n",
    "    text = re.sub(r\"(\\*|-|•|\\d+\\.)\\s\", \"\", text)\n",
    "    text = re.sub(r\"\\n+\", \" \", text)\n",
    "    \n",
    "    # Remove anything that's not alphanumeric, standard punctuation, or whitespace\n",
    "    text = re.sub(r\"[^a-zA-Z0-9.,!?;:\\-()\\'\\\"\\s]\", \"\", text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"Explain the difference between supervised and unsupervised learning.\"\n",
    "\n",
    "# Encode the prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate text using different decoding strategies\n",
    "with torch.no_grad():\n",
    "    greedy_output = model.generate(**inputs, max_length=100)\n",
    "    top_k_output = model.generate(**inputs, max_length=100, top_k=50)\n",
    "    top_p_output = model.generate(**inputs, max_length=100, top_p=0.9)\n",
    "    beam_output = model.generate(**inputs, max_length=100, num_beams=5)\n",
    "\n",
    "# Decode the outputs\n",
    "generated_texts = {\n",
    "    \"Greedy\": tokenizer.decode(greedy_output[0], skip_special_tokens=True),\n",
    "    \"Top-K\": tokenizer.decode(top_k_output[0], skip_special_tokens=True),\n",
    "    \"Top-P\": tokenizer.decode(top_p_output[0], skip_special_tokens=True),\n",
    "    \"Beam Search\": tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
    "}\n",
    "\n",
    "# Clean all generated texts\n",
    "cleaned_texts = {strategy: clean_output(text, prompt) for strategy, text in generated_texts.items()}\n",
    "\n",
    "# Reference text (ground truth)\n",
    "reference = [\n",
    "    \"Supervised learning uses labeled data to learn a mapping from inputs to outputs, while unsupervised learning tries to find patterns or groupings within unlabeled data.\"\n",
    "]\n",
    "\n",
    "# Calculate BERTScore for each strategy\n",
    "results = {}\n",
    "for strategy, text in cleaned_texts.items():\n",
    "    P, R, F1 = score([text], reference, lang=\"en\", model_type=\"microsoft/deberta-xlarge-mnli\")\n",
    "    results[strategy] = {\n",
    "        \"Generated Text\": text,\n",
    "        \"BERTScore P\": P.mean().item(),\n",
    "        \"BERTScore R\": R.mean().item(),\n",
    "        \"BERTScore F1\": F1.mean().item(),\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "dataframe=pd.DataFrame.from_dict(results, orient='index')\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(dataframe.to_html(float_format=\"%.4f\", justify=\"center\", index=True, border=0, classes='dataframe')))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS776env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
